# -*- coding: utf-8 -*-
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from bs4 import BeautifulSoup
from datetime import datetime
from urllib.parse import urlparse
import pandas as pd
import time
import os
import re
import requests

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
STOCKS = [
    {"name": "ì‚¼ì„±ì „ì", "keywords": ["ì‚¼ì„±ì „ì"]},
    {"name": "NAVER", "keywords": ["NAVER"]},
    {"name": "ì¹´ì¹´ì˜¤", "keywords": ["ì¹´ì¹´ì˜¤"]},
    {"name": "í˜„ëŒ€ì°¨", "keywords": ["í˜„ëŒ€ì°¨"]},
]

RESULT_PATH = './results'
os.makedirs(RESULT_PATH, exist_ok=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë„ë©”ì¸ë³„ ë³¸ë¬¸ selector
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DOMAIN_SELECTORS = {
    "news.naver.com": ["#dic_area", "#articleBodyContents"],
    "etnews.com": ["div.article_txt"],
    "hankooki.com": ["#article-view-content-div"],
    "joongangenews.com": ["#article-view-content-div"],
    "wowtv.co.kr": ["div.view-cont"],
    # í•„ìš”í•œ ì–¸ë¡ ì‚¬ ê³„ì† ì¶”ê°€
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë³¸ë¬¸ ì •ì œ í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def clean_text(html_fragment):
    if not html_fragment:
        return ""
    text = re.sub(r'<[^>]+>', '', str(html_fragment))
    return text.strip()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë³¸ë¬¸ í¬ë¡¤ë§ í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_article_content(url):
    try:
        resp = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
        soup = BeautifulSoup(resp.text, "html.parser")

        domain = urlparse(url).netloc.replace('www.', '')
        selectors = DOMAIN_SELECTORS.get(domain, [])

        for sel in selectors:
            node = soup.select_one(sel)
            if node:
                print(f"    â†³ ë³¸ë¬¸ selector '{sel}' ì„±ê³µ (by domain)")
                return clean_text(node)

        fallback_selectors = ["#dic_area", "#articleBodyContents", "div#contents .newsct_body", "article#dic_area"]
        for sel in fallback_selectors:
            node = soup.select_one(sel)
            if node:
                print(f"    â†³ ë³¸ë¬¸ selector '{sel}' ì„±ê³µ (fallback)")
                return clean_text(node)

        print("    â†³ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨")
        return ""
    except Exception as e:
        print(f"    â†³ ë³¸ë¬¸ ì˜¤ë¥˜: {e}")
        return ""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë‰´ìŠ¤ ìˆ˜ì§‘ í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def crawler_with_scroll(keyword, start_date, end_date, sort, output_file, max_scroll=5):
    options = Options()
    options.add_argument('--disable-gpu')
    driver = webdriver.Chrome(options=options)

    s_date = start_date.replace(".", "")
    e_date = end_date.replace(".", "")
    url = f"https://search.naver.com/search.naver?where=news&query={keyword}&sort={sort}&ds={start_date}&de={end_date}&nso=so:dd,p:from{s_date}to{e_date},a:&start=1"

    print(f"[í¬ë¡¤ë§ ì‹œì‘] í‚¤ì›Œë“œ: {keyword}")
    print(f"[URL] {url}")

    driver.get(url)
    body = driver.find_element(By.TAG_NAME, "body")
    for i in range(max_scroll):
        body.send_keys(Keys.END)
        time.sleep(1.5)
        print(f"  [ìŠ¤í¬ë¡¤] {i+1}/{max_scroll}íšŒ ì™„ë£Œ")

    soup = BeautifulSoup(driver.page_source, 'html.parser')
    with open(f"{keyword}_page_source.html", "w", encoding="utf-8") as f:
        f.write(driver.page_source)
    driver.quit()

    news_area = soup.select("div.sds-comps-full-layout.dlCcC3TMVQiSy85T_1b2")
    print(f"[DEBUG] ë‰´ìŠ¤ ì˜ì—­ ì¶”ì¶œ ê²°ê³¼ (new selector): {len(news_area)}ê±´")

    titles, links, sources, dates, summaries, contents = [], [], [], [], [], []

    for idx, news in enumerate(news_area):
        a_tag = news.select_one("a")
        if not a_tag or "href" not in a_tag.attrs:
            continue

        link = a_tag["href"]
        title_tag = a_tag.select_one("span.sds-comps-text-type-headline1")
        title = title_tag.get_text(strip=True) if title_tag else "[ì œëª© ì—†ìŒ]"

        print(f"  [ê¸°ì‚¬ {idx+1}] {title} â†’ {link}")

        full_content = fetch_article_content(link)
        print(f"    â†³ ë³¸ë¬¸ ê¸¸ì´: {len(full_content)}ì")

        titles.append(title)
        links.append(link)
        sources.append("")
        summaries.append("")
        dates.append("")
        contents.append(full_content)

    if not titles:
        print(f"[WARN] '{keyword}' í‚¤ì›Œë“œì— ëŒ€í•´ ìˆ˜ì§‘ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    df = pd.DataFrame({
        "keyword": [keyword] * len(titles),
        "date": dates,
        "title": titles,
        "source": sources,
        "summary": summaries,
        "full_content": contents,
        "link": links
    })

    if os.path.exists(output_file):
        prev = pd.read_csv(output_file, encoding="utf-8-sig")
        df = pd.concat([prev, df], ignore_index=True)

    df.to_csv(output_file, index=False, encoding="utf-8-sig")
    print(f"  â–¶ '{keyword}' done, saved to {output_file} (ì´ {len(df)}ê±´)")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    print("ğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘")

    # ì…ë ¥ ì œê±° & í…ŒìŠ¤íŠ¸ìš© ê¸°ë³¸ê°’ ì§€ì •
    max_scroll = 4
    sort = "1"  # ìµœì‹ ìˆœ
    s_date = "2025.08.06"
    e_date = "2025.08.07"
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")

    for stock in STOCKS:
        name = stock["name"]
        fname = f"{name}_{s_date.replace('.', '')}_{e_date.replace('.', '')}_{timestamp}.csv"
        path = os.path.join(RESULT_PATH, fname)
        print(f"\nğŸ“Œ [{name}] ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘ â†’ ì €ì¥ ê²½ë¡œ: {path}")
        for kw in stock["keywords"]:
            print(f" - í‚¤ì›Œë“œ: {kw}")
            crawler_with_scroll(kw, s_date, e_date, sort, path, max_scroll)

    print("\nâœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()