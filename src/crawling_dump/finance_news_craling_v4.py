# -*- coding: utf-8 -*-
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from bs4 import BeautifulSoup
from datetime import datetime
import pandas as pd
import time
import os
import re
import requests

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. ì„¤ì • ì˜ì—­
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
STOCKS = [
    {"name": "ì‚¼ì„±ì „ì", "keywords": ["ì‚¼ì„±ì „ì", "ì‚¼ì„±ì „ì ì£¼ê°€", "ì‚¼ì„±ì „ì ë‰´ìŠ¤"]},
    {"name": "NAVER", "keywords": ["NAVER", "NAVER ì£¼ê°€", "NAVER ë‰´ìŠ¤"]},
    {"name": "ì¹´ì¹´ì˜¤", "keywords": ["ì¹´ì¹´ì˜¤", "ì¹´ì¹´ì˜¤ ì£¼ê°€", "ì¹´ì¹´ì˜¤ ë‰´ìŠ¤"]},
    {"name": "í˜„ëŒ€ì°¨", "keywords": ["í˜„ëŒ€ì°¨", "í˜„ëŒ€ì°¨ ì£¼ê°€", "í˜„ëŒ€ì°¨ ë‰´ìŠ¤"]},
]

RESULT_PATH = './results'
os.makedirs(RESULT_PATH, exist_ok=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. ë³¸ë¬¸ ì •ì œ í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def clean_text(html_fragment):
    if not html_fragment:
        return ""
    text = re.sub(r'<[^>]+>', '', str(html_fragment))
    return text.strip()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. ë³¸ë¬¸ í¬ë¡¤ë§ í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_article_content(url):
    try:
        resp = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
        soup = BeautifulSoup(resp.text, "html.parser")
        selectors = [
            "#dic_area", "#articleBodyContents", "div#contents .newsct_body", "article#dic_area"
        ]
        for sel in selectors:
            node = soup.select_one(sel)
            if node:
                print(f"    â†³ ë³¸ë¬¸ selector '{sel}' ì„±ê³µ")
                return clean_text(node)
        print("    â†³ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨")
        return ""
    except Exception as e:
        return f"[ë³¸ë¬¸ ì˜¤ë¥˜] {e}"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ + ë³¸ë¬¸ ìˆ˜ì§‘ í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def crawler_with_scroll(keyword, start_date, end_date, sort, output_file, max_scroll=5):
    options = Options()
    options.add_argument('--disable-gpu')
    # options.add_argument('--headless')  # ë””ë²„ê¹… ì‹œì—” ì£¼ì„ ì²˜ë¦¬
    driver = webdriver.Chrome(options=options)

    s_date = start_date.replace(".", "")
    e_date = end_date.replace(".", "")
    url = f"https://search.naver.com/search.naver?where=news&query={keyword}&sort={sort}&ds={start_date}&de={end_date}&nso=so:dd,p:from{s_date}to{e_date},a:&start=1"

    print(f"[í¬ë¡¤ë§ ì‹œì‘] í‚¤ì›Œë“œ: {keyword}")
    print(f"[URL] {url}")

    driver.get(url)
    body = driver.find_element(By.TAG_NAME, "body")

    for i in range(max_scroll):
        body.send_keys(Keys.END)
        time.sleep(1.5)
        print(f"  [ìŠ¤í¬ë¡¤] {i+1}/{max_scroll}íšŒ ì™„ë£Œ")

    soup = BeautifulSoup(driver.page_source, 'html.parser')

    # ğŸ“Œ ë””ë²„ê¹…ìš© HTML ì €ì¥
    with open(f"{keyword}_page_source.html", "w", encoding="utf-8") as f:
        f.write(driver.page_source)
        print(f"[DEBUG] page_source ì €ì¥ ì™„ë£Œ â†’ {keyword}_page_source.html")

    driver.quit()

    titles, links, sources, dates, summaries, contents = [], [], [], [], [], []

    # ğŸ” ê¸°ì¡´: news_area = soup.select("ul.list_news div.news_area")
    news_area = soup.select("div.sds-comps-full-layout.dlCcC3TMVQiSy85T_1b2")
    print(f"[DEBUG] ë‰´ìŠ¤ ì˜ì—­ ì¶”ì¶œ ê²°ê³¼ (new selector): {len(news_area)}ê±´")

    for idx, news in enumerate(news_area):
        a_tag = news.select_one("a")
        if not a_tag or "href" not in a_tag.attrs:
            print(f"  [SKIP] ë§í¬ ì—†ìŒ â†’ idx={idx}")
            continue

        link = a_tag["href"]
        title_tag = a_tag.select_one("span.sds-comps-text-type-headline1")
        title = title_tag.get_text(strip=True) if title_tag else "[ì œëª© ì—†ìŒ]"

        print(f"  [ê¸°ì‚¬ {idx+1}] {title} â†’ {link}")

        source_tag = news.select_one("a.info.press")
        source = source_tag.get_text(strip=True) if source_tag else ""

        summary_tag = news.select_one("div.dsc_wrap")
        summary = summary_tag.get_text(strip=True) if summary_tag else ""

        date_tag = news.select_one("span.info")
        date = date_tag.get_text(strip=True) if date_tag else ""

        full_content = fetch_article_content(link)
        print(f"    â†³ ë³¸ë¬¸ ê¸¸ì´: {len(full_content)}ì")

        titles.append(title)
        links.append(link)
        sources.append(source)
        summaries.append(summary)
        dates.append(date)
        contents.append(full_content)

    if not titles:
        print(f"[WARN] '{keyword}' í‚¤ì›Œë“œì— ëŒ€í•´ ìˆ˜ì§‘ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    df = pd.DataFrame({
        "keyword": [keyword] * len(titles),
        "date": dates,
        "title": titles,
        "source": sources,
        "summary": summaries,
        "full_content": contents,
        "link": links
    })

    if os.path.exists(output_file):
        prev = pd.read_csv(output_file, encoding="utf-8-sig")
        df = pd.concat([prev, df], ignore_index=True)

    df.to_csv(output_file, index=False, encoding="utf-8-sig")
    print(f"  â–¶ '{keyword}' done, saved to {output_file} (ì´ {len(df)}ê±´)")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def main():
#     print("ğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘")
#     max_scroll = int(input("ìŠ¤í¬ë¡¤ íšŸìˆ˜ (ì˜ˆ:5)> ").strip())
#     sort    = input("ì •ë ¬ ë°©ì‹ (ê´€ë ¨ë„=0, ìµœì‹ =1, ì˜¤ë˜ëœ=2)> ").strip()
#     s_date  = input("ì‹œì‘ì¼ YYYY.MM.DD> ").strip()
#     e_date  = input("ì¢…ë£Œì¼ YYYY.MM.DD> ").strip()
#     timestamp = datetime.now().strftime("%Y%m%d_%H%M")

#     for stock in STOCKS:
#         name = stock["name"]
#         fname = f"{name}_{s_date.replace('.', '')}_{e_date.replace('.', '')}_{timestamp}.csv"
#         path  = os.path.join(RESULT_PATH, fname)
#         print(f"\nğŸ“Œ [{name}] ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘ â†’ ì €ì¥ ê²½ë¡œ: {path}")
#         for kw in stock["keywords"]:
#             print(f" - í‚¤ì›Œë“œ: {kw}")
#             crawler_with_scroll(kw, s_date, e_date, sort, path, max_scroll)

#     print("\nâœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

# if __name__ == "__main__":
#     main()

def main():
    print("ğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘")

    # âœ… ì…ë ¥ê°’ ì œê±° & í…ŒìŠ¤íŠ¸ê°’ ì§€ì •
    max_scroll = 4
    sort = "1"  # ìµœì‹ ìˆœ
    s_date = "2025.08.06"
    e_date = "2025.08.07"
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")

    for stock in STOCKS:
        name = stock["name"]
        fname = f"{name}_{s_date.replace('.', '')}_{e_date.replace('.', '')}_{timestamp}.csv"
        path = os.path.join(RESULT_PATH, fname)
        print(f"\nğŸ“Œ [{name}] ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘ â†’ ì €ì¥ ê²½ë¡œ: {path}")
        for kw in stock["keywords"]:
            print(f" - í‚¤ì›Œë“œ: {kw}")
            crawler_with_scroll(kw, s_date, e_date, sort, path, max_scroll)

    print("\nâœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()