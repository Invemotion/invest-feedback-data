{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e2ba98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Naver Finance ë‰´ìŠ¤ â†’ í†µí•©Â·ì •ì œÂ·ì¤‘ë³µ ì œê±° â†’ news_clean_YYYYMMDD.csv\n",
    "\"\"\"\n",
    "import glob, re, unicodedata, pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. íŒŒì¼ ë¡œë“œ\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def load_csvs(pattern=\"/Users/yujimin/KB AI CHALLENGE/project/data/news_raw/*.csv\") -> pd.DataFrame:\n",
    "    files = glob.glob(pattern)\n",
    "    print(f\"ðŸ“‚ Found {len(files)} CSV file(s)\")\n",
    "    return pd.concat((pd.read_csv(f, encoding=\"utf-8\") for f in files), ignore_index=True)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2. í…ìŠ¤íŠ¸ ì •ê·œí™”\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "URL_RE = re.compile(r\"https?://\\S+\")\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFKC\", str(text))\n",
    "    text = URL_RE.sub(\"\", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3. ì „ì²˜ë¦¬ & ì¤‘ë³µ ì œê±°\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.dropna(subset=[\"title\", \"content\"]).drop_duplicates(subset=[\"link\"])\n",
    "\n",
    "    df[\"title\"]   = df[\"title\"].apply(normalize)\n",
    "    df[\"content\"] = df[\"content\"].apply(normalize)\n",
    "    df[\"text\"]    = df[\"title\"] + \" [SEP] \" + df[\"content\"]\n",
    "\n",
    "    # (ì„ íƒ) ë™ì¼ ì œëª©Â·ë‚ ì§œ ì¤‘ë³µ ì œê±° â€“ ë³¸ë¬¸ ê¸¸ì´ ê¸´ ê¸°ì‚¬ ìš°ì„ \n",
    "    df[\"len\"] = df[\"content\"].str.len()\n",
    "    df = (\n",
    "        df.sort_values(\"len\", ascending=False)\n",
    "          .drop_duplicates(subset=[\"title\", \"datetime\"], keep=\"first\")\n",
    "          .drop(columns=\"len\")\n",
    "    )\n",
    "    return df[[\"stock_name\", \"datetime\", \"text\", \"link\"]]\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4. ì €ìž¥ íŒŒì¼ëª… ê²°ì • ë¡œì§\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def decide_filename(df: pd.DataFrame, out_dir=\"/Users/yujimin/KB AI CHALLENGE/project\") -> str:\n",
    "    # datetime ì»¬ëŸ¼ì—ì„œ â€˜YYYY.MM.DDâ€™ ë¶€ë¶„ë§Œ ì¶”ì¶œ\n",
    "    dates = df[\"datetime\"].astype(str).str.extract(r\"(\\d{4}\\.\\d{2}\\.\\d{2})\")[0]\n",
    "    # ê°€ìž¥ ì´ë¥¸ ë‚ ì§œ(earliest) ì„ íƒ â€” í•„ìš” ì‹œ latest/most common ìœ¼ë¡œ êµì²´\n",
    "    target = pd.to_datetime(dates, format=\"%Y.%m.%d\").min().strftime(\"%Y%m%d\")\n",
    "    return f\"{out_dir}/news_clean_{target}.csv\"\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5. ë©”ì¸\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if __name__ == \"__main__\":\n",
    "    raw_df   = load_csvs()\n",
    "    clean_df = preprocess(raw_df)\n",
    "    print(f\"âœ… After cleaning: {clean_df.shape}\")\n",
    "\n",
    "    out_csv = decide_filename(clean_df)\n",
    "    clean_df.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "    print(f\"ðŸŽ‰ Saved â†’ {out_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b602477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from IPython.display import clear_output\n",
    "drive.mount('/content/drive')\n",
    "#!pip install \"transformers>=4.41.0\" \"torch>=2.2.0\" pandas\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e275d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "news_clean_250805.csv â†’ KoBART ìž¬ìš”ì•½(ìµœì¢… ìš”ì•½ ê¸¸ì´: ë‹¨ì–´ 80~110 í† í° íƒ€ê¹ƒ) â†’ text ì»¬ëŸ¼ ì œê±°\n",
    "ì¶œë ¥: news_summary_250805_len80_110.csv\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1) ê²½ë¡œÂ·í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Colab ì˜ˆì‹œ\n",
    "IN_CSV  = \"/content/drive/MyDrive/Invemotion/news_clean_20250805.csv\"\n",
    "OUT_CSV = \"/content/drive/MyDrive/Invemotion/news_summary_20250805_len80_110.csv\"\n",
    "\n",
    "# ë¡œì»¬ ì˜ˆì‹œ\n",
    "# IN_CSV  = \"/Users/yujimin/KB AI CHALLENGE/project/news_clean_250805.csv\"\n",
    "# OUT_CSV = \"/Users/yujimin/KB AI CHALLENGE/project/news_summary_250805_len80_110.csv\"\n",
    "\n",
    "DEVICE          = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE      = 8            # T4(16GB)ì—ì„œ 8~16 ê¶Œìž¥\n",
    "MAX_INPUT_LEN   = 512          # ì¸ì½”ë” ìž…ë ¥ ìµœëŒ€ í† í°\n",
    "CHUNK_TOKENS    = 400          # ê¸´ ë¬¸ì„œ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì¡°ê° í¬ê¸°\n",
    "OVERLAP_TOKENS  = 80           # ì¡°ê° ê°„ ê²¹ì¹¨\n",
    "\n",
    "# â”€â”€ ìµœì¢… ìš”ì•½(ë‹¨ì–´) ëª©í‘œ ë²”ìœ„ â†’ BPE í™˜ì‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ê²½í—˜ì¹˜: í•œêµ­ì–´ KoBARTì—ì„œ \"ë‹¨ì–´ í† í° 1ê°œ â‰ˆ BPE 1.8ê°œ\" ê°€ëŸ‰\n",
    "BPE_PER_WORD        = 1.8\n",
    "FINAL_WORD_RANGE    = (80, 110)    # ìµœì¢… ìš”ì•½ ëª©í‘œ(ë‹¨ì–´ ê¸°ì¤€)\n",
    "CHUNK_WORD_RANGE    = (50, 80)     # ì¡°ê° ìš”ì•½ ëª©í‘œ(ë‹¨ì–´ ê¸°ì¤€, ë©”íƒ€ìš”ì•½ ì „ ë‹¨ê³„)\n",
    "\n",
    "FINAL_MIN_NEW = int(FINAL_WORD_RANGE[0] * BPE_PER_WORD)   # â‰ˆ 144\n",
    "FINAL_MAX_NEW = int(FINAL_WORD_RANGE[1] * BPE_PER_WORD)   # â‰ˆ 198\n",
    "\n",
    "CHUNK_MIN_NEW = int(CHUNK_WORD_RANGE[0] * BPE_PER_WORD)   # â‰ˆ 90\n",
    "CHUNK_MAX_NEW = int(CHUNK_WORD_RANGE[1] * BPE_PER_WORD)   # â‰ˆ 144\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2) ëª¨ë¸ ë¡œë“œ\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ðŸ”„  Loading KoBART summarizer â€¦\")\n",
    "tok   = AutoTokenizer.from_pretrained(\"digit82/kobart-summarization\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"digit82/kobart-summarization\").to(DEVICE)\n",
    "if DEVICE == \"cuda\":\n",
    "    model = model.to(dtype=torch.float16)  # VRAM ì ˆì•½\n",
    "model.eval()\n",
    "\n",
    "@torch.inference_mode()\n",
    "def summarize_batch(\n",
    "    texts: list[str],\n",
    "    min_new_tokens: int,\n",
    "    max_new_tokens: int\n",
    ") -> list[str]:\n",
    "    \"\"\"KoBART ë°°ì¹˜ ìš”ì•½ (token_type_ids ì œê±°, ê¸¸ì´ íƒ€ê¹ƒ ì§€ì •)\"\"\"\n",
    "    enc = tok(\n",
    "        texts,\n",
    "        max_length=MAX_INPUT_LEN,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    enc.pop(\"token_type_ids\", None)\n",
    "    enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "\n",
    "    outs = model.generate(\n",
    "        **enc,\n",
    "        min_new_tokens=min_new_tokens,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_beams=4,\n",
    "        length_penalty=1.2,        # ì•½ê°„ ë” ê¸¸ê²Œ ìœ ë„\n",
    "        no_repeat_ngram_size=3,    # ë°˜ë³µ ê°ì†Œ\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return tok.batch_decode(outs, skip_special_tokens=True)\n",
    "\n",
    "def chunk_by_tokens(text: str,\n",
    "                    chunk_tokens=CHUNK_TOKENS,\n",
    "                    overlap=OVERLAP_TOKENS) -> list[str]:\n",
    "    \"\"\"í† í° ë‹¨ìœ„ ìŠ¬ë¼ì´ì‹± â†’ ë¶€ë¶„ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜\"\"\"\n",
    "    ids = tok.encode(text, add_special_tokens=True)\n",
    "    chunks = []\n",
    "    step = max(1, chunk_tokens - overlap)\n",
    "    for s in range(0, len(ids), step):\n",
    "        piece = ids[s:s + chunk_tokens]\n",
    "        if not piece:\n",
    "            break\n",
    "        chunks.append(tok.decode(piece, skip_special_tokens=True))\n",
    "        if s + chunk_tokens >= len(ids):\n",
    "            break\n",
    "    return chunks\n",
    "\n",
    "def summarize_long(text: str) -> str:\n",
    "    \"\"\"ê¸´ ë¬¸ì„œ: ì¡°ê° ìš”ì•½(ì§§ê²Œ) â†’ ë©”íƒ€ ìš”ì•½(ìµœì¢… ê¸¸ì´ íƒ€ê¹ƒ)\"\"\"\n",
    "    parts = chunk_by_tokens(text)\n",
    "    if not parts:\n",
    "        return \"\"\n",
    "    # 1ì°¨: ì¡°ê°ë³„ ìš”ì•½ (ì¡°ê°ì€ ì§§ê²Œ ì••ì¶•)\n",
    "    part_sums = []\n",
    "    for i in range(0, len(parts), BATCH_SIZE):\n",
    "        part_sums += summarize_batch(parts[i:i + BATCH_SIZE],\n",
    "                                     min_new_tokens=CHUNK_MIN_NEW,\n",
    "                                     max_new_tokens=CHUNK_MAX_NEW)\n",
    "    # 2ì°¨: ìš”ì•½ë“¤ì˜ ìš”ì•½(ìµœì¢… ê¸¸ì´ íƒ€ê¹ƒìœ¼ë¡œ ìž¬ìš”ì•½)\n",
    "    final = summarize_batch([\" \".join(part_sums)],\n",
    "                            min_new_tokens=FINAL_MIN_NEW,\n",
    "                            max_new_tokens=FINAL_MAX_NEW)[0]\n",
    "    return final\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3) ë°ì´í„° ë¡œë“œ\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df = pd.read_csv(IN_CSV, encoding=\"utf-8\")\n",
    "assert \"text\" in df.columns, \"ìž…ë ¥ CSVì— 'text' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\"\n",
    "df[\"text\"] = df[\"text\"].fillna(\"\")\n",
    "\n",
    "print(f\"ðŸ“‚ Loaded {len(df):,} rows\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4) 512í† í° ì´ˆê³¼ ì—¬ë¶€ ì¸¡ì •\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "enc = tok(df[\"text\"].tolist(), truncation=False, padding=False)\n",
    "df[\"tok_len\"]   = [len(x) for x in enc[\"input_ids\"]]\n",
    "df[\"truncated\"] = df[\"tok_len\"] > MAX_INPUT_LEN\n",
    "print(f\"âš™ï¸  >512 tokens: {df['truncated'].mean():.1%}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5) ìš”ì•½ ì‹¤í–‰ (ìµœì¢… 80~110 'ë‹¨ì–´' í† í° íƒ€ê¹ƒ)\n",
    "#  - ì§§ì€ ë¬¸ì„œ(â‰¤512): ê¸¸ì´ íƒ€ê¹ƒìœ¼ë¡œ ë°°ì¹˜ ìš”ì•½\n",
    "#  - ê¸´ ë¬¸ì„œ(>512):  ìŠ¬ë¼ì´ë”© ìœˆë„ìš° â†’ ë©”íƒ€ ìš”ì•½(ê¸¸ì´ íƒ€ê¹ƒ)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "summaries = [\"\"] * len(df)\n",
    "\n",
    "# 5-a) ì§§ì€ ë¬¸ì„œ ì¼ê´„ ì²˜ë¦¬\n",
    "short_idx = df.index[~df[\"truncated\"]].tolist()\n",
    "for i in tqdm(range(0, len(short_idx), BATCH_SIZE), desc=\"Short docs\"):\n",
    "    batch_ids = short_idx[i:i + BATCH_SIZE]\n",
    "    texts = df.loc[batch_ids, \"text\"].tolist()\n",
    "    outs = summarize_batch(texts,\n",
    "                           min_new_tokens=FINAL_MIN_NEW,\n",
    "                           max_new_tokens=FINAL_MAX_NEW)\n",
    "    for j, idx in enumerate(batch_ids):\n",
    "        summaries[idx] = outs[j]\n",
    "\n",
    "# 5-b) ê¸´ ë¬¸ì„œ ê°œë³„ ì²˜ë¦¬ (í’ˆì§ˆ ìš°ì„ )\n",
    "long_idx = df.index[df[\"truncated\"]].tolist()\n",
    "for idx in tqdm(long_idx, desc=\"Long docs\"):\n",
    "    summaries[idx] = summarize_long(df.at[idx, \"text\"])\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 6) ì €ìž¥\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df.insert(2, \"summary\", summaries)\n",
    "df = df.drop(columns=[\"text\", \"tok_len\", \"truncated\"])\n",
    "\n",
    "out_path = Path(OUT_CSV)\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"ðŸŽ‰ Saved â†’ {out_path}  ({len(df):,} rows, cols: {list(df.columns)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d960d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Minimal prep for KR-FinBert\n",
    "import pandas as pd, unicodedata as ud, re\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "IN  = \"/Users/yujimin/KB AI CHALLENGE/project/src/news_summary_20250805_len80_110.csv\"\n",
    "OUT = \"/Users/yujimin/KB AI CHALLENGE/project/src/news_summary_20250805_len80_110_clean.csv\"\n",
    "\n",
    "df = pd.read_csv(IN)\n",
    "assert set([\"stock_name\",\"datetime\",\"summary\",\"link\"]).issubset(df.columns)\n",
    "\n",
    "def clean_summary(s: str) -> str:\n",
    "    s = \"\" if pd.isna(s) else str(s)\n",
    "    s = ud.normalize(\"NFKC\", s)          # í­/ì¡°í•© ì •ê·œí™”\n",
    "    s = s.replace(\"[SEP]\", \" \")          # í˜¹ì‹œ ë‚¨ì•„ìžˆë‹¤ë©´ ì œê±°\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()   # ê³µë°± ì •ë¦¬\n",
    "    if s and not re.search(r'[.!?\\\"â€â€™)\\]]$', s):  # ì¢…ê²°ë¶€í˜¸ ë³´ì •(ì—†ì„ ë•Œë§Œ)\n",
    "        s += \".\"\n",
    "    return s\n",
    "\n",
    "df[\"summary\"] = df[\"summary\"].apply(clean_summary)\n",
    "\n",
    "# (ì„ íƒ) ë§í¬ ê¸°ì¤€ ì¤‘ë³µ ì œê±° â€“ ì´ë¯¸ ì œê±°ë˜ì–´ ìžˆìœ¼ë©´ ë³€í™” ì—†ìŒ\n",
    "df = df.drop_duplicates(subset=[\"link\"]).reset_index(drop=True)\n",
    "\n",
    "# âœ… 512 í† í° ì´ˆê³¼ ì—¬ë¶€ ì ê²€(ì´ìƒì¹˜ íƒì§€ ìš©)\n",
    "tok = AutoTokenizer.from_pretrained(\"snunlp/KR-FinBert-SC\")\n",
    "enc = tok(df[\"summary\"].tolist(), truncation=False, padding=False)\n",
    "over512 = sum(len(ids) > 512 for ids in enc[\"input_ids\"])\n",
    "print(f\"Over 512 tokens: {over512} rows\")\n",
    "\n",
    "df.to_csv(OUT, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"Saved â†’ {OUT} ({len(df)} rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ca0696",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "KR-FinBERT-SC ê°ì„± ì¶”ë¡ \n",
    "ìž…ë ¥:  news_summary_20250805_len80_110_clean.csv (stock_name, datetime, summary, link)\n",
    "ì¶œë ¥:  news_20250805_sent_scored.csv (neg/neu/pos + pred_label/pred_conf ì¶”ê°€)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1) ê²½ë¡œ/í™˜ê²½\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "IN_CSV  = \"/content/drive/MyDrive/Invemotion/news_summary_20250805_len80_110_clean.csv\"\n",
    "OUT_CSV = \"/content/drive/MyDrive/Invemotion/news_20250805_sent_scored.csv\"\n",
    "\n",
    "MODEL_NAME = \"snunlp/KR-FinBert-SC\"   # ê¸ˆìœµ ë‰´ìŠ¤ ê°ì„± ë¶„ë¥˜ 3-í´ëž˜ìŠ¤ (neg/neu/pos)\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 32                        # T4(16GB) 32~64 ê¶Œìž¥ / CPUë©´ 8~16ë¡œ ë‚®ì¶”ì„¸ìš”\n",
    "MAX_LEN    = 512\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2) ë¡œë“œ\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ðŸ”„ Loading model/tokenizerâ€¦\")\n",
    "tok  = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "if DEVICE == \"cuda\":\n",
    "    model = model.to(dtype=torch.float16)  # VRAM ì ˆê°\n",
    "model.eval()\n",
    "\n",
    "# id2label ì•ˆì „ ë§¤í•‘ (ëª¨ë¸ ì„¤ì •ì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°)\n",
    "id2label = {int(i): lab for i, lab in model.config.id2label.items()}\n",
    "label_list = [id2label[i].lower() for i in range(model.config.num_labels)]\n",
    "# í‘œì¤€í™”ëœ ì—´ ì´ë¦„ ë§¤í•‘(ëª¨ë¸ ë¼ë²¨ëª…ì´ ëŒ€ì†Œë¬¸ìž/í‘œê¸° ë‹¤ë¥¼ ìˆ˜ ìžˆì–´ ë³´ì •)\n",
    "norm = {\"negative\":\"neg\", \"neg\":\"neg\",\n",
    "        \"neutral\":\"neu\",  \"neu\":\"neu\",\n",
    "        \"positive\":\"pos\", \"pos\":\"pos\"}\n",
    "out_cols = [norm.get(l, l) for l in label_list]  # ì˜ˆ: ['neg','neu','pos']\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3) ë°ì´í„°\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df = pd.read_csv(IN_CSV, encoding=\"utf-8\")\n",
    "assert set([\"stock_name\",\"datetime\",\"summary\",\"link\"]).issubset(df.columns)\n",
    "texts = df[\"summary\"].fillna(\"\").astype(str).tolist()\n",
    "print(f\"ðŸ“‚ Loaded {len(df):,} rows\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4) ë°°ì¹˜ ì¶”ë¡ \n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def softmax_np(x):\n",
    "    import numpy as np\n",
    "    e = np.exp(x - x.max(axis=1, keepdims=True))\n",
    "    return e / e.sum(axis=1, keepdims=True)\n",
    "\n",
    "probs_all = []\n",
    "pred_idx_all = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for i in tqdm(range(0, len(texts), BATCH_SIZE), desc=\"Scoring\"):\n",
    "        batch = texts[i:i+BATCH_SIZE]\n",
    "        enc = tok(batch, truncation=True, max_length=MAX_LEN,\n",
    "                  padding=True, return_tensors=\"pt\")\n",
    "        # token_type_idsëŠ” ëª¨ë¸ì— ë”°ë¼ ë¬´ì‹œë˜ì§€ë§Œ ì•ˆì „í•˜ê²Œ ì œê±°\n",
    "        enc.pop(\"token_type_ids\", None)\n",
    "        enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "\n",
    "        logits = model(**enc).logits  # [B, 3]\n",
    "        if DEVICE == \"cuda\":\n",
    "            logits = logits.float()   # softmax ìœ„í•´ fp32ë¡œ ìž„ì‹œ ë³€í™˜\n",
    "        probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "        pred_idx = probs.argmax(axis=1)\n",
    "        probs_all.append(probs)\n",
    "        pred_idx_all.append(pred_idx)\n",
    "\n",
    "import numpy as np\n",
    "probs_all = np.vstack(probs_all)              # shape [N, 3]\n",
    "pred_idx_all = np.concatenate(pred_idx_all)   # shape [N]\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5) ê²°ê³¼ í”„ë ˆìž„ êµ¬ì„±\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ë¼ë²¨ ìˆœì„œì— ë§žì¶° ì—´ ì´ë¦„ ì§€ì •\n",
    "probs_df = pd.DataFrame(probs_all, columns=out_cols)\n",
    "pred_labels = [label_list[i] for i in pred_idx_all]  # ëª¨ë¸ ì›ë¼ë²¨(ì†Œë¬¸ìž)\n",
    "pred_conf   = probs_all.max(axis=1)\n",
    "\n",
    "out = pd.concat([df[[\"stock_name\",\"datetime\",\"summary\",\"link\"]],\n",
    "                 probs_df], axis=1)\n",
    "out[\"pred_label\"] = [norm.get(l, l) for l in pred_labels]  # 'neg/neu/pos'ë¡œ í†µì¼\n",
    "out[\"pred_conf\"]  = pred_conf\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 6) ì €ìž¥\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "out_path = Path(OUT_CSV)\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "out.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"ðŸŽ‰ Saved â†’ {out_path}  (rows={len(out):,})\")\n",
    "print(f\"   Columns: {list(out.columns)}\")\n",
    "print(\"   Label order from model:\", label_list, \"-> mapped to:\", out_cols)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
