{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e2ba98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Naver Finance 뉴스 → 통합·정제·중복 제거 → news_clean_YYYYMMDD.csv\n",
    "\"\"\"\n",
    "import glob, re, unicodedata, pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 1. 파일 로드\n",
    "# ───────────────────────────────\n",
    "def load_csvs(pattern=\"/Users/yujimin/KB AI CHALLENGE/project/data/news_raw/*.csv\") -> pd.DataFrame:\n",
    "    files = glob.glob(pattern)\n",
    "    print(f\"📂 Found {len(files)} CSV file(s)\")\n",
    "    return pd.concat((pd.read_csv(f, encoding=\"utf-8\") for f in files), ignore_index=True)\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 2. 텍스트 정규화\n",
    "# ───────────────────────────────\n",
    "URL_RE = re.compile(r\"https?://\\S+\")\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFKC\", str(text))\n",
    "    text = URL_RE.sub(\"\", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 3. 전처리 & 중복 제거\n",
    "# ───────────────────────────────\n",
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.dropna(subset=[\"title\", \"content\"]).drop_duplicates(subset=[\"link\"])\n",
    "\n",
    "    df[\"title\"]   = df[\"title\"].apply(normalize)\n",
    "    df[\"content\"] = df[\"content\"].apply(normalize)\n",
    "    df[\"text\"]    = df[\"title\"] + \" [SEP] \" + df[\"content\"]\n",
    "\n",
    "    # (선택) 동일 제목·날짜 중복 제거 – 본문 길이 긴 기사 우선\n",
    "    df[\"len\"] = df[\"content\"].str.len()\n",
    "    df = (\n",
    "        df.sort_values(\"len\", ascending=False)\n",
    "          .drop_duplicates(subset=[\"title\", \"datetime\"], keep=\"first\")\n",
    "          .drop(columns=\"len\")\n",
    "    )\n",
    "    return df[[\"stock_name\", \"datetime\", \"text\", \"link\"]]\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 4. 저장 파일명 결정 로직\n",
    "# ───────────────────────────────\n",
    "def decide_filename(df: pd.DataFrame, out_dir=\"/Users/yujimin/KB AI CHALLENGE/project\") -> str:\n",
    "    # datetime 컬럼에서 ‘YYYY.MM.DD’ 부분만 추출\n",
    "    dates = df[\"datetime\"].astype(str).str.extract(r\"(\\d{4}\\.\\d{2}\\.\\d{2})\")[0]\n",
    "    # 가장 이른 날짜(earliest) 선택 — 필요 시 latest/most common 으로 교체\n",
    "    target = pd.to_datetime(dates, format=\"%Y.%m.%d\").min().strftime(\"%Y%m%d\")\n",
    "    return f\"{out_dir}/news_clean_{target}.csv\"\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 5. 메인\n",
    "# ───────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    raw_df   = load_csvs()\n",
    "    clean_df = preprocess(raw_df)\n",
    "    print(f\"✅ After cleaning: {clean_df.shape}\")\n",
    "\n",
    "    out_csv = decide_filename(clean_df)\n",
    "    clean_df.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "    print(f\"🎉 Saved → {out_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b602477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from IPython.display import clear_output\n",
    "drive.mount('/content/drive')\n",
    "#!pip install \"transformers>=4.41.0\" \"torch>=2.2.0\" pandas\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e275d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "news_clean_250805.csv → KoBART 재요약(최종 요약 길이: 단어 80~110 토큰 타깃) → text 컬럼 제거\n",
    "출력: news_summary_250805_len80_110.csv\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 1) 경로·하이퍼파라미터\n",
    "# ───────────────────────────────\n",
    "# Colab 예시\n",
    "IN_CSV  = \"/content/drive/MyDrive/Invemotion/news_clean_20250805.csv\"\n",
    "OUT_CSV = \"/content/drive/MyDrive/Invemotion/news_summary_20250805_len80_110.csv\"\n",
    "\n",
    "# 로컬 예시\n",
    "# IN_CSV  = \"/Users/yujimin/KB AI CHALLENGE/project/news_clean_250805.csv\"\n",
    "# OUT_CSV = \"/Users/yujimin/KB AI CHALLENGE/project/news_summary_250805_len80_110.csv\"\n",
    "\n",
    "DEVICE          = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE      = 8            # T4(16GB)에서 8~16 권장\n",
    "MAX_INPUT_LEN   = 512          # 인코더 입력 최대 토큰\n",
    "CHUNK_TOKENS    = 400          # 긴 문서 슬라이딩 윈도우 조각 크기\n",
    "OVERLAP_TOKENS  = 80           # 조각 간 겹침\n",
    "\n",
    "# ── 최종 요약(단어) 목표 범위 → BPE 환산 ─────────────────────────\n",
    "# 경험치: 한국어 KoBART에서 \"단어 토큰 1개 ≈ BPE 1.8개\" 가량\n",
    "BPE_PER_WORD        = 1.8\n",
    "FINAL_WORD_RANGE    = (80, 110)    # 최종 요약 목표(단어 기준)\n",
    "CHUNK_WORD_RANGE    = (50, 80)     # 조각 요약 목표(단어 기준, 메타요약 전 단계)\n",
    "\n",
    "FINAL_MIN_NEW = int(FINAL_WORD_RANGE[0] * BPE_PER_WORD)   # ≈ 144\n",
    "FINAL_MAX_NEW = int(FINAL_WORD_RANGE[1] * BPE_PER_WORD)   # ≈ 198\n",
    "\n",
    "CHUNK_MIN_NEW = int(CHUNK_WORD_RANGE[0] * BPE_PER_WORD)   # ≈ 90\n",
    "CHUNK_MAX_NEW = int(CHUNK_WORD_RANGE[1] * BPE_PER_WORD)   # ≈ 144\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 2) 모델 로드\n",
    "# ───────────────────────────────\n",
    "print(\"🔄  Loading KoBART summarizer …\")\n",
    "tok   = AutoTokenizer.from_pretrained(\"digit82/kobart-summarization\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"digit82/kobart-summarization\").to(DEVICE)\n",
    "if DEVICE == \"cuda\":\n",
    "    model = model.to(dtype=torch.float16)  # VRAM 절약\n",
    "model.eval()\n",
    "\n",
    "@torch.inference_mode()\n",
    "def summarize_batch(\n",
    "    texts: list[str],\n",
    "    min_new_tokens: int,\n",
    "    max_new_tokens: int\n",
    ") -> list[str]:\n",
    "    \"\"\"KoBART 배치 요약 (token_type_ids 제거, 길이 타깃 지정)\"\"\"\n",
    "    enc = tok(\n",
    "        texts,\n",
    "        max_length=MAX_INPUT_LEN,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    enc.pop(\"token_type_ids\", None)\n",
    "    enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "\n",
    "    outs = model.generate(\n",
    "        **enc,\n",
    "        min_new_tokens=min_new_tokens,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_beams=4,\n",
    "        length_penalty=1.2,        # 약간 더 길게 유도\n",
    "        no_repeat_ngram_size=3,    # 반복 감소\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return tok.batch_decode(outs, skip_special_tokens=True)\n",
    "\n",
    "def chunk_by_tokens(text: str,\n",
    "                    chunk_tokens=CHUNK_TOKENS,\n",
    "                    overlap=OVERLAP_TOKENS) -> list[str]:\n",
    "    \"\"\"토큰 단위 슬라이싱 → 부분 텍스트 리스트 반환\"\"\"\n",
    "    ids = tok.encode(text, add_special_tokens=True)\n",
    "    chunks = []\n",
    "    step = max(1, chunk_tokens - overlap)\n",
    "    for s in range(0, len(ids), step):\n",
    "        piece = ids[s:s + chunk_tokens]\n",
    "        if not piece:\n",
    "            break\n",
    "        chunks.append(tok.decode(piece, skip_special_tokens=True))\n",
    "        if s + chunk_tokens >= len(ids):\n",
    "            break\n",
    "    return chunks\n",
    "\n",
    "def summarize_long(text: str) -> str:\n",
    "    \"\"\"긴 문서: 조각 요약(짧게) → 메타 요약(최종 길이 타깃)\"\"\"\n",
    "    parts = chunk_by_tokens(text)\n",
    "    if not parts:\n",
    "        return \"\"\n",
    "    # 1차: 조각별 요약 (조각은 짧게 압축)\n",
    "    part_sums = []\n",
    "    for i in range(0, len(parts), BATCH_SIZE):\n",
    "        part_sums += summarize_batch(parts[i:i + BATCH_SIZE],\n",
    "                                     min_new_tokens=CHUNK_MIN_NEW,\n",
    "                                     max_new_tokens=CHUNK_MAX_NEW)\n",
    "    # 2차: 요약들의 요약(최종 길이 타깃으로 재요약)\n",
    "    final = summarize_batch([\" \".join(part_sums)],\n",
    "                            min_new_tokens=FINAL_MIN_NEW,\n",
    "                            max_new_tokens=FINAL_MAX_NEW)[0]\n",
    "    return final\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 3) 데이터 로드\n",
    "# ───────────────────────────────\n",
    "df = pd.read_csv(IN_CSV, encoding=\"utf-8\")\n",
    "assert \"text\" in df.columns, \"입력 CSV에 'text' 컬럼이 필요합니다.\"\n",
    "df[\"text\"] = df[\"text\"].fillna(\"\")\n",
    "\n",
    "print(f\"📂 Loaded {len(df):,} rows\")\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 4) 512토큰 초과 여부 측정\n",
    "# ───────────────────────────────\n",
    "enc = tok(df[\"text\"].tolist(), truncation=False, padding=False)\n",
    "df[\"tok_len\"]   = [len(x) for x in enc[\"input_ids\"]]\n",
    "df[\"truncated\"] = df[\"tok_len\"] > MAX_INPUT_LEN\n",
    "print(f\"⚙️  >512 tokens: {df['truncated'].mean():.1%}\")\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 5) 요약 실행 (최종 80~110 '단어' 토큰 타깃)\n",
    "#  - 짧은 문서(≤512): 길이 타깃으로 배치 요약\n",
    "#  - 긴 문서(>512):  슬라이딩 윈도우 → 메타 요약(길이 타깃)\n",
    "# ───────────────────────────────\n",
    "summaries = [\"\"] * len(df)\n",
    "\n",
    "# 5-a) 짧은 문서 일괄 처리\n",
    "short_idx = df.index[~df[\"truncated\"]].tolist()\n",
    "for i in tqdm(range(0, len(short_idx), BATCH_SIZE), desc=\"Short docs\"):\n",
    "    batch_ids = short_idx[i:i + BATCH_SIZE]\n",
    "    texts = df.loc[batch_ids, \"text\"].tolist()\n",
    "    outs = summarize_batch(texts,\n",
    "                           min_new_tokens=FINAL_MIN_NEW,\n",
    "                           max_new_tokens=FINAL_MAX_NEW)\n",
    "    for j, idx in enumerate(batch_ids):\n",
    "        summaries[idx] = outs[j]\n",
    "\n",
    "# 5-b) 긴 문서 개별 처리 (품질 우선)\n",
    "long_idx = df.index[df[\"truncated\"]].tolist()\n",
    "for idx in tqdm(long_idx, desc=\"Long docs\"):\n",
    "    summaries[idx] = summarize_long(df.at[idx, \"text\"])\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 6) 저장\n",
    "# ───────────────────────────────\n",
    "df.insert(2, \"summary\", summaries)\n",
    "df = df.drop(columns=[\"text\", \"tok_len\", \"truncated\"])\n",
    "\n",
    "out_path = Path(OUT_CSV)\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"🎉 Saved → {out_path}  ({len(df):,} rows, cols: {list(df.columns)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d960d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚙️ Minimal prep for KR-FinBert\n",
    "import pandas as pd, unicodedata as ud, re\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "IN  = \"/Users/yujimin/KB AI CHALLENGE/project/src/news_summary_20250805_len80_110.csv\"\n",
    "OUT = \"/Users/yujimin/KB AI CHALLENGE/project/src/news_summary_20250805_len80_110_clean.csv\"\n",
    "\n",
    "df = pd.read_csv(IN)\n",
    "assert set([\"stock_name\",\"datetime\",\"summary\",\"link\"]).issubset(df.columns)\n",
    "\n",
    "def clean_summary(s: str) -> str:\n",
    "    s = \"\" if pd.isna(s) else str(s)\n",
    "    s = ud.normalize(\"NFKC\", s)          # 폭/조합 정규화\n",
    "    s = s.replace(\"[SEP]\", \" \")          # 혹시 남아있다면 제거\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()   # 공백 정리\n",
    "    if s and not re.search(r'[.!?\\\"”’)\\]]$', s):  # 종결부호 보정(없을 때만)\n",
    "        s += \".\"\n",
    "    return s\n",
    "\n",
    "df[\"summary\"] = df[\"summary\"].apply(clean_summary)\n",
    "\n",
    "# (선택) 링크 기준 중복 제거 – 이미 제거되어 있으면 변화 없음\n",
    "df = df.drop_duplicates(subset=[\"link\"]).reset_index(drop=True)\n",
    "\n",
    "# ✅ 512 토큰 초과 여부 점검(이상치 탐지 용)\n",
    "tok = AutoTokenizer.from_pretrained(\"snunlp/KR-FinBert-SC\")\n",
    "enc = tok(df[\"summary\"].tolist(), truncation=False, padding=False)\n",
    "over512 = sum(len(ids) > 512 for ids in enc[\"input_ids\"])\n",
    "print(f\"Over 512 tokens: {over512} rows\")\n",
    "\n",
    "df.to_csv(OUT, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"Saved → {OUT} ({len(df)} rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ca0696",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "KR-FinBERT-SC 감성 추론\n",
    "입력:  news_summary_20250805_len80_110_clean.csv (stock_name, datetime, summary, link)\n",
    "출력:  news_20250805_sent_scored.csv (neg/neu/pos + pred_label/pred_conf 추가)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# 1) 경로/환경\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "IN_CSV  = \"/content/drive/MyDrive/Invemotion/news_summary_20250805_len80_110_clean.csv\"\n",
    "OUT_CSV = \"/content/drive/MyDrive/Invemotion/news_20250805_sent_scored.csv\"\n",
    "\n",
    "MODEL_NAME = \"snunlp/KR-FinBert-SC\"   # 금융 뉴스 감성 분류 3-클래스 (neg/neu/pos)\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 32                        # T4(16GB) 32~64 권장 / CPU면 8~16로 낮추세요\n",
    "MAX_LEN    = 512\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# 2) 로드\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "print(\"🔄 Loading model/tokenizer…\")\n",
    "tok  = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "if DEVICE == \"cuda\":\n",
    "    model = model.to(dtype=torch.float16)  # VRAM 절감\n",
    "model.eval()\n",
    "\n",
    "# id2label 안전 매핑 (모델 설정에서 직접 가져오기)\n",
    "id2label = {int(i): lab for i, lab in model.config.id2label.items()}\n",
    "label_list = [id2label[i].lower() for i in range(model.config.num_labels)]\n",
    "# 표준화된 열 이름 매핑(모델 라벨명이 대소문자/표기 다를 수 있어 보정)\n",
    "norm = {\"negative\":\"neg\", \"neg\":\"neg\",\n",
    "        \"neutral\":\"neu\",  \"neu\":\"neu\",\n",
    "        \"positive\":\"pos\", \"pos\":\"pos\"}\n",
    "out_cols = [norm.get(l, l) for l in label_list]  # 예: ['neg','neu','pos']\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# 3) 데이터\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "df = pd.read_csv(IN_CSV, encoding=\"utf-8\")\n",
    "assert set([\"stock_name\",\"datetime\",\"summary\",\"link\"]).issubset(df.columns)\n",
    "texts = df[\"summary\"].fillna(\"\").astype(str).tolist()\n",
    "print(f\"📂 Loaded {len(df):,} rows\")\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# 4) 배치 추론\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "def softmax_np(x):\n",
    "    import numpy as np\n",
    "    e = np.exp(x - x.max(axis=1, keepdims=True))\n",
    "    return e / e.sum(axis=1, keepdims=True)\n",
    "\n",
    "probs_all = []\n",
    "pred_idx_all = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for i in tqdm(range(0, len(texts), BATCH_SIZE), desc=\"Scoring\"):\n",
    "        batch = texts[i:i+BATCH_SIZE]\n",
    "        enc = tok(batch, truncation=True, max_length=MAX_LEN,\n",
    "                  padding=True, return_tensors=\"pt\")\n",
    "        # token_type_ids는 모델에 따라 무시되지만 안전하게 제거\n",
    "        enc.pop(\"token_type_ids\", None)\n",
    "        enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "\n",
    "        logits = model(**enc).logits  # [B, 3]\n",
    "        if DEVICE == \"cuda\":\n",
    "            logits = logits.float()   # softmax 위해 fp32로 임시 변환\n",
    "        probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "        pred_idx = probs.argmax(axis=1)\n",
    "        probs_all.append(probs)\n",
    "        pred_idx_all.append(pred_idx)\n",
    "\n",
    "import numpy as np\n",
    "probs_all = np.vstack(probs_all)              # shape [N, 3]\n",
    "pred_idx_all = np.concatenate(pred_idx_all)   # shape [N]\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# 5) 결과 프레임 구성\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# 라벨 순서에 맞춰 열 이름 지정\n",
    "probs_df = pd.DataFrame(probs_all, columns=out_cols)\n",
    "pred_labels = [label_list[i] for i in pred_idx_all]  # 모델 원라벨(소문자)\n",
    "pred_conf   = probs_all.max(axis=1)\n",
    "\n",
    "out = pd.concat([df[[\"stock_name\",\"datetime\",\"summary\",\"link\"]],\n",
    "                 probs_df], axis=1)\n",
    "out[\"pred_label\"] = [norm.get(l, l) for l in pred_labels]  # 'neg/neu/pos'로 통일\n",
    "out[\"pred_conf\"]  = pred_conf\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# 6) 저장\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "out_path = Path(OUT_CSV)\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "out.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"🎉 Saved → {out_path}  (rows={len(out):,})\")\n",
    "print(f\"   Columns: {list(out.columns)}\")\n",
    "print(\"   Label order from model:\", label_list, \"-> mapped to:\", out_cols)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
