{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ad77039",
   "metadata": {},
   "source": [
    "| ë‹¨ê³„             | í•µì‹¬ ë¡œì§                                                                      | ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬                                                          |\n",
    "| -------------- | ---------------------------------------------------------------------------- | ----------------------------------------------------------------- |\n",
    "| ğŸ“¥ **ë¡œë“œ**      | `pd.read_json()`ìœ¼ë¡œ í•œ ì¤„ JSON ë°°ì—´ì„ DataFrameí™”                                   | pandas                                                            |\n",
    "| ğŸ§¹ **ì „ì²˜ë¦¬**     | `str.split('[SEP]')`ë¡œ **ì œëª©/ë³¸ë¬¸ ë¶„ë¦¬**, íŠ¹ìˆ˜ë¬¸ìÂ·ê´‘ê³  ì œê±°,<br>512 í† í° ê´€ë¦¬ *(Truncation ë˜ëŠ” Summary)* | re, pandas                                                        |\n",
    "| âœ‚ï¸ **ìš”ì•½ (ì„ íƒ)** | ë³¸ë¬¸ì´ ê¸¸ë©´ **KoBART summarizer**ë¡œ 3â€“5ë¬¸ì¥ ìš”ì•½                                     | transformers (`gogamza/kobart-summarization`)  |\n",
    "| ğŸ· **ê°ì • ë¶„ë¥˜**   | `snunlp/KR-FinBERT-SC` â†’ Softmax â†’ *pos/neu/neg* í™•ë¥                         | transformers, **torch** ([Hugging Face][2])                       |\n",
    "| ğŸ“Š **ì§‘ê³„**      | `groupby('stock_name').agg()`ë¡œ **ì¢…ëª©ë³„ í‰ê· Â·ë¹„ìœ¨** ê³„ì‚° â†’ ì¼ì¼ Sentiment Index       | pandas                                                            |\n",
    "| ğŸ“¤ **ì €ì¥/ì—°ê³„**   | ê¸°ì‚¬ë‹¨ìœ„ ê²°ê³¼ `news_2025-08-05_sent_scored.jsonl`,<br>ì§‘ê³„ ê²°ê³¼ `daily_sentiment_20250805.csv` | â€”                                                                 |\n",
    "\n",
    "[2]: https://huggingface.co/snunlp/KR-FinBERT-SC \"snunlp/KR-FinBERT-SC on Hugging Face\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f652d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0.0s] ì‹¤í–‰ ë””ë°”ì´ìŠ¤: mps\n",
      "[   0.1s] ë‰´ìŠ¤ ë¡œë“œ ì™„ë£Œ: 728ê±´\n",
      "[   0.1s] ì œëª©/ë³¸ë¬¸ ë¶„ë¦¬ ì™„ë£Œ\n",
      "[   0.1s] KoBART ìš”ì•½ ëª¨ë¸ ë¡œë”© ì¤‘â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n",
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n",
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   3.7s] ë³¸ë¬¸ ìš”ì•½ ì‹œì‘â€¦\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daceedb940d44026b45720a30e6850ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1607.7s] ë³¸ë¬¸ ìš”ì•½ ì™„ë£Œ\n",
      "[1607.7s] FinBERT ëª¨ë¸ ë¡œë”© ì¤‘â€¦\n",
      "[1610.5s] ê°ì • ì ìˆ˜ ì‚°ì¶œ ì¤‘â€¦\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d677ff1f551a4950a31a95b2ef515969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1651.0s] ê°ì • ë¶„ì„ ì™„ë£Œ\n",
      "[1651.0s] ì§‘ê³„ ì™„ë£Œ\n",
      "[1651.1s] íŒŒì¼ ì €ì¥ ì™„ë£Œ â†’ news_2025-08-05_sent_scored.jsonl / daily_sentiment_20250805.csv\n"
     ]
    }
   ],
   "source": [
    "# sentiment_pipeline.py\n",
    "import re, time, json, torch, pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification, pipeline,\n",
    "    BartForConditionalGeneration\n",
    ")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 0) í™˜ê²½ ì„¤ì •\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "DEVICE = \"mps\" \n",
    "# if torch.cuda.is_available() else \"cpu\"\n",
    "# torch.set_num_threads(torch.get_num_threads())           # CPU ë©€í‹°ìŠ¤ë ˆë“œ\n",
    "\n",
    "tqdm.pandas(desc=\"Progress\")                             # pandas + tqdm ê²°í•©\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "def log(msg: str):\n",
    "    elapsed = time.perf_counter() - start_time\n",
    "    print(f\"[{elapsed:6.1f}s] {msg}\")\n",
    "\n",
    "log(f\"ì‹¤í–‰ ë””ë°”ì´ìŠ¤: {DEVICE}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1) ë°ì´í„° ë¡œë“œ\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "INPUT_FILE = Path(\"/Users/yujimin/KB AI CHALLENGE/project/llm_input/news_2025-08-05.json\")\n",
    "df = pd.read_json(INPUT_FILE)\n",
    "log(f\"ë‰´ìŠ¤ ë¡œë“œ ì™„ë£Œ: {len(df):,}ê±´\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2) ì œëª© Â· ë³¸ë¬¸ ë¶„ë¦¬\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def split_text(txt: str):\n",
    "    title, *body = re.split(r'\\s*\\[SEP\\]\\s*', txt, maxsplit=1)\n",
    "    return pd.Series([title.strip(), body[0].strip() if body else \"\"])\n",
    "\n",
    "df[[\"title\", \"body\"]] = df[\"text\"].apply(split_text)\n",
    "log(\"ì œëª©/ë³¸ë¬¸ ë¶„ë¦¬ ì™„ë£Œ\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3) (ì˜µì…˜) KoBART ìš”ì•½\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "USE_SUMMARY = True\n",
    "if USE_SUMMARY:\n",
    "    log(\"KoBART ìš”ì•½ ëª¨ë¸ ë¡œë”© ì¤‘â€¦\")\n",
    "    SUM_MODEL_NAME = \"gogamza/kobart-summarization\"\n",
    "    sum_tok = AutoTokenizer.from_pretrained(SUM_MODEL_NAME)\n",
    "    sum_mod = BartForConditionalGeneration.from_pretrained(SUM_MODEL_NAME).to(DEVICE)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def summarize(txt: str) -> str:\n",
    "        inputs = sum_tok(txt, return_tensors=\"pt\", max_length=1024,\n",
    "                         truncation=True).to(DEVICE)\n",
    "        inputs.pop(\"token_type_ids\", None)   # BARTëŠ” í•„ìš” ì—†ìŒ\n",
    "        summary_ids = sum_mod.generate(**inputs,\n",
    "                                       max_length=128, min_length=40,\n",
    "                                       do_sample=False)\n",
    "        return sum_tok.decode(summary_ids[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    log(\"ë³¸ë¬¸ ìš”ì•½ ì‹œì‘â€¦\")\n",
    "    df[\"body\"] = df[\"body\"].progress_apply(summarize)\n",
    "    log(\"ë³¸ë¬¸ ìš”ì•½ ì™„ë£Œ\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4) KR-FinBERT ê°ì • ë¶„ì„\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "log(\"FinBERT ëª¨ë¸ ë¡œë”© ì¤‘â€¦\")\n",
    "FINBERT_NAME = \"snunlp/KR-FinBERT-SC\"\n",
    "tok = AutoTokenizer.from_pretrained(FINBERT_NAME)\n",
    "mod = AutoModelForSequenceClassification.from_pretrained(FINBERT_NAME).to(DEVICE)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def classify(title: str, body: str):\n",
    "    enc = tok(title, body, truncation=True,\n",
    "              max_length=512, return_tensors=\"pt\").to(DEVICE)\n",
    "    probs = mod(**enc).logits.softmax(-1)[0].cpu().tolist()   # [neg, neu, pos]\n",
    "    return probs\n",
    "\n",
    "def score_row(row):\n",
    "    neg, neu, pos = classify(row.title, row.body)\n",
    "    return pd.Series({\"neg\": neg, \"neu\": neu, \"pos\": pos})\n",
    "\n",
    "log(\"ê°ì • ì ìˆ˜ ì‚°ì¶œ ì¤‘â€¦\")\n",
    "df[[\"neg\", \"neu\", \"pos\"]] = df.progress_apply(score_row, axis=1)\n",
    "log(\"ê°ì • ë¶„ì„ ì™„ë£Œ\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5) ì¢…ëª©ë³„ ì§‘ê³„\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "agg = (df.groupby(\"stock_name\")\n",
    "         .agg(article_cnt=(\"pos\", \"size\"),\n",
    "              pos_mean=(\"pos\", \"mean\"),\n",
    "              neg_mean=(\"neg\", \"mean\"),\n",
    "              neu_mean=(\"neu\", \"mean\"))\n",
    "         .reset_index())\n",
    "log(\"ì§‘ê³„ ì™„ë£Œ\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 6) ê²°ê³¼ ì €ì¥\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# df_out  = Path(\"news_2025-08-05_sent_scored.jsonl\")\n",
    "df_out  = Path(\"news_2025-08-05_sent_scored.csv\")\n",
    "agg_out = Path(\"daily_sentiment_20250805.csv\")\n",
    "\n",
    "# df.to_json(df_out, orient=\"records\", lines=True, force_ascii=False)\n",
    "df.to_csv(df_out, index=False)\n",
    "agg.to_csv(agg_out, index=False)\n",
    "log(f\"íŒŒì¼ ì €ì¥ ì™„ë£Œ â†’ {df_out} / {agg_out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7bf5a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out  = Path(\"news_2025-08-05_sent_scored.csv\")\n",
    "df.to_csv(df_out, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8681d3f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
