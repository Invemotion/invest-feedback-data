{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f19d29e0",
   "metadata": {},
   "source": [
    "| 단계                                            | 목적                 | 이유                                 |\n",
    "| --------------------------------------------- | ------------------ | ---------------------------------- |\n",
    "| 1️⃣ **라이트 정제**<br>(Unicode NFKC, 공백·URL 제거 등) | 요약 입력을 깨끗하게        | KoBART가 HTML 잔여·줄바꿈·URL에 민감 → 품질 ↓ |\n",
    "| 2️⃣ **1차 중복 제거**<br>(`link`·`title+datetime`) | 불필요한 인퍼런스 절감       | 요약·감성 분석 시간은 텍스트 건수에 비례            |\n",
    "| 3️⃣ **KoBART 요약**                             | `summary` 컬럼 생성    | 이후 단계에서 짧은 텍스트 활용 → RAM·CPU 부담↓    |\n",
    "| 4️⃣ **감성 분석**<br>(KB-ALBERT·FinBERT 등)        | `neg/neu/pos` 스코어  | 짧은 summary를 쓰면 분석 속도 2-3배 개선       |\n",
    "| 5️⃣ **최종 CSV 저장**                             | `raw_text`까지 보존 권장 | 추후 디버깅·재학습 대비                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea8ad19",
   "metadata": {},
   "source": [
    "위 절차 중 현재 파일에서는 1,2,3번 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807f67fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코랩에서 실행\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "news_clean_20250805.csv → KoBART 요약(긴 문서 슬라이딩 윈도우) → text 컬럼 제거\n",
    "출력: news_summary_20250805.csv\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 1) 경로·하이퍼파라미터\n",
    "# ───────────────────────────────\n",
    "# Colab 예시\n",
    "IN_CSV  = \"/content/drive/MyDrive/Invemotion/news_clean_20250805.csv\"\n",
    "OUT_CSV = \"/content/drive/MyDrive/Invemotion/news_summary_20250805(2).csv\"\n",
    "\n",
    "# 로컬 맥OS 예시 (원하면 위/아래 중 하나만 사용)\n",
    "# IN_CSV  = \"/Users/yujimin/KB AI CHALLENGE/project/news_clean_20250805.csv\"\n",
    "# OUT_CSV = \"/Users/yujimin/KB AI CHALLENGE/project/news_summary_20250805.csv\"\n",
    "\n",
    "BATCH_SIZE      = 8           # T4(16GB)에서 8~16 권장\n",
    "MAX_INPUT_LEN   = 512\n",
    "MAX_SUMMARY_LEN = 128\n",
    "DEVICE          = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "CHUNK_TOKENS    = 400         # 긴 문서 슬라이딩 윈도우 조각 크기\n",
    "OVERLAP_TOKENS  = 80          # 조각 간 겹침\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 2) 모델 로드\n",
    "# ───────────────────────────────\n",
    "print(\"🔄  Loading KoBART summarizer …\")\n",
    "tok   = AutoTokenizer.from_pretrained(\"digit82/kobart-summarization\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"digit82/kobart-summarization\").to(DEVICE)\n",
    "if DEVICE == \"cuda\":\n",
    "    model = model.to(dtype=torch.float16)  # VRAM 절약\n",
    "model.eval()\n",
    "\n",
    "@torch.inference_mode()\n",
    "def summarize_batch(texts: list[str]) -> list[str]:\n",
    "    \"\"\"KoBART 배치 요약 (token_type_ids 제거)\"\"\"\n",
    "    enc = tok(\n",
    "        texts,\n",
    "        max_length=MAX_INPUT_LEN,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    enc.pop(\"token_type_ids\", None)\n",
    "    enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "\n",
    "    outs = model.generate(\n",
    "        **enc,\n",
    "        max_length=MAX_SUMMARY_LEN,\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "        # 품질 안정 옵션(원하면 해제/조정)\n",
    "        no_repeat_ngram_size=3\n",
    "    )\n",
    "    return tok.batch_decode(outs, skip_special_tokens=True)\n",
    "\n",
    "def chunk_by_tokens(text: str,\n",
    "                    chunk_tokens=CHUNK_TOKENS,\n",
    "                    overlap=OVERLAP_TOKENS) -> list[str]:\n",
    "    \"\"\"토큰 단위로 슬라이싱하여 부분 텍스트 리스트 반환\"\"\"\n",
    "    ids = tok.encode(text, add_special_tokens=True)\n",
    "    chunks = []\n",
    "    step = max(1, chunk_tokens - overlap)\n",
    "    for s in range(0, len(ids), step):\n",
    "        piece = ids[s:s + chunk_tokens]\n",
    "        if not piece:\n",
    "            break\n",
    "        chunks.append(tok.decode(piece, skip_special_tokens=True))\n",
    "        if s + chunk_tokens >= len(ids):\n",
    "            break\n",
    "    return chunks\n",
    "\n",
    "def summarize_long(text: str) -> str:\n",
    "    \"\"\"긴 문서: 조각별 요약 → 메타 요약(map-reduce)\"\"\"\n",
    "    parts = chunk_by_tokens(text)\n",
    "    if not parts:\n",
    "        return \"\"\n",
    "    # 1차: 조각 요약 (배치 처리)\n",
    "    part_sums = []\n",
    "    for i in range(0, len(parts), BATCH_SIZE):\n",
    "        part_sums += summarize_batch(parts[i:i + BATCH_SIZE])\n",
    "    # 2차: 요약들의 요약\n",
    "    final = summarize_batch([\" \".join(part_sums)])[0]\n",
    "    return final\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 3) 데이터 로드\n",
    "# ───────────────────────────────\n",
    "df = pd.read_csv(IN_CSV, encoding=\"utf-8\")\n",
    "assert \"text\" in df.columns, \"입력 CSV에 'text' 컬럼이 필요합니다.\"\n",
    "df[\"text\"] = df[\"text\"].fillna(\"\")\n",
    "\n",
    "print(f\"📂 Loaded {len(df):,} rows\")\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 4) 512토큰 초과 여부 측정\n",
    "# ───────────────────────────────\n",
    "enc = tok(df[\"text\"].tolist(), truncation=False, padding=False)\n",
    "df[\"tok_len\"]   = [len(x) for x in enc[\"input_ids\"]]\n",
    "df[\"truncated\"] = df[\"tok_len\"] > MAX_INPUT_LEN\n",
    "print(f\"⚙️  >512 tokens: {df['truncated'].mean():.1%}\")\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 5) 요약 실행\n",
    "#  - 짧은 문서(≤512): 배치 요약\n",
    "#  - 긴 문서(>512):  슬라이딩 윈도우 요약\n",
    "# ───────────────────────────────\n",
    "summaries = [\"\"] * len(df)\n",
    "\n",
    "# 5-a) 짧은 문서 일괄 처리\n",
    "short_idx = df.index[~df[\"truncated\"]].tolist()\n",
    "for i in tqdm(range(0, len(short_idx), BATCH_SIZE), desc=\"Short docs\"):\n",
    "    batch_ids = short_idx[i:i + BATCH_SIZE]\n",
    "    texts = df.loc[batch_ids, \"text\"].tolist()\n",
    "    outs = summarize_batch(texts)\n",
    "    for j, idx in enumerate(batch_ids):\n",
    "        summaries[idx] = outs[j]\n",
    "\n",
    "# 5-b) 긴 문서 개별 처리 (속도보다 품질 우선)\n",
    "long_idx = df.index[df[\"truncated\"]].tolist()\n",
    "for idx in tqdm(long_idx, desc=\"Long docs\"):\n",
    "    summaries[idx] = summarize_long(df.at[idx, \"text\"])\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 6) 저장\n",
    "# ───────────────────────────────\n",
    "df.insert(2, \"summary\", summaries)\n",
    "df = df.drop(columns=[\"text\", \"tok_len\", \"truncated\"])\n",
    "\n",
    "out_path = Path(OUT_CSV)\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"🎉 Saved → {out_path}  ({len(df):,} rows, cols: {list(df.columns)})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
