{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f19d29e0",
   "metadata": {},
   "source": [
    "| ë‹¨ê³„                                            | ëª©ì                  | ì´ìœ                                  |\n",
    "| --------------------------------------------- | ------------------ | ---------------------------------- |\n",
    "| 1ï¸âƒ£ **ë¼ì´íŠ¸ ì •ì œ**<br>(Unicode NFKC, ê³µë°±Â·URL ì œê±° ë“±) | ìš”ì•½ ì…ë ¥ì„ ê¹¨ë—í•˜ê²Œ        | KoBARTê°€ HTML ì”ì—¬Â·ì¤„ë°”ê¿ˆÂ·URLì— ë¯¼ê° â†’ í’ˆì§ˆ â†“ |\n",
    "| 2ï¸âƒ£ **1ì°¨ ì¤‘ë³µ ì œê±°**<br>(`link`Â·`title+datetime`) | ë¶ˆí•„ìš”í•œ ì¸í¼ëŸ°ìŠ¤ ì ˆê°       | ìš”ì•½Â·ê°ì„± ë¶„ì„ ì‹œê°„ì€ í…ìŠ¤íŠ¸ ê±´ìˆ˜ì— ë¹„ë¡€            |\n",
    "| 3ï¸âƒ£ **KoBART ìš”ì•½**                             | `summary` ì»¬ëŸ¼ ìƒì„±    | ì´í›„ ë‹¨ê³„ì—ì„œ ì§§ì€ í…ìŠ¤íŠ¸ í™œìš© â†’ RAMÂ·CPU ë¶€ë‹´â†“    |\n",
    "| 4ï¸âƒ£ **ê°ì„± ë¶„ì„**<br>(KB-ALBERTÂ·FinBERT ë“±)        | `neg/neu/pos` ìŠ¤ì½”ì–´  | ì§§ì€ summaryë¥¼ ì“°ë©´ ë¶„ì„ ì†ë„ 2-3ë°° ê°œì„        |\n",
    "| 5ï¸âƒ£ **ìµœì¢… CSV ì €ì¥**                             | `raw_text`ê¹Œì§€ ë³´ì¡´ ê¶Œì¥ | ì¶”í›„ ë””ë²„ê¹…Â·ì¬í•™ìŠµ ëŒ€ë¹„                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea8ad19",
   "metadata": {},
   "source": [
    "ìœ„ ì ˆì°¨ ì¤‘ í˜„ì¬ íŒŒì¼ì—ì„œëŠ” 1,2,3ë²ˆ ìˆ˜í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807f67fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì½”ë©ì—ì„œ ì‹¤í–‰\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "news_clean_20250805.csv â†’ KoBART ìš”ì•½(ê¸´ ë¬¸ì„œ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°) â†’ text ì»¬ëŸ¼ ì œê±°\n",
    "ì¶œë ¥: news_summary_20250805.csv\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1) ê²½ë¡œÂ·í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Colab ì˜ˆì‹œ\n",
    "IN_CSV  = \"/content/drive/MyDrive/Invemotion/news_clean_20250805.csv\"\n",
    "OUT_CSV = \"/content/drive/MyDrive/Invemotion/news_summary_20250805(2).csv\"\n",
    "\n",
    "# ë¡œì»¬ ë§¥OS ì˜ˆì‹œ (ì›í•˜ë©´ ìœ„/ì•„ë˜ ì¤‘ í•˜ë‚˜ë§Œ ì‚¬ìš©)\n",
    "# IN_CSV  = \"/Users/yujimin/KB AI CHALLENGE/project/news_clean_20250805.csv\"\n",
    "# OUT_CSV = \"/Users/yujimin/KB AI CHALLENGE/project/news_summary_20250805.csv\"\n",
    "\n",
    "BATCH_SIZE      = 8           # T4(16GB)ì—ì„œ 8~16 ê¶Œì¥\n",
    "MAX_INPUT_LEN   = 512\n",
    "MAX_SUMMARY_LEN = 128\n",
    "DEVICE          = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "CHUNK_TOKENS    = 400         # ê¸´ ë¬¸ì„œ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì¡°ê° í¬ê¸°\n",
    "OVERLAP_TOKENS  = 80          # ì¡°ê° ê°„ ê²¹ì¹¨\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2) ëª¨ë¸ ë¡œë“œ\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ğŸ”„  Loading KoBART summarizer â€¦\")\n",
    "tok   = AutoTokenizer.from_pretrained(\"digit82/kobart-summarization\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"digit82/kobart-summarization\").to(DEVICE)\n",
    "if DEVICE == \"cuda\":\n",
    "    model = model.to(dtype=torch.float16)  # VRAM ì ˆì•½\n",
    "model.eval()\n",
    "\n",
    "@torch.inference_mode()\n",
    "def summarize_batch(texts: list[str]) -> list[str]:\n",
    "    \"\"\"KoBART ë°°ì¹˜ ìš”ì•½ (token_type_ids ì œê±°)\"\"\"\n",
    "    enc = tok(\n",
    "        texts,\n",
    "        max_length=MAX_INPUT_LEN,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    enc.pop(\"token_type_ids\", None)\n",
    "    enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "\n",
    "    outs = model.generate(\n",
    "        **enc,\n",
    "        max_length=MAX_SUMMARY_LEN,\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "        # í’ˆì§ˆ ì•ˆì • ì˜µì…˜(ì›í•˜ë©´ í•´ì œ/ì¡°ì •)\n",
    "        no_repeat_ngram_size=3\n",
    "    )\n",
    "    return tok.batch_decode(outs, skip_special_tokens=True)\n",
    "\n",
    "def chunk_by_tokens(text: str,\n",
    "                    chunk_tokens=CHUNK_TOKENS,\n",
    "                    overlap=OVERLAP_TOKENS) -> list[str]:\n",
    "    \"\"\"í† í° ë‹¨ìœ„ë¡œ ìŠ¬ë¼ì´ì‹±í•˜ì—¬ ë¶€ë¶„ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜\"\"\"\n",
    "    ids = tok.encode(text, add_special_tokens=True)\n",
    "    chunks = []\n",
    "    step = max(1, chunk_tokens - overlap)\n",
    "    for s in range(0, len(ids), step):\n",
    "        piece = ids[s:s + chunk_tokens]\n",
    "        if not piece:\n",
    "            break\n",
    "        chunks.append(tok.decode(piece, skip_special_tokens=True))\n",
    "        if s + chunk_tokens >= len(ids):\n",
    "            break\n",
    "    return chunks\n",
    "\n",
    "def summarize_long(text: str) -> str:\n",
    "    \"\"\"ê¸´ ë¬¸ì„œ: ì¡°ê°ë³„ ìš”ì•½ â†’ ë©”íƒ€ ìš”ì•½(map-reduce)\"\"\"\n",
    "    parts = chunk_by_tokens(text)\n",
    "    if not parts:\n",
    "        return \"\"\n",
    "    # 1ì°¨: ì¡°ê° ìš”ì•½ (ë°°ì¹˜ ì²˜ë¦¬)\n",
    "    part_sums = []\n",
    "    for i in range(0, len(parts), BATCH_SIZE):\n",
    "        part_sums += summarize_batch(parts[i:i + BATCH_SIZE])\n",
    "    # 2ì°¨: ìš”ì•½ë“¤ì˜ ìš”ì•½\n",
    "    final = summarize_batch([\" \".join(part_sums)])[0]\n",
    "    return final\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3) ë°ì´í„° ë¡œë“œ\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df = pd.read_csv(IN_CSV, encoding=\"utf-8\")\n",
    "assert \"text\" in df.columns, \"ì…ë ¥ CSVì— 'text' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\"\n",
    "df[\"text\"] = df[\"text\"].fillna(\"\")\n",
    "\n",
    "print(f\"ğŸ“‚ Loaded {len(df):,} rows\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4) 512í† í° ì´ˆê³¼ ì—¬ë¶€ ì¸¡ì •\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "enc = tok(df[\"text\"].tolist(), truncation=False, padding=False)\n",
    "df[\"tok_len\"]   = [len(x) for x in enc[\"input_ids\"]]\n",
    "df[\"truncated\"] = df[\"tok_len\"] > MAX_INPUT_LEN\n",
    "print(f\"âš™ï¸  >512 tokens: {df['truncated'].mean():.1%}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5) ìš”ì•½ ì‹¤í–‰\n",
    "#  - ì§§ì€ ë¬¸ì„œ(â‰¤512): ë°°ì¹˜ ìš”ì•½\n",
    "#  - ê¸´ ë¬¸ì„œ(>512):  ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìš”ì•½\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "summaries = [\"\"] * len(df)\n",
    "\n",
    "# 5-a) ì§§ì€ ë¬¸ì„œ ì¼ê´„ ì²˜ë¦¬\n",
    "short_idx = df.index[~df[\"truncated\"]].tolist()\n",
    "for i in tqdm(range(0, len(short_idx), BATCH_SIZE), desc=\"Short docs\"):\n",
    "    batch_ids = short_idx[i:i + BATCH_SIZE]\n",
    "    texts = df.loc[batch_ids, \"text\"].tolist()\n",
    "    outs = summarize_batch(texts)\n",
    "    for j, idx in enumerate(batch_ids):\n",
    "        summaries[idx] = outs[j]\n",
    "\n",
    "# 5-b) ê¸´ ë¬¸ì„œ ê°œë³„ ì²˜ë¦¬ (ì†ë„ë³´ë‹¤ í’ˆì§ˆ ìš°ì„ )\n",
    "long_idx = df.index[df[\"truncated\"]].tolist()\n",
    "for idx in tqdm(long_idx, desc=\"Long docs\"):\n",
    "    summaries[idx] = summarize_long(df.at[idx, \"text\"])\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 6) ì €ì¥\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df.insert(2, \"summary\", summaries)\n",
    "df = df.drop(columns=[\"text\", \"tok_len\", \"truncated\"])\n",
    "\n",
    "out_path = Path(OUT_CSV)\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"ğŸ‰ Saved â†’ {out_path}  ({len(df):,} rows, cols: {list(df.columns)})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
