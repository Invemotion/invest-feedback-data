{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd1f5986",
   "metadata": {},
   "source": [
    "# News CSV Preprocessing for KBâ€‘ALBERT\n",
    "This notebook demonstrates a lightweight preprocessing pipeline for Korean stockâ€‘related news data, preparing it for fineâ€‘tuning or inference with the **KBâ€‘ALBERTâ€‘charâ€‘v2** model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888b2e90",
   "metadata": {},
   "source": [
    "## 1. Load raw CSV files\n",
    "Place your raw news CSV files in a directory (e.g. `./data/`). All CSVs must share the schema described in your project:\n",
    "\n",
    "| column | description |\n",
    "| --- | --- |\n",
    "| `stock_name` | Stock ticker name |\n",
    "| `date` | News date `YYYY.MM.DD` |\n",
    "| `title` | News headline |\n",
    "| `source` | Publisher name |\n",
    "| `content` | Article body (plain text) |\n",
    "| `link` | Original article URL |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eebeb016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 CSV file(s)\n",
      "  stock_name          datetime                                       title  \\\n",
      "0      NAVER  2025.08.05 21:11  [ë°ì¼ë¦¬ì•ˆ ì˜¤ëŠ˜ë‰´ìŠ¤ ì¢…í•©] ì´ì¶˜ì„ 'ì£¼ì‹ ì°¨ëª…ê±°ë˜ ì˜í˜¹'â€¦ë‚œì²˜í•œ èˆ‡ \"...   \n",
      "1      NAVER  2025.08.05 20:41           â€œë‚¯ìµì€ ì–¼êµ´â€ êµ­ê°€ëŒ€í‘œëë‹¤ë”ë‹ˆâ€¦â€˜1400ì–µ ì­íŒŸâ€™ ë˜ ëŒ€ë°•   \n",
      "2      NAVER  2025.08.05 20:23  êµ¬ê¸€ â€œê°€ë¦¼ ì²˜ë¦¬í•´ ë³´ì•ˆ ìš°ë ¤ í•´ì†Œâ€â€¦ì •ë¶€, ì •ë°€ ì§€ë„ ë°˜ì¶œ ì´ë²ˆì—”.....   \n",
      "3      NAVER  2025.08.05 16:08   êµ¬ê¸€, â€˜ì§€ë„ ë°˜ì¶œâ€™ ë…¼ë€ì— â€œíë¦¿í•˜ê²Œ ì²˜ë¦¬ëœ êµ­ë‚´ ìœ„ì„± ì‚¬ì§„ êµ¬ë§¤....   \n",
      "4      NAVER  2025.08.05 14:50         êµ¬ê¸€, 'ê°€ë¦¼ ì²˜ë¦¬' êµ­ë‚´ ìœ„ì„±ì‚¬ì§„ êµ¬ë§¤ ê²€í† â€¦ë³´ì•ˆìš°ë ¤ ì˜ì‹í–ˆë‚˜   \n",
      "\n",
      "  source                                            content  \\\n",
      "0   ë°ì¼ë¦¬ì•ˆ  ì´ì¶˜ì„ êµ­íšŒ ë²•ì œì‚¬ë²•ìœ„ì›ì¥ì´ ì§€ë‚œ 1ì¼ ì˜¤í›„ êµ­íšŒì—ì„œ ì—´ë¦° ë²•ì œì‚¬ë²•ìœ„ì›íšŒ ì „ì²´íšŒì˜ ...   \n",
      "1  í—¤ëŸ´ë“œê²½ì œ  ê¹€ì„±í›ˆ ì—…ìŠ¤í…Œì´ì§€ ëŒ€í‘œ. [ì—…ìŠ¤í…Œì´ì§€ ìœ íŠœë¸Œ ê°ˆë¬´ë¦¬][í—¤ëŸ´ë“œê²½ì œ=ê¶Œì œì¸ ê¸°ì] â€œìŠ¤...   \n",
      "2   ê²½í–¥ì‹ ë¬¸  ì„¸ ì°¨ë¡€ 1:5000 ë°ì´í„° ìš”ì²­ì´ë²ˆì—” ë³´ì•ˆ ì‹œì„¤ ë“± íë¦¿í•œêµ­ë‚´ ìœ„ì„± ì‚¬ì§„ êµ¬ë§¤ì•ˆ ...   \n",
      "3   ê²½í–¥ì‹ ë¬¸  ë¡œì´í„°ì—°í•©ë‰´ìŠ¤êµ¬ê¸€ì´ ì •ë¶€ì˜ ì •ë°€ ì§€ë„ ë°˜ì¶œ ì—¬ë¶€ ê²°ì •ì„ ì•ë‘ê³  ë³´ì•ˆì‹œì„¤ ë“±ì„ íë¦¿í•˜...   \n",
      "4    ë‰´ìŠ¤1  ì •ë¶€ ë°˜ì¶œ ê²°ì • ì•ë‘ê³  \"ìš”êµ¬ì‚¬í•­ ì´í–‰ ë°©ì•ˆ ê¸´ë°€íˆ í˜‘ì˜ ì¤‘\"êµ¬ê¸€ \"1:5000ì€ ...   \n",
      "\n",
      "                                                link  \n",
      "0  https://finance.naver.com/item/news_read.naver...  \n",
      "1  https://finance.naver.com/item/news_read.naver...  \n",
      "2  https://finance.naver.com/item/news_read.naver...  \n",
      "3  https://finance.naver.com/item/news_read.naver...  \n",
      "4  https://finance.naver.com/item/news_read.naver...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, glob\n",
    "\n",
    "def load_csvs(path_pattern='/Users/yujimin/KB AI CHALLENGE/project/results/*.csv'):\n",
    "    files = glob.glob(path_pattern)\n",
    "    print(f'Found {len(files)} CSV file(s)')\n",
    "    return pd.concat((pd.read_csv(f, encoding='utf-8') for f in files), ignore_index=True)\n",
    "\n",
    "raw_df = load_csvs()\n",
    "print(raw_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177ecb48",
   "metadata": {},
   "source": [
    "## 2. Minimal textâ€‘level cleaning\n",
    "- **Drop rows** with missing `title` or `content`.\n",
    "- **Remove duplicates** based on `link`.\n",
    "- **Normalize whitespace & Unicode**, strip URLs and stray special chars.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0fb865c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning: (598, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_name</th>\n",
       "      <th>datetime</th>\n",
       "      <th>text</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NAVER</td>\n",
       "      <td>2025.08.05 21:11</td>\n",
       "      <td>[ë°ì¼ë¦¬ì•ˆ ì˜¤ëŠ˜ë‰´ìŠ¤ ì¢…í•©] ì´ì¶˜ì„ 'ì£¼ì‹ ì°¨ëª…ê±°ë˜ ì˜í˜¹'...ë‚œì²˜í•œ èˆ‡ \"... [...</td>\n",
       "      <td>https://finance.naver.com/item/news_read.naver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NAVER</td>\n",
       "      <td>2025.08.05 20:41</td>\n",
       "      <td>â€œë‚¯ìµì€ ì–¼êµ´â€ êµ­ê°€ëŒ€í‘œëë‹¤ë”ë‹ˆ...â€˜1400ì–µ ì­íŒŸâ€™ ë˜ ëŒ€ë°• [SEP] ê¹€ì„±í›ˆ ...</td>\n",
       "      <td>https://finance.naver.com/item/news_read.naver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NAVER</td>\n",
       "      <td>2025.08.05 20:23</td>\n",
       "      <td>êµ¬ê¸€ â€œê°€ë¦¼ ì²˜ë¦¬í•´ ë³´ì•ˆ ìš°ë ¤ í•´ì†Œâ€...ì •ë¶€, ì •ë°€ ì§€ë„ ë°˜ì¶œ ì´ë²ˆì—”..... [...</td>\n",
       "      <td>https://finance.naver.com/item/news_read.naver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NAVER</td>\n",
       "      <td>2025.08.05 16:08</td>\n",
       "      <td>êµ¬ê¸€, â€˜ì§€ë„ ë°˜ì¶œâ€™ ë…¼ë€ì— â€œíë¦¿í•˜ê²Œ ì²˜ë¦¬ëœ êµ­ë‚´ ìœ„ì„± ì‚¬ì§„ êµ¬ë§¤.... [SEP...</td>\n",
       "      <td>https://finance.naver.com/item/news_read.naver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NAVER</td>\n",
       "      <td>2025.08.05 14:50</td>\n",
       "      <td>êµ¬ê¸€, 'ê°€ë¦¼ ì²˜ë¦¬' êµ­ë‚´ ìœ„ì„±ì‚¬ì§„ êµ¬ë§¤ ê²€í† ...ë³´ì•ˆìš°ë ¤ ì˜ì‹í–ˆë‚˜ [SEP] ì •ë¶€...</td>\n",
       "      <td>https://finance.naver.com/item/news_read.naver...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  stock_name          datetime  \\\n",
       "0      NAVER  2025.08.05 21:11   \n",
       "1      NAVER  2025.08.05 20:41   \n",
       "2      NAVER  2025.08.05 20:23   \n",
       "3      NAVER  2025.08.05 16:08   \n",
       "4      NAVER  2025.08.05 14:50   \n",
       "\n",
       "                                                text  \\\n",
       "0  [ë°ì¼ë¦¬ì•ˆ ì˜¤ëŠ˜ë‰´ìŠ¤ ì¢…í•©] ì´ì¶˜ì„ 'ì£¼ì‹ ì°¨ëª…ê±°ë˜ ì˜í˜¹'...ë‚œì²˜í•œ èˆ‡ \"... [...   \n",
       "1  â€œë‚¯ìµì€ ì–¼êµ´â€ êµ­ê°€ëŒ€í‘œëë‹¤ë”ë‹ˆ...â€˜1400ì–µ ì­íŒŸâ€™ ë˜ ëŒ€ë°• [SEP] ê¹€ì„±í›ˆ ...   \n",
       "2  êµ¬ê¸€ â€œê°€ë¦¼ ì²˜ë¦¬í•´ ë³´ì•ˆ ìš°ë ¤ í•´ì†Œâ€...ì •ë¶€, ì •ë°€ ì§€ë„ ë°˜ì¶œ ì´ë²ˆì—”..... [...   \n",
       "3  êµ¬ê¸€, â€˜ì§€ë„ ë°˜ì¶œâ€™ ë…¼ë€ì— â€œíë¦¿í•˜ê²Œ ì²˜ë¦¬ëœ êµ­ë‚´ ìœ„ì„± ì‚¬ì§„ êµ¬ë§¤.... [SEP...   \n",
       "4  êµ¬ê¸€, 'ê°€ë¦¼ ì²˜ë¦¬' êµ­ë‚´ ìœ„ì„±ì‚¬ì§„ êµ¬ë§¤ ê²€í† ...ë³´ì•ˆìš°ë ¤ ì˜ì‹í–ˆë‚˜ [SEP] ì •ë¶€...   \n",
       "\n",
       "                                                link  \n",
       "0  https://finance.naver.com/item/news_read.naver...  \n",
       "1  https://finance.naver.com/item/news_read.naver...  \n",
       "2  https://finance.naver.com/item/news_read.naver...  \n",
       "3  https://finance.naver.com/item/news_read.naver...  \n",
       "4  https://finance.naver.com/item/news_read.naver...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë¶ˆí•„ìš”í•œ ë…¸ì´ì¦ˆ ê±·ì–´ë‚´ê³  ëª¨ë¸ í•™ìŠµ ì‹ í˜¸ ìµœëŒ€í•œ ë³´ì¡´\n",
    "import re, unicodedata\n",
    "import pandas as pd\n",
    "\n",
    "url_pattern = re.compile(r'https?://\\S+')\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    text = unicodedata.normalize('NFKC', str(text))\n",
    "    text = url_pattern.sub('', text)          # strip URLs\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # collapse whitespace\n",
    "    return text\n",
    "\n",
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.dropna(subset=['title', 'content'])\n",
    "    df = df.drop_duplicates(subset=['link'])\n",
    "    df['title'] = df['title'].apply(normalize)\n",
    "    df['content'] = df['content'].apply(normalize)\n",
    "    # combine title & body for model input\n",
    "    df['text'] = df['title'] + ' [SEP] ' + df['content']\n",
    "    # retain only useful cols\n",
    "    return df[['stock_name', 'datetime', 'text', 'link']]\n",
    "\n",
    "clean_df = preprocess(raw_df)\n",
    "print('After cleaning:', clean_df.shape)\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24fe81e",
   "metadata": {},
   "source": [
    "## 3. Save cleaned corpus\n",
    "Two common formats:\n",
    "- **Parquet** (efficient columnar storage)\n",
    "- **CSV** (for quick inspection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f74ce399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved `news_clean.parquet` and `news_clean.csv`\n"
     ]
    }
   ],
   "source": [
    "# clean_df.to_parquet('news_clean.parquet', index=False)\n",
    "clean_df.to_csv('news_clean.csv', index=False, encoding='utf-8')\n",
    "print('âœ… Saved `news_clean.parquet` and `news_clean.csv`')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebff8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/yujimin/Downloads'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()  # í˜„ì¬ ì‘ì—… ë””ë ‰í„°ë¦¬ ì ˆëŒ€ê²½ë¡œ í™•ì¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4a03a",
   "metadata": {},
   "source": [
    "---\n",
    "### Next steps\n",
    "Load `news_clean.parquet` with **Huggingface Datasets** and tokenize using `AlbertTokenizer`:\n",
    "```python\n",
    "from datasets import Dataset\n",
    "from transformers import AlbertTokenizer\n",
    "\n",
    "ds = Dataset.from_parquet('news_clean.parquet')\n",
    "tokenizer = AlbertTokenizer.from_pretrained('./kb-albert-char-v2')\n",
    "def tok(batch):\n",
    "    return tokenizer(batch['text'], truncation=True, max_length=512)\n",
    "ds = ds.map(tok, batched=True)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb88c8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›ë³¸    : (8066, 6)  rows\n",
      "ì „ì²˜ë¦¬ë³¸: (8065, 4) rows\n",
      "\n",
      "ğŸ—‘ï¸  ì¤‘ë³µ/ê²°ì¸¡ìœ¼ë¡œ ì œê±°ëœ ê¸°ì‚¬ ìˆ˜: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_name</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>source</th>\n",
       "      <th>content</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>í˜„ëŒ€ì°¨</td>\n",
       "      <td>2025.08.07</td>\n",
       "      <td>[ì†ë³´] í˜„ëŒ€ì°¨, ë¯¸êµ­ GMê³¼ ì°¨ëŸ‰ 5ì¢… ê³µë™ ê°œë°œí•œë‹¤</td>\n",
       "      <td>ë””ì§€í„¸íƒ€ì„ìŠ¤</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://finance.naver.com/item/news_read.naver...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    stock_name        date                           title  source content  \\\n",
       "249        í˜„ëŒ€ì°¨  2025.08.07  [ì†ë³´] í˜„ëŒ€ì°¨, ë¯¸êµ­ GMê³¼ ì°¨ëŸ‰ 5ì¢… ê³µë™ ê°œë°œí•œë‹¤  ë””ì§€í„¸íƒ€ì„ìŠ¤     NaN   \n",
       "\n",
       "                                                  link  \n",
       "249  https://finance.naver.com/item/news_read.naver...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœï¸  ì •ê·œí™”ë¡œ ë°”ë€ ì œëª© ìˆ˜: 8065\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>title_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HDí˜„ëŒ€, ç¾ ì•ˆë‘ë¦´ê³¼ ì†ì¡ê³  AI ë¬´ì¸êµ°í•¨ ê°œë°œí•œë‹¤</td>\n",
       "      <td>HDí˜„ëŒ€, ç¾ ì•ˆë‘ë¦´ê³¼ ì†ì¡ê³  AI ë¬´ì¸êµ°í•¨ ê°œë°œí•œë‹¤ [SEP] ì—°í•©ë‰´ìŠ¤HDí˜„ëŒ€ê°€ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'ë² íŠ¸ë‚¨ ì„œì—´ 1ìœ„' ë°©í•œâ€¦ï§¡ëŒ€í†µë ¹ ë§Œì°¬ì— ì¬ê³„ ì´ìˆ˜ë“¤ ì°¸ì„</td>\n",
       "      <td>'ë² íŠ¸ë‚¨ ì„œì—´ 1ìœ„' ë°©í•œ...æëŒ€í†µë ¹ ë§Œì°¬ì— ì¬ê³„ ì´ìˆ˜ë“¤ ì°¸ì„ [SEP] [ì„œìš¸=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ë‹¨ë…] 'ë² íŠ¸ë‚¨ ì„œì—´ 1ìœ„' ë°©í•œ ë§Œì°¬ì— ëŒ€ê¸°ì—… ì´ìˆ˜ë“¤ ì°¸ì„</td>\n",
       "      <td>[ë‹¨ë…] 'ë² íŠ¸ë‚¨ ì„œì—´ 1ìœ„' ë°©í•œ ë§Œì°¬ì— ëŒ€ê¸°ì—… ì´ìˆ˜ë“¤ ì°¸ì„ [SEP] 10ì¼ ë°©...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                title  \\\n",
       "0       HDí˜„ëŒ€, ç¾ ì•ˆë‘ë¦´ê³¼ ì†ì¡ê³  AI ë¬´ì¸êµ°í•¨ ê°œë°œí•œë‹¤   \n",
       "1   'ë² íŠ¸ë‚¨ ì„œì—´ 1ìœ„' ë°©í•œâ€¦ï§¡ëŒ€í†µë ¹ ë§Œì°¬ì— ì¬ê³„ ì´ìˆ˜ë“¤ ì°¸ì„   \n",
       "2  [ë‹¨ë…] 'ë² íŠ¸ë‚¨ ì„œì—´ 1ìœ„' ë°©í•œ ë§Œì°¬ì— ëŒ€ê¸°ì—… ì´ìˆ˜ë“¤ ì°¸ì„   \n",
       "\n",
       "                                         title_clean  \n",
       "0  HDí˜„ëŒ€, ç¾ ì•ˆë‘ë¦´ê³¼ ì†ì¡ê³  AI ë¬´ì¸êµ°í•¨ ê°œë°œí•œë‹¤ [SEP] ì—°í•©ë‰´ìŠ¤HDí˜„ëŒ€ê°€ ...  \n",
       "1  'ë² íŠ¸ë‚¨ ì„œì—´ 1ìœ„' ë°©í•œ...æëŒ€í†µë ¹ ë§Œì°¬ì— ì¬ê³„ ì´ìˆ˜ë“¤ ì°¸ì„ [SEP] [ì„œìš¸=...  \n",
       "2  [ë‹¨ë…] 'ë² íŠ¸ë‚¨ ì„œì—´ 1ìœ„' ë°©í•œ ë§Œì°¬ì— ëŒ€ê¸°ì—… ì´ìˆ˜ë“¤ ì°¸ì„ [SEP] 10ì¼ ë°©...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ì›ë³¸ ë³¸ë¬¸:\n",
      "ì—°í•©ë‰´ìŠ¤HDí˜„ëŒ€ê°€ ë¯¸êµ­ì˜ ì¸ê³µì§€ëŠ¥(AI) ë°©ì‚°ê¸°ì—… ì•ˆë‘ë¦´ ì¸í„°ìŠ¤íŠ¸ë¦¬ì™€ ì†ì¡ê³  í•¨ì • ê°œë°œì— ë‚˜ì„ ë‹¤.  AI í•¨ì • ê¸°ìˆ ì„ í•¨ê»˜ ê°œë°œí•´ í•œë¯¸ ì–‘êµ­ ì‹œì¥ì— ì§„ì¶œí•˜ê² ë‹¤ëŠ” ê³„íšì´ë‹¤.HDí˜„ëŒ€ëŠ” ì•ˆë‘ë¦´ê³¼ ê²½ê¸°ë„ ì„±ë‚¨ HDí˜„ëŒ€ ê¸€ë¡œë²ŒR&amp;Dì„¼í„°(GRC)ì—ì„œ â€˜í•¨ì • ê°œë°œ í˜‘ë ¥ì„ ìœ„í•œ í•©ì˜ê°ì„œ(MOA)â€™ë¥¼ ì²´ê²°í–ˆë‹¤. ì•ì„œ HDí˜„ëŒ€ì™€ ì•ˆë‘ë¦´ì€ ì§€ë‚œ 4ì›” í•¨ì • ê°œë°œ í˜‘ë ¥ì„ìœ„í•œ  ì—…ë¬´í˜‘ì•½(MOU)ë¥¼ ë§ºì—ˆëŠ”ë° ì´ë²ˆ MOAëŠ” í˜‘ë ¥ ë‚´ìš©ì„ ë” êµ¬ì²´í™”í•œ í˜‘ì•½ì´ë‹¤. ì–‘ì‚¬ëŠ” ì´ë²ˆ MOAë¥¼ í†µí•´ HDí˜„ëŒ€ëŠ” AI í•¨ì • ììœ¨í™” ê¸°ìˆ  ë° í•¨ì • ì„¤ê³„Â·ê¸°ìˆ ì„ ì œê³µí•˜ê³  ì•ˆë‘ ...\n",
      "\n",
      "ì •ì œ ë³¸ë¬¸:\n",
      "ì—°í•©ë‰´ìŠ¤HDí˜„ëŒ€ê°€ ë¯¸êµ­ì˜ ì¸ê³µì§€ëŠ¥(AI) ë°©ì‚°ê¸°ì—… ì•ˆë‘ë¦´ ì¸í„°ìŠ¤íŠ¸ë¦¬ì™€ ì†ì¡ê³  í•¨ì • ê°œë°œì— ë‚˜ì„ ë‹¤. AI í•¨ì • ê¸°ìˆ ì„ í•¨ê»˜ ê°œë°œí•´ í•œë¯¸ ì–‘êµ­ ì‹œì¥ì— ì§„ì¶œí•˜ê² ë‹¤ëŠ” ê³„íšì´ë‹¤.HDí˜„ëŒ€ëŠ” ì•ˆë‘ë¦´ê³¼ ê²½ê¸°ë„ ì„±ë‚¨ HDí˜„ëŒ€ ê¸€ë¡œë²ŒR&amp;Dì„¼í„°(GRC)ì—ì„œ â€˜í•¨ì • ê°œë°œ í˜‘ë ¥ì„ ìœ„í•œ í•©ì˜ê°ì„œ(MOA)â€™ë¥¼ ì²´ê²°í–ˆë‹¤. ì•ì„œ HDí˜„ëŒ€ì™€ ì•ˆë‘ë¦´ì€ ì§€ë‚œ 4ì›” í•¨ì • ê°œë°œ í˜‘ë ¥ì„ìœ„í•œ ì—…ë¬´í˜‘ì•½(MOU)ë¥¼ ë§ºì—ˆëŠ”ë° ì´ë²ˆ MOAëŠ” í˜‘ë ¥ ë‚´ìš©ì„ ë” êµ¬ì²´í™”í•œ í˜‘ì•½ì´ë‹¤. ì–‘ì‚¬ëŠ” ì´ë²ˆ MOAë¥¼ í†µí•´ HDí˜„ëŒ€ëŠ” AI í•¨ì • ììœ¨í™” ê¸°ìˆ  ë° í•¨ì • ì„¤ê³„Â·ê¸°ìˆ ì„ ì œê³µí•˜ê³  ì•ˆë‘ë¦´ì€ ...\n"
     ]
    }
   ],
   "source": [
    "# # ì•„ë˜ ì˜ˆì‹œëŠ” ì›ë³¸ CSV ë¬¶ìŒ(./data/*.csv) ê³¼ ì •ì œ ê²°ê³¼(news_clean.csv) ë¥¼ ë¹„êµí•´ \n",
    "# # â€œì¤‘ë³µ ì œê±°Â·í…ìŠ¤íŠ¸ ì •ê·œí™”ê°€ ì‹¤ì œë¡œ ì–´ë–»ê²Œ ë°˜ì˜ëëŠ”ì§€â€ ëª‡ ê°€ì§€ ìƒ˜í”Œì„ ë°”ë¡œ í™•ì¸í•  ìˆ˜ ìˆëŠ” \n",
    "# # íŒë‹¤ìŠ¤ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "# import pandas as pd, glob\n",
    "\n",
    "# # â”€â”€ 0. íŒŒì¼ ê²½ë¡œ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# RAW_PATH   = '/Users/yujimin/KB AI CHALLENGE/project/results/*.csv'      # ì›ë³¸ CSV ëª¨ì•„ë‘” í´ë”\n",
    "# CLEAN_FILE = '/Users/yujimin/KB AI CHALLENGE/project/news_clean.csv'    # ì „ì²˜ë¦¬ ê²°ê³¼\n",
    "\n",
    "# # â”€â”€ 1. ë°ì´í„° ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# orig_df  = pd.concat((pd.read_csv(f, encoding='utf-8') for f in glob.glob(RAW_PATH)),\n",
    "#                      ignore_index=True)\n",
    "# clean_df = pd.read_csv(CLEAN_FILE, encoding='utf-8')\n",
    "\n",
    "# print(f'ì›ë³¸    : {orig_df.shape}  rows')\n",
    "# print(f'ì „ì²˜ë¦¬ë³¸: {clean_df.shape} rows\\n')\n",
    "\n",
    "# # â”€â”€ 2. (ì˜ˆì‹œ A) ì¤‘ë³µìœ¼ë¡œ ì‚­ì œëœ ê¸°ì‚¬ ì‚´í´ë³´ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# dropped_links = set(orig_df['link']) - set(clean_df['link'])\n",
    "# dropped = orig_df[orig_df['link'].isin(dropped_links)]\n",
    "# print(f'ğŸ—‘ï¸  ì¤‘ë³µ/ê²°ì¸¡ìœ¼ë¡œ ì œê±°ëœ ê¸°ì‚¬ ìˆ˜: {len(dropped)}')\n",
    "# display(dropped.head(3))\n",
    "\n",
    "# # â”€â”€ 3. (ì˜ˆì‹œ B) ì œëª©Â·ë³¸ë¬¸ ì •ê·œí™” ì°¨ì´ í™•ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# # clean_df['text'] = \"<title> [SEP] <content>\" í˜•íƒœ â†’ ì œëª©ë§Œ ë¶„ë¦¬\n",
    "# tmp = clean_df[['link', 'text']].copy()\n",
    "# tmp['title_clean'] = tmp['text'].str.split(' [SEP] ').str[0]\n",
    "\n",
    "# merged = orig_df.merge(tmp[['link', 'title_clean']], on='link', how='inner')\n",
    "\n",
    "# diff_title = merged[merged['title'] != merged['title_clean']]\n",
    "# print(f'âœï¸  ì •ê·œí™”ë¡œ ë°”ë€ ì œëª© ìˆ˜: {len(diff_title)}')\n",
    "# display(diff_title[['title', 'title_clean']].head(3))\n",
    "\n",
    "# # â”€â”€ 4. (ì˜ˆì‹œ C) ë³¸ë¬¸ ê³µë°±Â·URL ì œê±° ì˜ˆì‹œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# def show_sample(idx):\n",
    "#     print(\"\\nì›ë³¸ ë³¸ë¬¸:\")\n",
    "#     print(orig_df.loc[idx, 'content'][:300], '...')\n",
    "#     print(\"\\nì •ì œ ë³¸ë¬¸:\")\n",
    "#     print(clean_df.loc[idx, 'text'].split(' [SEP] ')[1][:300], '...')\n",
    "\n",
    "# # ì¤‘ë³µ ì œê±° ì—†ëŠ” ì„ì˜ ê¸°ì‚¬ ì¸ë±ìŠ¤ ì°¾ì•„ì„œ ì‹œì—°\n",
    "# sample_idx = orig_df[~orig_df['link'].isin(dropped_links)].index[0]\n",
    "# show_sample(sample_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66baa739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ì •ê·œí™” í›„ ì œëª© ì™„ì „ ì¼ì¹˜ìœ¨: 100.0%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_norm_orig</th>\n",
       "      <th>title_norm_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title_norm_orig, title_norm_clean]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import pandas as pd, glob, re, html, unicodedata\n",
    "\n",
    "# # â”€â”€ 0. íŒŒì¼ ê²½ë¡œ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# RAW_PATH   = '/Users/yujimin/KB AI CHALLENGE/project/results/*.csv'      # ì›ë³¸ CSV ëª¨ì•„ë‘” í´ë”\n",
    "# CLEAN_FILE = '/Users/yujimin/KB AI CHALLENGE/project/news_clean.csv'    # ì „ì²˜ë¦¬ ê²°ê³¼\n",
    "\n",
    "# orig_df  = pd.concat((pd.read_csv(f) for f in glob.glob(RAW_PATH)), ignore_index=True)\n",
    "# clean_df = pd.read_csv(CLEAN_FILE)\n",
    "\n",
    "# # â”€â”€ 1) link ê¸°ì¤€ inner-join\n",
    "# merged = orig_df[['link', 'title']].merge(clean_df[['link', 'text']], on='link', how='inner')\n",
    "\n",
    "# # â”€â”€ 2) ì œëª© ë¶€ë¶„ë§Œ ì¶”ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# sep_regex = re.compile(r'\\s*\\[\\s*SEP\\s*\\]\\s*', flags=re.IGNORECASE)\n",
    "# merged['title_clean_raw'] = merged['text'].str.split(sep_regex).str[0]\n",
    "\n",
    "# # â”€â”€ 3) ë™ì¼ ê·œì¹™ìœ¼ë¡œ ì–‘ìª½ ì •ê·œí™”\n",
    "# def normalize(t: str) -> str:\n",
    "#     t = unicodedata.normalize('NFKC', str(t))\n",
    "#     t = html.unescape(t)                # &amp; â†’ &\n",
    "#     t = re.sub(r'https?://\\S+', '', t)  # URL ì œê±°\n",
    "#     t = re.sub(r'\\s+', ' ', t).strip()  # ê³µë°± ì¶•ì†Œ\n",
    "#     return t\n",
    "\n",
    "# merged['title_norm_orig']  = merged['title'].apply(normalize)\n",
    "# merged['title_norm_clean'] = merged['title_clean_raw'].apply(normalize)\n",
    "\n",
    "# # â”€â”€ 4) ì¼ì¹˜ìœ¨ ì¬í™•ì¸\n",
    "# match_rate = (merged['title_norm_orig'] == merged['title_norm_clean']).mean()\n",
    "# print(f\"ğŸ” ì •ê·œí™” í›„ ì œëª© ì™„ì „ ì¼ì¹˜ìœ¨: {match_rate:.1%}\")\n",
    "\n",
    "# # ë¶ˆì¼ì¹˜ ìƒ˜í”Œ 3ê±´ë§Œ í™•ì¸\n",
    "# mismatch = merged.query('title_norm_orig != title_norm_clean').head(3)\n",
    "# display(mismatch[['title_norm_orig', 'title_norm_clean']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4794e4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[link] ê¸°ì¤€ ì¤‘ë³µ ê±´ìˆ˜  : 0\n",
      "[title+date] ê¸°ì¤€ ì¤‘ë³µ ê±´ìˆ˜: 2472\n",
      "[ì „ì²´ ì»¬ëŸ¼] ì™„ì „ ì¤‘ë³µ í–‰ìˆ˜ : 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_name</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [stock_name, date, text, link, title]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # clean_df ë‚´ë¶€ â€˜ì¤‘ë³µ í–‰â€™ íƒìƒ‰ ìŠ¤ë‹ˆí«\n",
    "# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# import pandas as pd\n",
    "\n",
    "# # (ê°€ì •) ì´ë¯¸ clean_df DataFrame ì´ ë©”ëª¨ë¦¬ì— ì¡´ì¬\n",
    "# CLEAN_FILE = '/Users/yujimin/KB AI CHALLENGE/project/news_clean.csv'  \n",
    "\n",
    "# # 1) link ê¸°ì¤€ â€• ê°€ì¥ í™•ì‹¤í•œ ì¤‘ë³µ ì²´í¬\n",
    "# dup_by_link = clean_df[clean_df.duplicated(subset=[\"link\"], keep=False)]\n",
    "# print(f\"[link] ê¸°ì¤€ ì¤‘ë³µ ê±´ìˆ˜  : {dup_by_link.shape[0]}\")\n",
    "# # display(dup_by_link.head(3))\n",
    "\n",
    "# # 2) title+date ê¸°ì¤€ â€• ë™ì¼ ê¸°ì‚¬ì¸ë° ë§í¬ê°€ ë°”ë€Œì—ˆì„ ê°€ëŠ¥ì„± íƒì§€\n",
    "# #    text ì»¬ëŸ¼ì—ì„œ ì œëª©ë§Œ ë¶„ë¦¬í•´ ì„ì‹œ ì»¬ëŸ¼ ìƒì„±\n",
    "# clean_df[\"title\"] = clean_df[\"text\"].str.split(r\"\\s*\\[\\s*SEP\\s*\\]\\s*\", n=1).str[0]\n",
    "\n",
    "# dup_by_title_date = clean_df[\n",
    "#     clean_df.duplicated(subset=[\"title\", \"date\"], keep=False)\n",
    "# ]\n",
    "# print(f\"[title+date] ê¸°ì¤€ ì¤‘ë³µ ê±´ìˆ˜: {dup_by_title_date.shape[0]}\")\n",
    "# # display(dup_by_title_date.head(3))\n",
    "\n",
    "# # 3) ì „ì²´ ì»¬ëŸ¼ ì™„ì „ ë™ì¼ í–‰\n",
    "# dup_full = clean_df[clean_df.duplicated(keep=False)]\n",
    "# print(f\"[ì „ì²´ ì»¬ëŸ¼] ì™„ì „ ì¤‘ë³µ í–‰ìˆ˜ : {dup_full.shape[0]}\")\n",
    "# # display(dup_full.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4926ba94",
   "metadata": {},
   "source": [
    "í˜„í™© ì§„ë‹¨\n",
    "- link ê¸°ì¤€ ì¤‘ë³µ 0ê±´ â†’ URLì€ ëª¨ë‘ ê³ ìœ \n",
    "\n",
    "- title + date ê¸°ì¤€ ì¤‘ë³µ 2 472ê±´ â†’ ë™ì¼í•œ ì œëª©ì´ ê°™ì€ ë‚ ì— ì—¬ëŸ¬ URL(ì–¸ë¡ ì‚¬Â·ëª¨ë°”ì¼/PC ë²„ì „ ë“±)ë¡œ ì¡´ì¬\n",
    "\n",
    "- ì „ì²´ 8 065ê±´ ì¤‘ ì•½ 31 %ê°€ â€œì‚¬ì‹¤ìƒ ê°™ì€ ê¸°ì‚¬â€ ì´ë¯€ë¡œ, ëª¨ë¸ í•™ìŠµÂ·ì¶”ë¡  íš¨ìœ¨ì„ ìœ„í•´ ì œê±°ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "655ce500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸  ì¤‘ë³µ ì œê±° ì „: 8,065\n",
      "âš™ï¸  ì¤‘ë³µ ì œê±° í›„: 6,716\n",
      "âœ… Saved â†’ news_clean_dedup.csv\n"
     ]
    }
   ],
   "source": [
    "#  ë³¸ë¬¸ ê¸¸ì´ê°€ ê°€ì¥ ê¸´ ê¸°ì‚¬\tì „ë¬¸(å…¨æ–‡)Â·ëª¨ë°”ì¼/PC í†µí•© ë²„ì „ì¼ ê°€ëŠ¥ì„± â†‘\n",
    "\n",
    "import pandas as pd, re\n",
    "\n",
    "CLEAN_FILE = '/Users/yujimin/KB AI CHALLENGE/project/news_clean.csv'  \n",
    "\n",
    "# 1) ì œëª© ë¶„ë¦¬í•´ ì„ì‹œ ì €ì¥\n",
    "sep_regex = re.compile(r\"\\s*\\[\\s*SEP\\s*\\]\\s*\", flags=re.IGNORECASE)\n",
    "clean_df[\"title\"]   = clean_df[\"text\"].str.split(sep_regex, n=1).str[0]\n",
    "clean_df[\"content\"] = clean_df[\"text\"].str.split(sep_regex, n=1).str[1]\n",
    "\n",
    "# 2) ë³¸ë¬¸ ê¸¸ì´ ê³„ì‚°\n",
    "clean_df[\"len\"] = clean_df[\"content\"].str.len()\n",
    "\n",
    "# 3) len ë‚´ë¦¼ì°¨ìˆœ â†’ ì¤‘ë³µ(title, date) ì œê±° â†’ ì •ë ¬ ë³µì›\n",
    "dedup = (\n",
    "    clean_df\n",
    "    .sort_values(\"len\", ascending=False)            # ê¸¸ì´ ê¸´ ê¸°ì‚¬ ìš°ì„ \n",
    "    .drop_duplicates(subset=[\"title\", \"date\"], keep=\"first\")\n",
    "    .sort_index()                                  # ì›ë˜ ìˆœì„œë¡œ ì •ë ¬(ì„ íƒ)\n",
    "    .drop(columns=[\"title\", \"content\", \"len\"])     # ì„ì‹œ ì»¬ëŸ¼ ì •ë¦¬\n",
    ")\n",
    "\n",
    "print(f\"âš™ï¸  ì¤‘ë³µ ì œê±° ì „: {clean_df.shape[0]:,}\")\n",
    "print(f\"âš™ï¸  ì¤‘ë³µ ì œê±° í›„: {dedup.shape[0]:,}\")\n",
    "\n",
    "# 4) ì €ì¥\n",
    "dedup.to_csv(\"news_clean_dedup.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"âœ… Saved â†’ news_clean_dedup.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39775d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_name</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>í˜„ëŒ€ì°¨</td>\n",
       "      <td>2025.08.07</td>\n",
       "      <td>HDí˜„ëŒ€, ç¾ ì•ˆë‘ë¦´ê³¼ ì†ì¡ê³  AI ë¬´ì¸êµ°í•¨ ê°œë°œí•œë‹¤ [SEP] ì—°í•©ë‰´ìŠ¤HDí˜„ëŒ€ê°€ ...</td>\n",
       "      <td>https://finance.naver.com/item/news_read.naver...</td>\n",
       "      <td>HDí˜„ëŒ€, ç¾ ì•ˆë‘ë¦´ê³¼ ì†ì¡ê³  AI ë¬´ì¸êµ°í•¨ ê°œë°œí•œë‹¤</td>\n",
       "      <td>ì—°í•©ë‰´ìŠ¤HDí˜„ëŒ€ê°€ ë¯¸êµ­ì˜ ì¸ê³µì§€ëŠ¥(AI) ë°©ì‚°ê¸°ì—… ì•ˆë‘ë¦´ ì¸í„°ìŠ¤íŠ¸ë¦¬ì™€ ì†ì¡ê³  í•¨ì • ...</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>í˜„ëŒ€ì°¨</td>\n",
       "      <td>2025.08.07</td>\n",
       "      <td>'ë² íŠ¸ë‚¨ ì„œì—´ 1ìœ„' ë°©í•œ...æëŒ€í†µë ¹ ë§Œì°¬ì— ì¬ê³„ ì´ìˆ˜ë“¤ ì°¸ì„ [SEP] [ì„œìš¸=...</td>\n",
       "      <td>https://finance.naver.com/item/news_read.naver...</td>\n",
       "      <td>'ë² íŠ¸ë‚¨ ì„œì—´ 1ìœ„' ë°©í•œ...æëŒ€í†µë ¹ ë§Œì°¬ì— ì¬ê³„ ì´ìˆ˜ë“¤ ì°¸ì„</td>\n",
       "      <td>[ì„œìš¸=ë‰´ì‹œìŠ¤] ìµœì§„ì„ ê¸°ì = ì´ì¬ëª… ëŒ€í†µë ¹ì´ 13ì¼ ì„œìš¸ ìš©ì‚° ëŒ€í†µë ¹ì‹¤ì—ì„œ ì—´ë¦°...</td>\n",
       "      <td>841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>í˜„ëŒ€ì°¨</td>\n",
       "      <td>2025.08.07</td>\n",
       "      <td>[ë‹¨ë…] 'ë² íŠ¸ë‚¨ ì„œì—´ 1ìœ„' ë°©í•œ ë§Œì°¬ì— ëŒ€ê¸°ì—… ì´ìˆ˜ë“¤ ì°¸ì„ [SEP] 10ì¼ ë°©...</td>\n",
       "      <td>https://finance.naver.com/item/news_read.naver...</td>\n",
       "      <td>[ë‹¨ë…] 'ë² íŠ¸ë‚¨ ì„œì—´ 1ìœ„' ë°©í•œ ë§Œì°¬ì— ëŒ€ê¸°ì—… ì´ìˆ˜ë“¤ ì°¸ì„</td>\n",
       "      <td>10ì¼ ë°©í•œí•˜ëŠ” ë˜ ëŸ¼ ë² íŠ¸ë‚¨ ê³µì‚°ë‹¹ ì„œê¸°ì¥ì´ ì´í‹€ì— ê±¸ì³ êµ­ë‚´ ì£¼ìš” ëŒ€ê¸°ì—… ì´ìˆ˜ ...</td>\n",
       "      <td>1705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>í˜„ëŒ€ì°¨</td>\n",
       "      <td>2025.08.07</td>\n",
       "      <td>ì†Œë¹„ì¿ í° ë¨¹ê³  ë§ˆì‹œëŠ” ë° ì£¼ë¡œ ì¼ë‹¤...ì†Œìƒê³µì¸ ë§¤ì¶œì¦ëŒ€ë¡œ [SEP] ìŒì‹ì , ë§ˆíŠ¸...</td>\n",
       "      <td>https://finance.naver.com/item/news_read.naver...</td>\n",
       "      <td>ì†Œë¹„ì¿ í° ë¨¹ê³  ë§ˆì‹œëŠ” ë° ì£¼ë¡œ ì¼ë‹¤...ì†Œìƒê³µì¸ ë§¤ì¶œì¦ëŒ€ë¡œ</td>\n",
       "      <td>ìŒì‹ì , ë§ˆíŠ¸Â·ì‹ë£Œí’ˆ, ë³‘ì›Â·ì•½êµ­ ë“± ë§¤ì¸¨ ëŠ˜ì–´ë‚˜2ì£¼ê°„ 5ì¡°7679ì–µì› ì§€ê¸ˆÂ·Â·Â·2...</td>\n",
       "      <td>1274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>í˜„ëŒ€ì°¨</td>\n",
       "      <td>2025.08.07</td>\n",
       "      <td>ë°¥ ë¨¹ê³ , ë³‘ì›ë¹„ ë‚´ê³ .. 'ì†Œë¹„ì¿ í°' 2ì¡° ì› ì–´ë”” ëª°ë ¸ë‚˜ ë´¤ë”ë‹ˆ [SEP] ì œì£¼...</td>\n",
       "      <td>https://finance.naver.com/item/news_read.naver...</td>\n",
       "      <td>ë°¥ ë¨¹ê³ , ë³‘ì›ë¹„ ë‚´ê³ .. 'ì†Œë¹„ì¿ í°' 2ì¡° ì› ì–´ë”” ëª°ë ¸ë‚˜ ë´¤ë”ë‹ˆ</td>\n",
       "      <td>ì œì£¼ì˜ í•œ ë¯¼ìƒíšŒë³µ ì†Œë¹„ì¿ í° ì‚¬ìš© ê°€ëŠ¥ ë§¤ì¥ì „ êµ­ë¯¼ì—ê²Œ ì§€ê¸‰ëœ ë¯¼ìƒíšŒë³µ ì†Œë¹„ì¿ í°ì´ ...</td>\n",
       "      <td>1402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  stock_name        date                                               text  \\\n",
       "0        í˜„ëŒ€ì°¨  2025.08.07  HDí˜„ëŒ€, ç¾ ì•ˆë‘ë¦´ê³¼ ì†ì¡ê³  AI ë¬´ì¸êµ°í•¨ ê°œë°œí•œë‹¤ [SEP] ì—°í•©ë‰´ìŠ¤HDí˜„ëŒ€ê°€ ...   \n",
       "1        í˜„ëŒ€ì°¨  2025.08.07  'ë² íŠ¸ë‚¨ ì„œì—´ 1ìœ„' ë°©í•œ...æëŒ€í†µë ¹ ë§Œì°¬ì— ì¬ê³„ ì´ìˆ˜ë“¤ ì°¸ì„ [SEP] [ì„œìš¸=...   \n",
       "2        í˜„ëŒ€ì°¨  2025.08.07  [ë‹¨ë…] 'ë² íŠ¸ë‚¨ ì„œì—´ 1ìœ„' ë°©í•œ ë§Œì°¬ì— ëŒ€ê¸°ì—… ì´ìˆ˜ë“¤ ì°¸ì„ [SEP] 10ì¼ ë°©...   \n",
       "3        í˜„ëŒ€ì°¨  2025.08.07  ì†Œë¹„ì¿ í° ë¨¹ê³  ë§ˆì‹œëŠ” ë° ì£¼ë¡œ ì¼ë‹¤...ì†Œìƒê³µì¸ ë§¤ì¶œì¦ëŒ€ë¡œ [SEP] ìŒì‹ì , ë§ˆíŠ¸...   \n",
       "4        í˜„ëŒ€ì°¨  2025.08.07  ë°¥ ë¨¹ê³ , ë³‘ì›ë¹„ ë‚´ê³ .. 'ì†Œë¹„ì¿ í°' 2ì¡° ì› ì–´ë”” ëª°ë ¸ë‚˜ ë´¤ë”ë‹ˆ [SEP] ì œì£¼...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://finance.naver.com/item/news_read.naver...   \n",
       "1  https://finance.naver.com/item/news_read.naver...   \n",
       "2  https://finance.naver.com/item/news_read.naver...   \n",
       "3  https://finance.naver.com/item/news_read.naver...   \n",
       "4  https://finance.naver.com/item/news_read.naver...   \n",
       "\n",
       "                                   title  \\\n",
       "0          HDí˜„ëŒ€, ç¾ ì•ˆë‘ë¦´ê³¼ ì†ì¡ê³  AI ë¬´ì¸êµ°í•¨ ê°œë°œí•œë‹¤   \n",
       "1    'ë² íŠ¸ë‚¨ ì„œì—´ 1ìœ„' ë°©í•œ...æëŒ€í†µë ¹ ë§Œì°¬ì— ì¬ê³„ ì´ìˆ˜ë“¤ ì°¸ì„   \n",
       "2     [ë‹¨ë…] 'ë² íŠ¸ë‚¨ ì„œì—´ 1ìœ„' ë°©í•œ ë§Œì°¬ì— ëŒ€ê¸°ì—… ì´ìˆ˜ë“¤ ì°¸ì„   \n",
       "3       ì†Œë¹„ì¿ í° ë¨¹ê³  ë§ˆì‹œëŠ” ë° ì£¼ë¡œ ì¼ë‹¤...ì†Œìƒê³µì¸ ë§¤ì¶œì¦ëŒ€ë¡œ   \n",
       "4  ë°¥ ë¨¹ê³ , ë³‘ì›ë¹„ ë‚´ê³ .. 'ì†Œë¹„ì¿ í°' 2ì¡° ì› ì–´ë”” ëª°ë ¸ë‚˜ ë´¤ë”ë‹ˆ   \n",
       "\n",
       "                                             content   len  \n",
       "0  ì—°í•©ë‰´ìŠ¤HDí˜„ëŒ€ê°€ ë¯¸êµ­ì˜ ì¸ê³µì§€ëŠ¥(AI) ë°©ì‚°ê¸°ì—… ì•ˆë‘ë¦´ ì¸í„°ìŠ¤íŠ¸ë¦¬ì™€ ì†ì¡ê³  í•¨ì • ...  1200  \n",
       "1  [ì„œìš¸=ë‰´ì‹œìŠ¤] ìµœì§„ì„ ê¸°ì = ì´ì¬ëª… ëŒ€í†µë ¹ì´ 13ì¼ ì„œìš¸ ìš©ì‚° ëŒ€í†µë ¹ì‹¤ì—ì„œ ì—´ë¦°...   841  \n",
       "2  10ì¼ ë°©í•œí•˜ëŠ” ë˜ ëŸ¼ ë² íŠ¸ë‚¨ ê³µì‚°ë‹¹ ì„œê¸°ì¥ì´ ì´í‹€ì— ê±¸ì³ êµ­ë‚´ ì£¼ìš” ëŒ€ê¸°ì—… ì´ìˆ˜ ...  1705  \n",
       "3  ìŒì‹ì , ë§ˆíŠ¸Â·ì‹ë£Œí’ˆ, ë³‘ì›Â·ì•½êµ­ ë“± ë§¤ì¸¨ ëŠ˜ì–´ë‚˜2ì£¼ê°„ 5ì¡°7679ì–µì› ì§€ê¸ˆÂ·Â·Â·2...  1274  \n",
       "4  ì œì£¼ì˜ í•œ ë¯¼ìƒíšŒë³µ ì†Œë¹„ì¿ í° ì‚¬ìš© ê°€ëŠ¥ ë§¤ì¥ì „ êµ­ë¯¼ì—ê²Œ ì§€ê¸‰ëœ ë¯¼ìƒíšŒë³µ ì†Œë¹„ì¿ í°ì´ ...  1402  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e5c5927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“… ì¢…ëª©ë³„ ë‚ ì§œ ë²”ìœ„Â·ê¸°ì‚¬ ìˆ˜\n",
      "stock_name      start        end  days  articles\n",
      "     NAVER 2025-07-22 2025-08-07    17      1664\n",
      "      ì‚¼ì„±ì „ì 2025-07-31 2025-08-07     8      1680\n",
      "       ì¹´ì¹´ì˜¤ 2025-07-07 2025-08-07    32      1710\n",
      "       í˜„ëŒ€ì°¨ 2025-07-28 2025-08-07    11      1662\n",
      "\n",
      "ğŸ“° NAVER â€“ ê¸°ì‚¬ ìˆ˜ ìƒìœ„ 5ì¼\n",
      "stock_name       date  articles\n",
      "     NAVER 2025-08-04       238\n",
      "     NAVER 2025-08-05       206\n",
      "     NAVER 2025-07-23       161\n",
      "     NAVER 2025-07-24       161\n",
      "     NAVER 2025-07-30       124\n",
      "\n",
      "ğŸ“° ì‚¼ì„±ì „ì â€“ ê¸°ì‚¬ ìˆ˜ ìƒìœ„ 5ì¼\n",
      "stock_name       date  articles\n",
      "      ì‚¼ì„±ì „ì 2025-07-31       286\n",
      "      ì‚¼ì„±ì „ì 2025-08-07       285\n",
      "      ì‚¼ì„±ì „ì 2025-08-01       257\n",
      "      ì‚¼ì„±ì „ì 2025-08-05       251\n",
      "      ì‚¼ì„±ì „ì 2025-08-06       222\n",
      "\n",
      "ğŸ“° ì¹´ì¹´ì˜¤ â€“ ê¸°ì‚¬ ìˆ˜ ìƒìœ„ 5ì¼\n",
      "stock_name       date  articles\n",
      "       ì¹´ì¹´ì˜¤ 2025-08-07       151\n",
      "       ì¹´ì¹´ì˜¤ 2025-08-05       116\n",
      "       ì¹´ì¹´ì˜¤ 2025-07-21       103\n",
      "       ì¹´ì¹´ì˜¤ 2025-07-14       101\n",
      "       ì¹´ì¹´ì˜¤ 2025-07-09        88\n",
      "\n",
      "ğŸ“° í˜„ëŒ€ì°¨ â€“ ê¸°ì‚¬ ìˆ˜ ìƒìœ„ 5ì¼\n",
      "stock_name       date  articles\n",
      "       í˜„ëŒ€ì°¨ 2025-07-31       314\n",
      "       í˜„ëŒ€ì°¨ 2025-07-30       253\n",
      "       í˜„ëŒ€ì°¨ 2025-08-07       187\n",
      "       í˜„ëŒ€ì°¨ 2025-07-29       154\n",
      "       í˜„ëŒ€ì°¨ 2025-08-04       152\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# â”€â”€ 0) CSV ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df = pd.read_csv(\"/Users/yujimin/KB AI CHALLENGE/project/news_clean_dedup.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# â”€â”€ 1) date ì»¬ëŸ¼ì„ ë‚ ì§œí˜•ìœ¼ë¡œ ë³€í™˜ â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y.%m.%d\")\n",
    "\n",
    "# â”€â”€ 2) ì¢…ëª©ë³„ ë‚ ì§œ ë²”ìœ„ ìš”ì•½ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "range_tbl = (\n",
    "    df.groupby(\"stock_name\")[\"date\"]\n",
    "      .agg(start=\"min\", end=\"max\", days=\"nunique\", articles=\"count\")\n",
    "      .reset_index()\n",
    "      .sort_values(\"stock_name\")\n",
    ")\n",
    "print(\"ğŸ“… ì¢…ëª©ë³„ ë‚ ì§œ ë²”ìœ„Â·ê¸°ì‚¬ ìˆ˜\")\n",
    "print(range_tbl.to_string(index=False))\n",
    "\n",
    "# 1) ì¢…ëª©-ë‚ ì§œë³„ ê¸°ì‚¬ ìˆ˜ ì§‘ê³„\n",
    "cnt = (\n",
    "    df.groupby([\"stock_name\", \"date\"])\n",
    "      .size()\n",
    "      .reset_index(name=\"articles\")\n",
    ")\n",
    "\n",
    "# 2) ì¢…ëª©ë³„ ìƒìœ„ 5ì¼ ì¶”ì¶œ\n",
    "top5_each = (\n",
    "    cnt.sort_values([\"stock_name\", \"articles\"], ascending=[True, False])\n",
    "       .groupby(\"stock_name\")\n",
    "       .head(5)\n",
    ")\n",
    "\n",
    "# 3) ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥\n",
    "for stock, sub in top5_each.groupby(\"stock_name\"):\n",
    "    print(f\"\\nğŸ“° {stock} â€“ ê¸°ì‚¬ ìˆ˜ ìƒìœ„ 5ì¼\")\n",
    "    print(sub.sort_values(\"articles\", ascending=False)\n",
    "              .reset_index(drop=True)\n",
    "              .to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ae2a729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Found 4 CSV file(s)\n",
      "âœ… After cleaning: (728, 4)\n",
      "ğŸ‰ Saved â†’ /Users/yujimin/KB AI CHALLENGE/project/news_clean_20250805.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Naver Finance ë‰´ìŠ¤ â†’ í†µí•©Â·ì •ì œÂ·ì¤‘ë³µ ì œê±° â†’ news_clean_YYYYMMDD.csv\n",
    "\"\"\"\n",
    "import glob, re, unicodedata, pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. íŒŒì¼ ë¡œë“œ\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def load_csvs(pattern=\"/Users/yujimin/KB AI CHALLENGE/project/results/*.csv\") -> pd.DataFrame:\n",
    "    files = glob.glob(pattern)\n",
    "    print(f\"ğŸ“‚ Found {len(files)} CSV file(s)\")\n",
    "    return pd.concat((pd.read_csv(f, encoding=\"utf-8\") for f in files), ignore_index=True)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2. í…ìŠ¤íŠ¸ ì •ê·œí™”\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "URL_RE = re.compile(r\"https?://\\S+\")\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFKC\", str(text))\n",
    "    text = URL_RE.sub(\"\", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3. ì „ì²˜ë¦¬ & ì¤‘ë³µ ì œê±°\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.dropna(subset=[\"title\", \"content\"]).drop_duplicates(subset=[\"link\"])\n",
    "\n",
    "    df[\"title\"]   = df[\"title\"].apply(normalize)\n",
    "    df[\"content\"] = df[\"content\"].apply(normalize)\n",
    "    df[\"text\"]    = df[\"title\"] + \" [SEP] \" + df[\"content\"]\n",
    "\n",
    "    # (ì„ íƒ) ë™ì¼ ì œëª©Â·ë‚ ì§œ ì¤‘ë³µ ì œê±° â€“ ë³¸ë¬¸ ê¸¸ì´ ê¸´ ê¸°ì‚¬ ìš°ì„ \n",
    "    df[\"len\"] = df[\"content\"].str.len()\n",
    "    df = (\n",
    "        df.sort_values(\"len\", ascending=False)\n",
    "          .drop_duplicates(subset=[\"title\", \"datetime\"], keep=\"first\")\n",
    "          .drop(columns=\"len\")\n",
    "    )\n",
    "    return df[[\"stock_name\", \"datetime\", \"text\", \"link\"]]\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4. ì €ì¥ íŒŒì¼ëª… ê²°ì • ë¡œì§\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def decide_filename(df: pd.DataFrame, out_dir=\"/Users/yujimin/KB AI CHALLENGE/project\") -> str:\n",
    "    # datetime ì»¬ëŸ¼ì—ì„œ â€˜YYYY.MM.DDâ€™ ë¶€ë¶„ë§Œ ì¶”ì¶œ\n",
    "    dates = df[\"datetime\"].astype(str).str.extract(r\"(\\d{4}\\.\\d{2}\\.\\d{2})\")[0]\n",
    "    # ê°€ì¥ ì´ë¥¸ ë‚ ì§œ(earliest) ì„ íƒ â€” í•„ìš” ì‹œ latest/most common ìœ¼ë¡œ êµì²´\n",
    "    target = pd.to_datetime(dates, format=\"%Y.%m.%d\").min().strftime(\"%Y%m%d\")\n",
    "    return f\"{out_dir}/news_clean_{target}.csv\"\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5. ë©”ì¸\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if __name__ == \"__main__\":\n",
    "    raw_df   = load_csvs()\n",
    "    clean_df = preprocess(raw_df)\n",
    "    print(f\"âœ… After cleaning: {clean_df.shape}\")\n",
    "\n",
    "    out_csv = decide_filename(clean_df)\n",
    "    clean_df.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "    print(f\"ğŸ‰ Saved â†’ {out_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a1b50c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
