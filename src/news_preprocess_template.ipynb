{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd1f5986",
   "metadata": {},
   "source": [
    "# News CSV Preprocessing for KB‑ALBERT\n",
    "This notebook demonstrates a lightweight preprocessing pipeline for Korean stock‑related news data, preparing it for fine‑tuning or inference with the **KB‑ALBERT‑char‑v2** model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888b2e90",
   "metadata": {},
   "source": [
    "## 1. Load raw CSV files\n",
    "Place your raw news CSV files in a directory (e.g. `./data/`). All CSVs must share the schema described in your project:\n",
    "\n",
    "| column | description |\n",
    "| --- | --- |\n",
    "| `stock_name` | Stock ticker name |\n",
    "| `date` | News date `YYYY.MM.DD` |\n",
    "| `title` | News headline |\n",
    "| `source` | Publisher name |\n",
    "| `content` | Article body (plain text) |\n",
    "| `link` | Original article URL |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eebeb016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 CSV file(s)\n",
      "  stock_name          datetime                                       title  \\\n",
      "0      NAVER  2025.08.05 21:11  [데일리안 오늘뉴스 종합] 이춘석 '주식 차명거래 의혹'…난처한 與 \"...   \n",
      "1      NAVER  2025.08.05 20:41           “낯익은 얼굴” 국가대표됐다더니…‘1400억 잭팟’ 또 대박   \n",
      "2      NAVER  2025.08.05 20:23  구글 “가림 처리해 보안 우려 해소”…정부, 정밀 지도 반출 이번엔.....   \n",
      "3      NAVER  2025.08.05 16:08   구글, ‘지도 반출’ 논란에 “흐릿하게 처리된 국내 위성 사진 구매....   \n",
      "4      NAVER  2025.08.05 14:50         구글, '가림 처리' 국내 위성사진 구매 검토…보안우려 의식했나   \n",
      "\n",
      "  source                                            content  \\\n",
      "0   데일리안  이춘석 국회 법제사법위원장이 지난 1일 오후 국회에서 열린 법제사법위원회 전체회의 ...   \n",
      "1  헤럴드경제  김성훈 업스테이지 대표. [업스테이지 유튜브 갈무리][헤럴드경제=권제인 기자] “스...   \n",
      "2   경향신문  세 차례 1:5000 데이터 요청이번엔 보안 시설 등 흐릿한국내 위성 사진 구매안 ...   \n",
      "3   경향신문  로이터연합뉴스구글이 정부의 정밀 지도 반출 여부 결정을 앞두고 보안시설 등을 흐릿하...   \n",
      "4    뉴스1  정부 반출 결정 앞두고 \"요구사항 이행 방안 긴밀히 협의 중\"구글 \"1:5000은 ...   \n",
      "\n",
      "                                                link  \n",
      "0  https://finance.naver.com/item/news_read.naver...  \n",
      "1  https://finance.naver.com/item/news_read.naver...  \n",
      "2  https://finance.naver.com/item/news_read.naver...  \n",
      "3  https://finance.naver.com/item/news_read.naver...  \n",
      "4  https://finance.naver.com/item/news_read.naver...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, glob\n",
    "\n",
    "def load_csvs(path_pattern='/Users/yujimin/KB AI CHALLENGE/project/results/*.csv'):\n",
    "    files = glob.glob(path_pattern)\n",
    "    print(f'Found {len(files)} CSV file(s)')\n",
    "    return pd.concat((pd.read_csv(f, encoding='utf-8') for f in files), ignore_index=True)\n",
    "\n",
    "raw_df = load_csvs()\n",
    "print(raw_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177ecb48",
   "metadata": {},
   "source": [
    "## 2. Minimal text‑level cleaning\n",
    "- **Drop rows** with missing `title` or `content`.\n",
    "- **Remove duplicates** based on `link`.\n",
    "- **Normalize whitespace & Unicode**, strip URLs and stray special chars.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0fb865c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning: (598, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_name</th>\n",
       "      <th>datetime</th>\n",
       "      <th>text</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NAVER</td>\n",
       "      <td>2025.08.05 21:11</td>\n",
       "      <td>[데일리안 오늘뉴스 종합] 이춘석 '주식 차명거래 의혹'...난처한 與 \"... [...</td>\n",
       "      <td>https://finance.naver.com/item/news_read.naver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NAVER</td>\n",
       "      <td>2025.08.05 20:41</td>\n",
       "      <td>“낯익은 얼굴” 국가대표됐다더니...‘1400억 잭팟’ 또 대박 [SEP] 김성훈 ...</td>\n",
       "      <td>https://finance.naver.com/item/news_read.naver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NAVER</td>\n",
       "      <td>2025.08.05 20:23</td>\n",
       "      <td>구글 “가림 처리해 보안 우려 해소”...정부, 정밀 지도 반출 이번엔..... [...</td>\n",
       "      <td>https://finance.naver.com/item/news_read.naver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NAVER</td>\n",
       "      <td>2025.08.05 16:08</td>\n",
       "      <td>구글, ‘지도 반출’ 논란에 “흐릿하게 처리된 국내 위성 사진 구매.... [SEP...</td>\n",
       "      <td>https://finance.naver.com/item/news_read.naver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NAVER</td>\n",
       "      <td>2025.08.05 14:50</td>\n",
       "      <td>구글, '가림 처리' 국내 위성사진 구매 검토...보안우려 의식했나 [SEP] 정부...</td>\n",
       "      <td>https://finance.naver.com/item/news_read.naver...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  stock_name          datetime  \\\n",
       "0      NAVER  2025.08.05 21:11   \n",
       "1      NAVER  2025.08.05 20:41   \n",
       "2      NAVER  2025.08.05 20:23   \n",
       "3      NAVER  2025.08.05 16:08   \n",
       "4      NAVER  2025.08.05 14:50   \n",
       "\n",
       "                                                text  \\\n",
       "0  [데일리안 오늘뉴스 종합] 이춘석 '주식 차명거래 의혹'...난처한 與 \"... [...   \n",
       "1  “낯익은 얼굴” 국가대표됐다더니...‘1400억 잭팟’ 또 대박 [SEP] 김성훈 ...   \n",
       "2  구글 “가림 처리해 보안 우려 해소”...정부, 정밀 지도 반출 이번엔..... [...   \n",
       "3  구글, ‘지도 반출’ 논란에 “흐릿하게 처리된 국내 위성 사진 구매.... [SEP...   \n",
       "4  구글, '가림 처리' 국내 위성사진 구매 검토...보안우려 의식했나 [SEP] 정부...   \n",
       "\n",
       "                                                link  \n",
       "0  https://finance.naver.com/item/news_read.naver...  \n",
       "1  https://finance.naver.com/item/news_read.naver...  \n",
       "2  https://finance.naver.com/item/news_read.naver...  \n",
       "3  https://finance.naver.com/item/news_read.naver...  \n",
       "4  https://finance.naver.com/item/news_read.naver...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불필요한 노이즈 걷어내고 모델 학습 신호 최대한 보존\n",
    "import re, unicodedata\n",
    "import pandas as pd\n",
    "\n",
    "url_pattern = re.compile(r'https?://\\S+')\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    text = unicodedata.normalize('NFKC', str(text))\n",
    "    text = url_pattern.sub('', text)          # strip URLs\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # collapse whitespace\n",
    "    return text\n",
    "\n",
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.dropna(subset=['title', 'content'])\n",
    "    df = df.drop_duplicates(subset=['link'])\n",
    "    df['title'] = df['title'].apply(normalize)\n",
    "    df['content'] = df['content'].apply(normalize)\n",
    "    # combine title & body for model input\n",
    "    df['text'] = df['title'] + ' [SEP] ' + df['content']\n",
    "    # retain only useful cols\n",
    "    return df[['stock_name', 'datetime', 'text', 'link']]\n",
    "\n",
    "clean_df = preprocess(raw_df)\n",
    "print('After cleaning:', clean_df.shape)\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24fe81e",
   "metadata": {},
   "source": [
    "## 3. Save cleaned corpus\n",
    "Two common formats:\n",
    "- **Parquet** (efficient columnar storage)\n",
    "- **CSV** (for quick inspection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f74ce399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved `news_clean.parquet` and `news_clean.csv`\n"
     ]
    }
   ],
   "source": [
    "# clean_df.to_parquet('news_clean.parquet', index=False)\n",
    "clean_df.to_csv('news_clean.csv', index=False, encoding='utf-8')\n",
    "print('✅ Saved `news_clean.parquet` and `news_clean.csv`')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebff8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/yujimin/Downloads'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()  # 현재 작업 디렉터리 절대경로 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4a03a",
   "metadata": {},
   "source": [
    "---\n",
    "### Next steps\n",
    "Load `news_clean.parquet` with **Huggingface Datasets** and tokenize using `AlbertTokenizer`:\n",
    "```python\n",
    "from datasets import Dataset\n",
    "from transformers import AlbertTokenizer\n",
    "\n",
    "ds = Dataset.from_parquet('news_clean.parquet')\n",
    "tokenizer = AlbertTokenizer.from_pretrained('./kb-albert-char-v2')\n",
    "def tok(batch):\n",
    "    return tokenizer(batch['text'], truncation=True, max_length=512)\n",
    "ds = ds.map(tok, batched=True)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb88c8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본    : (8066, 6)  rows\n",
      "전처리본: (8065, 4) rows\n",
      "\n",
      "🗑️  중복/결측으로 제거된 기사 수: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_name</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>source</th>\n",
       "      <th>content</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>현대차</td>\n",
       "      <td>2025.08.07</td>\n",
       "      <td>[속보] 현대차, 미국 GM과 차량 5종 공동 개발한다</td>\n",
       "      <td>디지털타임스</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://finance.naver.com/item/news_read.naver...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    stock_name        date                           title  source content  \\\n",
       "249        현대차  2025.08.07  [속보] 현대차, 미국 GM과 차량 5종 공동 개발한다  디지털타임스     NaN   \n",
       "\n",
       "                                                  link  \n",
       "249  https://finance.naver.com/item/news_read.naver...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✏️  정규화로 바뀐 제목 수: 8065\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>title_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HD현대, 美 안두릴과 손잡고 AI 무인군함 개발한다</td>\n",
       "      <td>HD현대, 美 안두릴과 손잡고 AI 무인군함 개발한다 [SEP] 연합뉴스HD현대가 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'베트남 서열 1위' 방한…李대통령 만찬에 재계 총수들 참석</td>\n",
       "      <td>'베트남 서열 1위' 방한...李대통령 만찬에 재계 총수들 참석 [SEP] [서울=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[단독] '베트남 서열 1위' 방한 만찬에 대기업 총수들 참석</td>\n",
       "      <td>[단독] '베트남 서열 1위' 방한 만찬에 대기업 총수들 참석 [SEP] 10일 방...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                title  \\\n",
       "0       HD현대, 美 안두릴과 손잡고 AI 무인군함 개발한다   \n",
       "1   '베트남 서열 1위' 방한…李대통령 만찬에 재계 총수들 참석   \n",
       "2  [단독] '베트남 서열 1위' 방한 만찬에 대기업 총수들 참석   \n",
       "\n",
       "                                         title_clean  \n",
       "0  HD현대, 美 안두릴과 손잡고 AI 무인군함 개발한다 [SEP] 연합뉴스HD현대가 ...  \n",
       "1  '베트남 서열 1위' 방한...李대통령 만찬에 재계 총수들 참석 [SEP] [서울=...  \n",
       "2  [단독] '베트남 서열 1위' 방한 만찬에 대기업 총수들 참석 [SEP] 10일 방...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "원본 본문:\n",
      "연합뉴스HD현대가 미국의 인공지능(AI) 방산기업 안두릴 인터스트리와 손잡고 함정 개발에 나선다.  AI 함정 기술을 함께 개발해 한미 양국 시장에 진출하겠다는 계획이다.HD현대는 안두릴과 경기도 성남 HD현대 글로벌R&amp;D센터(GRC)에서 ‘함정 개발 협력을 위한 합의각서(MOA)’를 체결했다. 앞서 HD현대와 안두릴은 지난 4월 함정 개발 협력을위한  업무협약(MOU)를 맺었는데 이번 MOA는 협력 내용을 더 구체화한 협약이다. 양사는 이번 MOA를 통해 HD현대는 AI 함정 자율화 기술 및 함정 설계·기술을 제공하고 안두 ...\n",
      "\n",
      "정제 본문:\n",
      "연합뉴스HD현대가 미국의 인공지능(AI) 방산기업 안두릴 인터스트리와 손잡고 함정 개발에 나선다. AI 함정 기술을 함께 개발해 한미 양국 시장에 진출하겠다는 계획이다.HD현대는 안두릴과 경기도 성남 HD현대 글로벌R&amp;D센터(GRC)에서 ‘함정 개발 협력을 위한 합의각서(MOA)’를 체결했다. 앞서 HD현대와 안두릴은 지난 4월 함정 개발 협력을위한 업무협약(MOU)를 맺었는데 이번 MOA는 협력 내용을 더 구체화한 협약이다. 양사는 이번 MOA를 통해 HD현대는 AI 함정 자율화 기술 및 함정 설계·기술을 제공하고 안두릴은 ...\n"
     ]
    }
   ],
   "source": [
    "# # 아래 예시는 원본 CSV 묶음(./data/*.csv) 과 정제 결과(news_clean.csv) 를 비교해 \n",
    "# # “중복 제거·텍스트 정규화가 실제로 어떻게 반영됐는지” 몇 가지 샘플을 바로 확인할 수 있는 \n",
    "# # 판다스 스크립트입니다.\n",
    "\n",
    "# import pandas as pd, glob\n",
    "\n",
    "# # ── 0. 파일 경로 설정 ────────────────────────────────────────────────\n",
    "# RAW_PATH   = '/Users/yujimin/KB AI CHALLENGE/project/results/*.csv'      # 원본 CSV 모아둔 폴더\n",
    "# CLEAN_FILE = '/Users/yujimin/KB AI CHALLENGE/project/news_clean.csv'    # 전처리 결과\n",
    "\n",
    "# # ── 1. 데이터 로드 ───────────────────────────────────────────────────\n",
    "# orig_df  = pd.concat((pd.read_csv(f, encoding='utf-8') for f in glob.glob(RAW_PATH)),\n",
    "#                      ignore_index=True)\n",
    "# clean_df = pd.read_csv(CLEAN_FILE, encoding='utf-8')\n",
    "\n",
    "# print(f'원본    : {orig_df.shape}  rows')\n",
    "# print(f'전처리본: {clean_df.shape} rows\\n')\n",
    "\n",
    "# # ── 2. (예시 A) 중복으로 삭제된 기사 살펴보기 ───────────────────────\n",
    "# dropped_links = set(orig_df['link']) - set(clean_df['link'])\n",
    "# dropped = orig_df[orig_df['link'].isin(dropped_links)]\n",
    "# print(f'🗑️  중복/결측으로 제거된 기사 수: {len(dropped)}')\n",
    "# display(dropped.head(3))\n",
    "\n",
    "# # ── 3. (예시 B) 제목·본문 정규화 차이 확인 ─────────────────────────\n",
    "# # clean_df['text'] = \"<title> [SEP] <content>\" 형태 → 제목만 분리\n",
    "# tmp = clean_df[['link', 'text']].copy()\n",
    "# tmp['title_clean'] = tmp['text'].str.split(' [SEP] ').str[0]\n",
    "\n",
    "# merged = orig_df.merge(tmp[['link', 'title_clean']], on='link', how='inner')\n",
    "\n",
    "# diff_title = merged[merged['title'] != merged['title_clean']]\n",
    "# print(f'✏️  정규화로 바뀐 제목 수: {len(diff_title)}')\n",
    "# display(diff_title[['title', 'title_clean']].head(3))\n",
    "\n",
    "# # ── 4. (예시 C) 본문 공백·URL 제거 예시 ────────────────────────────\n",
    "# def show_sample(idx):\n",
    "#     print(\"\\n원본 본문:\")\n",
    "#     print(orig_df.loc[idx, 'content'][:300], '...')\n",
    "#     print(\"\\n정제 본문:\")\n",
    "#     print(clean_df.loc[idx, 'text'].split(' [SEP] ')[1][:300], '...')\n",
    "\n",
    "# # 중복 제거 없는 임의 기사 인덱스 찾아서 시연\n",
    "# sample_idx = orig_df[~orig_df['link'].isin(dropped_links)].index[0]\n",
    "# show_sample(sample_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66baa739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 정규화 후 제목 완전 일치율: 100.0%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_norm_orig</th>\n",
       "      <th>title_norm_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title_norm_orig, title_norm_clean]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import pandas as pd, glob, re, html, unicodedata\n",
    "\n",
    "# # ── 0. 파일 경로 설정 ────────────────────────────────────────────────\n",
    "# RAW_PATH   = '/Users/yujimin/KB AI CHALLENGE/project/results/*.csv'      # 원본 CSV 모아둔 폴더\n",
    "# CLEAN_FILE = '/Users/yujimin/KB AI CHALLENGE/project/news_clean.csv'    # 전처리 결과\n",
    "\n",
    "# orig_df  = pd.concat((pd.read_csv(f) for f in glob.glob(RAW_PATH)), ignore_index=True)\n",
    "# clean_df = pd.read_csv(CLEAN_FILE)\n",
    "\n",
    "# # ── 1) link 기준 inner-join\n",
    "# merged = orig_df[['link', 'title']].merge(clean_df[['link', 'text']], on='link', how='inner')\n",
    "\n",
    "# # ── 2) 제목 부분만 추출 ─────────────────────────────────────────────\n",
    "# sep_regex = re.compile(r'\\s*\\[\\s*SEP\\s*\\]\\s*', flags=re.IGNORECASE)\n",
    "# merged['title_clean_raw'] = merged['text'].str.split(sep_regex).str[0]\n",
    "\n",
    "# # ── 3) 동일 규칙으로 양쪽 정규화\n",
    "# def normalize(t: str) -> str:\n",
    "#     t = unicodedata.normalize('NFKC', str(t))\n",
    "#     t = html.unescape(t)                # &amp; → &\n",
    "#     t = re.sub(r'https?://\\S+', '', t)  # URL 제거\n",
    "#     t = re.sub(r'\\s+', ' ', t).strip()  # 공백 축소\n",
    "#     return t\n",
    "\n",
    "# merged['title_norm_orig']  = merged['title'].apply(normalize)\n",
    "# merged['title_norm_clean'] = merged['title_clean_raw'].apply(normalize)\n",
    "\n",
    "# # ── 4) 일치율 재확인\n",
    "# match_rate = (merged['title_norm_orig'] == merged['title_norm_clean']).mean()\n",
    "# print(f\"🔍 정규화 후 제목 완전 일치율: {match_rate:.1%}\")\n",
    "\n",
    "# # 불일치 샘플 3건만 확인\n",
    "# mismatch = merged.query('title_norm_orig != title_norm_clean').head(3)\n",
    "# display(mismatch[['title_norm_orig', 'title_norm_clean']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4794e4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[link] 기준 중복 건수  : 0\n",
      "[title+date] 기준 중복 건수: 2472\n",
      "[전체 컬럼] 완전 중복 행수 : 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_name</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [stock_name, date, text, link, title]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # clean_df 내부 ‘중복 행’ 탐색 스니펫\n",
    "# # ───────────────────────────────────────────\n",
    "# import pandas as pd\n",
    "\n",
    "# # (가정) 이미 clean_df DataFrame 이 메모리에 존재\n",
    "# CLEAN_FILE = '/Users/yujimin/KB AI CHALLENGE/project/news_clean.csv'  \n",
    "\n",
    "# # 1) link 기준 ― 가장 확실한 중복 체크\n",
    "# dup_by_link = clean_df[clean_df.duplicated(subset=[\"link\"], keep=False)]\n",
    "# print(f\"[link] 기준 중복 건수  : {dup_by_link.shape[0]}\")\n",
    "# # display(dup_by_link.head(3))\n",
    "\n",
    "# # 2) title+date 기준 ― 동일 기사인데 링크가 바뀌었을 가능성 탐지\n",
    "# #    text 컬럼에서 제목만 분리해 임시 컬럼 생성\n",
    "# clean_df[\"title\"] = clean_df[\"text\"].str.split(r\"\\s*\\[\\s*SEP\\s*\\]\\s*\", n=1).str[0]\n",
    "\n",
    "# dup_by_title_date = clean_df[\n",
    "#     clean_df.duplicated(subset=[\"title\", \"date\"], keep=False)\n",
    "# ]\n",
    "# print(f\"[title+date] 기준 중복 건수: {dup_by_title_date.shape[0]}\")\n",
    "# # display(dup_by_title_date.head(3))\n",
    "\n",
    "# # 3) 전체 컬럼 완전 동일 행\n",
    "# dup_full = clean_df[clean_df.duplicated(keep=False)]\n",
    "# print(f\"[전체 컬럼] 완전 중복 행수 : {dup_full.shape[0]}\")\n",
    "# # display(dup_full.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4926ba94",
   "metadata": {},
   "source": [
    "현황 진단\n",
    "- link 기준 중복 0건 → URL은 모두 고유\n",
    "\n",
    "- title + date 기준 중복 2 472건 → 동일한 제목이 같은 날에 여러 URL(언론사·모바일/PC 버전 등)로 존재\n",
    "\n",
    "- 전체 8 065건 중 약 31 %가 “사실상 같은 기사” 이므로, 모델 학습·추론 효율을 위해 제거를 권장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "655ce500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️  중복 제거 전: 8,065\n",
      "⚙️  중복 제거 후: 6,716\n",
      "✅ Saved → news_clean_dedup.csv\n"
     ]
    }
   ],
   "source": [
    "#  본문 길이가 가장 긴 기사\t전문(全文)·모바일/PC 통합 버전일 가능성 ↑\n",
    "\n",
    "import pandas as pd, re\n",
    "\n",
    "CLEAN_FILE = '/Users/yujimin/KB AI CHALLENGE/project/news_clean.csv'  \n",
    "\n",
    "# 1) 제목 분리해 임시 저장\n",
    "sep_regex = re.compile(r\"\\s*\\[\\s*SEP\\s*\\]\\s*\", flags=re.IGNORECASE)\n",
    "clean_df[\"title\"]   = clean_df[\"text\"].str.split(sep_regex, n=1).str[0]\n",
    "clean_df[\"content\"] = clean_df[\"text\"].str.split(sep_regex, n=1).str[1]\n",
    "\n",
    "# 2) 본문 길이 계산\n",
    "clean_df[\"len\"] = clean_df[\"content\"].str.len()\n",
    "\n",
    "# 3) len 내림차순 → 중복(title, date) 제거 → 정렬 복원\n",
    "dedup = (\n",
    "    clean_df\n",
    "    .sort_values(\"len\", ascending=False)            # 길이 긴 기사 우선\n",
    "    .drop_duplicates(subset=[\"title\", \"date\"], keep=\"first\")\n",
    "    .sort_index()                                  # 원래 순서로 정렬(선택)\n",
    "    .drop(columns=[\"title\", \"content\", \"len\"])     # 임시 컬럼 정리\n",
    ")\n",
    "\n",
    "print(f\"⚙️  중복 제거 전: {clean_df.shape[0]:,}\")\n",
    "print(f\"⚙️  중복 제거 후: {dedup.shape[0]:,}\")\n",
    "\n",
    "# 4) 저장\n",
    "dedup.to_csv(\"news_clean_dedup.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"✅ Saved → news_clean_dedup.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39775d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_name</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>현대차</td>\n",
       "      <td>2025.08.07</td>\n",
       "      <td>HD현대, 美 안두릴과 손잡고 AI 무인군함 개발한다 [SEP] 연합뉴스HD현대가 ...</td>\n",
       "      <td>https://finance.naver.com/item/news_read.naver...</td>\n",
       "      <td>HD현대, 美 안두릴과 손잡고 AI 무인군함 개발한다</td>\n",
       "      <td>연합뉴스HD현대가 미국의 인공지능(AI) 방산기업 안두릴 인터스트리와 손잡고 함정 ...</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>현대차</td>\n",
       "      <td>2025.08.07</td>\n",
       "      <td>'베트남 서열 1위' 방한...李대통령 만찬에 재계 총수들 참석 [SEP] [서울=...</td>\n",
       "      <td>https://finance.naver.com/item/news_read.naver...</td>\n",
       "      <td>'베트남 서열 1위' 방한...李대통령 만찬에 재계 총수들 참석</td>\n",
       "      <td>[서울=뉴시스] 최진석 기자 = 이재명 대통령이 13일 서울 용산 대통령실에서 열린...</td>\n",
       "      <td>841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>현대차</td>\n",
       "      <td>2025.08.07</td>\n",
       "      <td>[단독] '베트남 서열 1위' 방한 만찬에 대기업 총수들 참석 [SEP] 10일 방...</td>\n",
       "      <td>https://finance.naver.com/item/news_read.naver...</td>\n",
       "      <td>[단독] '베트남 서열 1위' 방한 만찬에 대기업 총수들 참석</td>\n",
       "      <td>10일 방한하는 또 럼 베트남 공산당 서기장이 이틀에 걸쳐 국내 주요 대기업 총수 ...</td>\n",
       "      <td>1705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>현대차</td>\n",
       "      <td>2025.08.07</td>\n",
       "      <td>소비쿠폰 먹고 마시는 데 주로 썼다...소상공인 매출증대로 [SEP] 음식점, 마트...</td>\n",
       "      <td>https://finance.naver.com/item/news_read.naver...</td>\n",
       "      <td>소비쿠폰 먹고 마시는 데 주로 썼다...소상공인 매출증대로</td>\n",
       "      <td>음식점, 마트·식료품, 병원·약국 등 매츨 늘어나2주간 5조7679억원 지금···2...</td>\n",
       "      <td>1274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>현대차</td>\n",
       "      <td>2025.08.07</td>\n",
       "      <td>밥 먹고, 병원비 내고.. '소비쿠폰' 2조 원 어디 몰렸나 봤더니 [SEP] 제주...</td>\n",
       "      <td>https://finance.naver.com/item/news_read.naver...</td>\n",
       "      <td>밥 먹고, 병원비 내고.. '소비쿠폰' 2조 원 어디 몰렸나 봤더니</td>\n",
       "      <td>제주의 한 민생회복 소비쿠폰 사용 가능 매장전 국민에게 지급된 민생회복 소비쿠폰이 ...</td>\n",
       "      <td>1402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  stock_name        date                                               text  \\\n",
       "0        현대차  2025.08.07  HD현대, 美 안두릴과 손잡고 AI 무인군함 개발한다 [SEP] 연합뉴스HD현대가 ...   \n",
       "1        현대차  2025.08.07  '베트남 서열 1위' 방한...李대통령 만찬에 재계 총수들 참석 [SEP] [서울=...   \n",
       "2        현대차  2025.08.07  [단독] '베트남 서열 1위' 방한 만찬에 대기업 총수들 참석 [SEP] 10일 방...   \n",
       "3        현대차  2025.08.07  소비쿠폰 먹고 마시는 데 주로 썼다...소상공인 매출증대로 [SEP] 음식점, 마트...   \n",
       "4        현대차  2025.08.07  밥 먹고, 병원비 내고.. '소비쿠폰' 2조 원 어디 몰렸나 봤더니 [SEP] 제주...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://finance.naver.com/item/news_read.naver...   \n",
       "1  https://finance.naver.com/item/news_read.naver...   \n",
       "2  https://finance.naver.com/item/news_read.naver...   \n",
       "3  https://finance.naver.com/item/news_read.naver...   \n",
       "4  https://finance.naver.com/item/news_read.naver...   \n",
       "\n",
       "                                   title  \\\n",
       "0          HD현대, 美 안두릴과 손잡고 AI 무인군함 개발한다   \n",
       "1    '베트남 서열 1위' 방한...李대통령 만찬에 재계 총수들 참석   \n",
       "2     [단독] '베트남 서열 1위' 방한 만찬에 대기업 총수들 참석   \n",
       "3       소비쿠폰 먹고 마시는 데 주로 썼다...소상공인 매출증대로   \n",
       "4  밥 먹고, 병원비 내고.. '소비쿠폰' 2조 원 어디 몰렸나 봤더니   \n",
       "\n",
       "                                             content   len  \n",
       "0  연합뉴스HD현대가 미국의 인공지능(AI) 방산기업 안두릴 인터스트리와 손잡고 함정 ...  1200  \n",
       "1  [서울=뉴시스] 최진석 기자 = 이재명 대통령이 13일 서울 용산 대통령실에서 열린...   841  \n",
       "2  10일 방한하는 또 럼 베트남 공산당 서기장이 이틀에 걸쳐 국내 주요 대기업 총수 ...  1705  \n",
       "3  음식점, 마트·식료품, 병원·약국 등 매츨 늘어나2주간 5조7679억원 지금···2...  1274  \n",
       "4  제주의 한 민생회복 소비쿠폰 사용 가능 매장전 국민에게 지급된 민생회복 소비쿠폰이 ...  1402  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e5c5927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📅 종목별 날짜 범위·기사 수\n",
      "stock_name      start        end  days  articles\n",
      "     NAVER 2025-07-22 2025-08-07    17      1664\n",
      "      삼성전자 2025-07-31 2025-08-07     8      1680\n",
      "       카카오 2025-07-07 2025-08-07    32      1710\n",
      "       현대차 2025-07-28 2025-08-07    11      1662\n",
      "\n",
      "📰 NAVER – 기사 수 상위 5일\n",
      "stock_name       date  articles\n",
      "     NAVER 2025-08-04       238\n",
      "     NAVER 2025-08-05       206\n",
      "     NAVER 2025-07-23       161\n",
      "     NAVER 2025-07-24       161\n",
      "     NAVER 2025-07-30       124\n",
      "\n",
      "📰 삼성전자 – 기사 수 상위 5일\n",
      "stock_name       date  articles\n",
      "      삼성전자 2025-07-31       286\n",
      "      삼성전자 2025-08-07       285\n",
      "      삼성전자 2025-08-01       257\n",
      "      삼성전자 2025-08-05       251\n",
      "      삼성전자 2025-08-06       222\n",
      "\n",
      "📰 카카오 – 기사 수 상위 5일\n",
      "stock_name       date  articles\n",
      "       카카오 2025-08-07       151\n",
      "       카카오 2025-08-05       116\n",
      "       카카오 2025-07-21       103\n",
      "       카카오 2025-07-14       101\n",
      "       카카오 2025-07-09        88\n",
      "\n",
      "📰 현대차 – 기사 수 상위 5일\n",
      "stock_name       date  articles\n",
      "       현대차 2025-07-31       314\n",
      "       현대차 2025-07-30       253\n",
      "       현대차 2025-08-07       187\n",
      "       현대차 2025-07-29       154\n",
      "       현대차 2025-08-04       152\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ── 0) CSV 로드 ────────────────────────────\n",
    "df = pd.read_csv(\"/Users/yujimin/KB AI CHALLENGE/project/news_clean_dedup.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# ── 1) date 컬럼을 날짜형으로 변환 ────────\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y.%m.%d\")\n",
    "\n",
    "# ── 2) 종목별 날짜 범위 요약 ──────────────\n",
    "range_tbl = (\n",
    "    df.groupby(\"stock_name\")[\"date\"]\n",
    "      .agg(start=\"min\", end=\"max\", days=\"nunique\", articles=\"count\")\n",
    "      .reset_index()\n",
    "      .sort_values(\"stock_name\")\n",
    ")\n",
    "print(\"📅 종목별 날짜 범위·기사 수\")\n",
    "print(range_tbl.to_string(index=False))\n",
    "\n",
    "# 1) 종목-날짜별 기사 수 집계\n",
    "cnt = (\n",
    "    df.groupby([\"stock_name\", \"date\"])\n",
    "      .size()\n",
    "      .reset_index(name=\"articles\")\n",
    ")\n",
    "\n",
    "# 2) 종목별 상위 5일 추출\n",
    "top5_each = (\n",
    "    cnt.sort_values([\"stock_name\", \"articles\"], ascending=[True, False])\n",
    "       .groupby(\"stock_name\")\n",
    "       .head(5)\n",
    ")\n",
    "\n",
    "# 3) 보기 좋게 출력\n",
    "for stock, sub in top5_each.groupby(\"stock_name\"):\n",
    "    print(f\"\\n📰 {stock} – 기사 수 상위 5일\")\n",
    "    print(sub.sort_values(\"articles\", ascending=False)\n",
    "              .reset_index(drop=True)\n",
    "              .to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ae2a729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Found 4 CSV file(s)\n",
      "✅ After cleaning: (728, 4)\n",
      "🎉 Saved → /Users/yujimin/KB AI CHALLENGE/project/news_clean_20250805.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Naver Finance 뉴스 → 통합·정제·중복 제거 → news_clean_YYYYMMDD.csv\n",
    "\"\"\"\n",
    "import glob, re, unicodedata, pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 1. 파일 로드\n",
    "# ───────────────────────────────\n",
    "def load_csvs(pattern=\"/Users/yujimin/KB AI CHALLENGE/project/results/*.csv\") -> pd.DataFrame:\n",
    "    files = glob.glob(pattern)\n",
    "    print(f\"📂 Found {len(files)} CSV file(s)\")\n",
    "    return pd.concat((pd.read_csv(f, encoding=\"utf-8\") for f in files), ignore_index=True)\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 2. 텍스트 정규화\n",
    "# ───────────────────────────────\n",
    "URL_RE = re.compile(r\"https?://\\S+\")\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFKC\", str(text))\n",
    "    text = URL_RE.sub(\"\", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 3. 전처리 & 중복 제거\n",
    "# ───────────────────────────────\n",
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.dropna(subset=[\"title\", \"content\"]).drop_duplicates(subset=[\"link\"])\n",
    "\n",
    "    df[\"title\"]   = df[\"title\"].apply(normalize)\n",
    "    df[\"content\"] = df[\"content\"].apply(normalize)\n",
    "    df[\"text\"]    = df[\"title\"] + \" [SEP] \" + df[\"content\"]\n",
    "\n",
    "    # (선택) 동일 제목·날짜 중복 제거 – 본문 길이 긴 기사 우선\n",
    "    df[\"len\"] = df[\"content\"].str.len()\n",
    "    df = (\n",
    "        df.sort_values(\"len\", ascending=False)\n",
    "          .drop_duplicates(subset=[\"title\", \"datetime\"], keep=\"first\")\n",
    "          .drop(columns=\"len\")\n",
    "    )\n",
    "    return df[[\"stock_name\", \"datetime\", \"text\", \"link\"]]\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 4. 저장 파일명 결정 로직\n",
    "# ───────────────────────────────\n",
    "def decide_filename(df: pd.DataFrame, out_dir=\"/Users/yujimin/KB AI CHALLENGE/project\") -> str:\n",
    "    # datetime 컬럼에서 ‘YYYY.MM.DD’ 부분만 추출\n",
    "    dates = df[\"datetime\"].astype(str).str.extract(r\"(\\d{4}\\.\\d{2}\\.\\d{2})\")[0]\n",
    "    # 가장 이른 날짜(earliest) 선택 — 필요 시 latest/most common 으로 교체\n",
    "    target = pd.to_datetime(dates, format=\"%Y.%m.%d\").min().strftime(\"%Y%m%d\")\n",
    "    return f\"{out_dir}/news_clean_{target}.csv\"\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 5. 메인\n",
    "# ───────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    raw_df   = load_csvs()\n",
    "    clean_df = preprocess(raw_df)\n",
    "    print(f\"✅ After cleaning: {clean_df.shape}\")\n",
    "\n",
    "    out_csv = decide_filename(clean_df)\n",
    "    clean_df.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "    print(f\"🎉 Saved → {out_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a1b50c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
