{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f19d29e0",
   "metadata": {},
   "source": [
    "| ë‹¨ê³„                                            | ëª©ì                  | ì´ìœ                                  |\n",
    "| --------------------------------------------- | ------------------ | ---------------------------------- |\n",
    "| 1ï¸âƒ£ **ë¼ì´íŠ¸ ì •ì œ**<br>(Unicode NFKC, ê³µë°±Â·URL ì œê±° ë“±) | ìš”ì•½ ì…ë ¥ì„ ê¹¨ë—í•˜ê²Œ        | KoBARTê°€ HTML ì”ì—¬Â·ì¤„ë°”ê¿ˆÂ·URLì— ë¯¼ê° â†’ í’ˆì§ˆ â†“ |\n",
    "| 2ï¸âƒ£ **1ì°¨ ì¤‘ë³µ ì œê±°**<br>(`link`Â·`title+datetime`) | ë¶ˆí•„ìš”í•œ ì¸í¼ëŸ°ìŠ¤ ì ˆê°       | ìš”ì•½Â·ê°ì„± ë¶„ì„ ì‹œê°„ì€ í…ìŠ¤íŠ¸ ê±´ìˆ˜ì— ë¹„ë¡€            |\n",
    "| 3ï¸âƒ£ **KoBART ìš”ì•½**                             | `summary` ì»¬ëŸ¼ ìƒì„±    | ì´í›„ ë‹¨ê³„ì—ì„œ ì§§ì€ í…ìŠ¤íŠ¸ í™œìš© â†’ RAMÂ·CPU ë¶€ë‹´â†“    |\n",
    "| 4ï¸âƒ£ **ê°ì„± ë¶„ì„**<br>(KB-ALBERTÂ·FinBERT ë“±)        | `neg/neu/pos` ìŠ¤ì½”ì–´  | ì§§ì€ summaryë¥¼ ì“°ë©´ ë¶„ì„ ì†ë„ 2-3ë°° ê°œì„        |\n",
    "| 5ï¸âƒ£ **ìµœì¢… CSV ì €ì¥**                             | `raw_text`ê¹Œì§€ ë³´ì¡´ ê¶Œì¥ | ì¶”í›„ ë””ë²„ê¹…Â·ì¬í•™ìŠµ ëŒ€ë¹„                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea8ad19",
   "metadata": {},
   "source": [
    "ìœ„ ì ˆì°¨ ì¤‘ í˜„ì¬ íŒŒì¼ì—ì„œëŠ” 1,2,3ë²ˆ ìˆ˜í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae2a729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Found 4 CSV file(s)\n",
      "âœ… After cleaning: (728, 4)\n",
      "ğŸ‰ Saved â†’ /Users/yujimin/KB AI CHALLENGE/project/news_clean_20250805.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Naver Finance ë‰´ìŠ¤ â†’ í†µí•©Â·ì •ì œÂ·ì¤‘ë³µ ì œê±° â†’ news_clean_YYYYMMDD.csv\n",
    "\"\"\"\n",
    "import glob, re, unicodedata, pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. íŒŒì¼ ë¡œë“œ\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def load_csvs(pattern=\"/Users/yujimin/KB AI CHALLENGE/project/results/*.csv\") -> pd.DataFrame:\n",
    "    files = glob.glob(pattern)\n",
    "    print(f\"ğŸ“‚ Found {len(files)} CSV file(s)\")\n",
    "    return pd.concat((pd.read_csv(f, encoding=\"utf-8\") for f in files), ignore_index=True)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2. í…ìŠ¤íŠ¸ ì •ê·œí™”\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "URL_RE = re.compile(r\"https?://\\S+\")\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFKC\", str(text))\n",
    "    text = URL_RE.sub(\"\", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3. ì „ì²˜ë¦¬ & ì¤‘ë³µ ì œê±°\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.dropna(subset=[\"title\", \"content\"]).drop_duplicates(subset=[\"link\"])\n",
    "\n",
    "    df[\"title\"]   = df[\"title\"].apply(normalize)\n",
    "    df[\"content\"] = df[\"content\"].apply(normalize)\n",
    "    df[\"text\"]    = df[\"title\"] + \" [SEP] \" + df[\"content\"]\n",
    "\n",
    "    # (ì„ íƒ) ë™ì¼ ì œëª©Â·ë‚ ì§œ ì¤‘ë³µ ì œê±° â€“ ë³¸ë¬¸ ê¸¸ì´ ê¸´ ê¸°ì‚¬ ìš°ì„ \n",
    "    df[\"len\"] = df[\"content\"].str.len()\n",
    "    df = (\n",
    "        df.sort_values(\"len\", ascending=False)\n",
    "          .drop_duplicates(subset=[\"title\", \"datetime\"], keep=\"first\")\n",
    "          .drop(columns=\"len\")\n",
    "    )\n",
    "    return df[[\"stock_name\", \"datetime\", \"text\", \"link\"]]\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4. ì €ì¥ íŒŒì¼ëª… ê²°ì • ë¡œì§\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def decide_filename(df: pd.DataFrame, out_dir=\"/Users/yujimin/KB AI CHALLENGE/project\") -> str:\n",
    "    # datetime ì»¬ëŸ¼ì—ì„œ â€˜YYYY.MM.DDâ€™ ë¶€ë¶„ë§Œ ì¶”ì¶œ\n",
    "    dates = df[\"datetime\"].astype(str).str.extract(r\"(\\d{4}\\.\\d{2}\\.\\d{2})\")[0]\n",
    "    # ê°€ì¥ ì´ë¥¸ ë‚ ì§œ(earliest) ì„ íƒ â€” í•„ìš” ì‹œ latest/most common ìœ¼ë¡œ êµì²´\n",
    "    target = pd.to_datetime(dates, format=\"%Y.%m.%d\").min().strftime(\"%Y%m%d\")\n",
    "    return f\"{out_dir}/news_clean_{target}.csv\"\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5. ë©”ì¸\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if __name__ == \"__main__\":\n",
    "    raw_df   = load_csvs()\n",
    "    clean_df = preprocess(raw_df)\n",
    "    print(f\"âœ… After cleaning: {clean_df.shape}\")\n",
    "\n",
    "    out_csv = decide_filename(clean_df)\n",
    "    clean_df.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "    print(f\"ğŸ‰ Saved â†’ {out_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eb5e01",
   "metadata": {},
   "source": [
    "### ë‰´ìŠ¤ í…ìŠ¤íŠ¸ ìš”ì•½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc7b4071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„  Loading KoBART summarizer â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=30000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì½”ë©\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "news_clean_20250805.csv â†’ KoBART ìš”ì•½ â†’ text ì»¬ëŸ¼ ì œê±° â†’ news_summary_20250805.csv\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import math, sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. ê²½ë¡œÂ·í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# IN_CSV  = \"/content/drive/MyDrive/Invemotion/news_clean_20250805.csv\"\n",
    "# OUT_CSV = \"/content/drive/MyDrive/Invemotion/news_summary_20250805.csv\"\n",
    "IN_CSV  = \"/Users/yujimin/KB AI CHALLENGE/project/news_clean_20250805.csv\"\n",
    "OUT_CSV = \"/Users/yujimin/KB AI CHALLENGE/project/news_summary_20250805.csv\"\n",
    "\n",
    "\n",
    "BATCH_SIZE      = 8          # GPU VRAM 4 GB ê¸°ì¤€\n",
    "MAX_INPUT_LEN   = 512\n",
    "MAX_SUMMARY_LEN = 128\n",
    "DEVICE          = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2. ëª¨ë¸ ë¡œë“œ\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ğŸ”„  Loading KoBART summarizer â€¦\")\n",
    "tok   = AutoTokenizer.from_pretrained(\"digit82/kobart-summarization\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"digit82/kobart-summarization\").to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4783e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49313186813186816\n"
     ]
    }
   ],
   "source": [
    "# df = pd.read_csv(IN_CSV, encoding=\"utf-8\")\n",
    "# # print(f\"ğŸ“‚ Loaded {len(df):,} rows\")\n",
    "\n",
    "# # ìš”ì•½ ëŒë¦¬ê¸° ì „ì— í•œ ë²ˆë§Œ ì‹¤í–‰\n",
    "# enc = tok(df[\"text\"].tolist(), truncation=False, padding=False)\n",
    "# df[\"tok_len\"] = [len(x) for x in enc[\"input_ids\"]]\n",
    "# df[\"truncated\"] = df[\"tok_len\"] > MAX_INPUT_LEN\n",
    "# print(df[\"truncated\"].mean())  # 512í† í° ì´ˆê³¼ ë¹„ìœ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4142e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "CHUNK_TOKENS   = 400   # ê° ì¡°ê° ì…ë ¥ ê¸¸ì´ (512ë³´ë‹¤ ì—¬ìœ ìˆê²Œ)\n",
    "OVERLAP_TOKENS = 80    # ì¡°ê° ê°„ ê²¹ì¹¨(ë¬¸ë§¥ ë³´ì¡´)\n",
    "\n",
    "def chunk_by_tokens(text: str, chunk_tokens=CHUNK_TOKENS, overlap=OVERLAP_TOKENS) -> list[str]:\n",
    "    ids = tok.encode(text, add_special_tokens=True)\n",
    "    chunks = []\n",
    "    step = max(1, chunk_tokens - overlap)\n",
    "    for s in range(0, len(ids), step):\n",
    "        piece = ids[s:s+chunk_tokens]\n",
    "        if not piece:\n",
    "            break\n",
    "        chunks.append(tok.decode(piece, skip_special_tokens=True))\n",
    "        if s + chunk_tokens >= len(ids):\n",
    "            break\n",
    "    return chunks\n",
    "\n",
    "def summarize_long(text: str) -> str:\n",
    "    parts = chunk_by_tokens(text)\n",
    "    # 1ì°¨: ì¡°ê°ë³„ ìš”ì•½ (ë°°ì¹˜ ì²˜ë¦¬)\n",
    "    part_sums = []\n",
    "    for i in range(0, len(parts), BATCH_SIZE):\n",
    "        part_sums += summarize_batch(parts[i:i+BATCH_SIZE])\n",
    "    # 2ì°¨: ìš”ì•½ë“¤ì˜ ìš”ì•½(ë©”íƒ€ ìš”ì•½)\n",
    "    final = summarize_batch([\" \".join(part_sums)])[0]\n",
    "    return final\n",
    "\n",
    "# 1) ê¸¸ì´ ì¸¡ì •\n",
    "enc = tok(df[\"text\"].tolist(), truncation=False, padding=False)\n",
    "df[\"tok_len\"]    = [len(x) for x in enc[\"input_ids\"]]\n",
    "df[\"truncated\"]  = df[\"tok_len\"] > MAX_INPUT_LEN\n",
    "\n",
    "# 2) ìš”ì•½ ì‹¤í–‰: ì§§ì€ ë¬¸ì„œ(â‰¤512)ëŠ” ê¸°ì¡´ ë°°ì¹˜ ìš”ì•½, ê¸´ ë¬¸ì„œ(>512)ëŠ” ìŠ¬ë¼ì´ë”© ìœˆë„ìš°\n",
    "summaries = [\"\"] * len(df)\n",
    "\n",
    "# 2-a) ì§§ì€ ë¬¸ì„œ ì¼ê´„ ì²˜ë¦¬\n",
    "short_idx = df.index[~df[\"truncated\"]].tolist()\n",
    "for i in tqdm(range(0, len(short_idx), BATCH_SIZE), desc=\"Short docs\"):\n",
    "    batch_ids = short_idx[i:i+BATCH_SIZE]\n",
    "    texts = df.loc[batch_ids, \"text\"].tolist()\n",
    "    outs = summarize_batch(texts)\n",
    "    for j, idx in enumerate(batch_ids):\n",
    "        summaries[idx] = outs[j]\n",
    "\n",
    "# 2-b) ê¸´ ë¬¸ì„œ ê°œë³„ ì²˜ë¦¬(ì¡°ê° ìš”ì•½ â†’ ë©”íƒ€ ìš”ì•½)\n",
    "long_idx = df.index[df[\"truncated\"]].tolist()\n",
    "for idx in tqdm(long_idx, desc=\"Long docs\"):\n",
    "    summaries[idx] = summarize_long(df.at[idx, \"text\"])\n",
    "\n",
    "# 3) ì»¬ëŸ¼ ë°˜ì˜ ë° ì •ë¦¬\n",
    "df.insert(2, \"summary\", summaries)\n",
    "df = df.drop(columns=[\"text\", \"tok_len\", \"truncated\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43a1b50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„  Loading KoBART summarizer â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Loaded 728 rows\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['token_type_ids'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(total_batches):\n\u001b[1;32m     71\u001b[0m     batch_texts \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[i \u001b[38;5;241m*\u001b[39m BATCH_SIZE:(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m BATCH_SIZE]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m---> 72\u001b[0m     summaries\u001b[38;5;241m.\u001b[39mextend(\u001b[43msummarize_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_texts\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i \u001b[38;5;241m==\u001b[39m total_batches \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     75\u001b[0m         processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m((i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m BATCH_SIZE, \u001b[38;5;28mlen\u001b[39m(df))\n",
      "Cell \u001b[0;32mIn[3], line 48\u001b[0m, in \u001b[0;36msummarize_batch\u001b[0;34m(texts)\u001b[0m\n\u001b[1;32m     39\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tok(\n\u001b[1;32m     40\u001b[0m     texts,\n\u001b[1;32m     41\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mMAX_INPUT_LEN,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     45\u001b[0m )\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 48\u001b[0m     outs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_SUMMARY_LEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tok\u001b[38;5;241m.\u001b[39mbatch_decode(outs, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/transformers/generation/utils.py:2384\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2379\u001b[0m assistant_tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# only used for assisted generation\u001b[39;00m\n\u001b[1;32m   2381\u001b[0m generation_config, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_generation_config(\n\u001b[1;32m   2382\u001b[0m     generation_config, use_model_defaults, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   2383\u001b[0m )\n\u001b[0;32m-> 2384\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2385\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_assistant(assistant_model, tokenizer, assistant_tokenizer)\n\u001b[1;32m   2387\u001b[0m \u001b[38;5;66;03m# 2. Set generation parameters if not already defined\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/transformers/generation/utils.py:1607\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m   1604\u001b[0m         unused_model_args\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[1;32m   1606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[0;32m-> 1607\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1608\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (note: typos in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1609\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m generate arguments will also show up in this list)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1610\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['token_type_ids'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "# #!/usr/bin/env python\n",
    "# # -*- coding: utf-8 -*-\n",
    "# \"\"\"\n",
    "# news_clean_20250805.csv â†’ KoBART ìš”ì•½ â†’ text ì»¬ëŸ¼ ì œê±° â†’ news_summary_20250805.csv\n",
    "# \"\"\"\n",
    "\n",
    "# from __future__ import annotations\n",
    "# import math, sys\n",
    "# from pathlib import Path\n",
    "\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# # 1. ê²½ë¡œÂ·í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# IN_CSV  = \"/content/drive/MyDrive/Invemotion/news_clean_20250805.csv\"\n",
    "# OUT_CSV = \"/content/drive/MyDrive/Invemotion/news_summary_20250805.csv\"\n",
    "\n",
    "# BATCH_SIZE      = 8          # GPU VRAM 4 GB ê¸°ì¤€\n",
    "# MAX_INPUT_LEN   = 512\n",
    "# MAX_SUMMARY_LEN = 128\n",
    "# DEVICE          = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# # 2. ëª¨ë¸ ë¡œë“œ\n",
    "# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# print(\"ğŸ”„  Loading KoBART summarizer â€¦\")\n",
    "# tok   = AutoTokenizer.from_pretrained(\"digit82/kobart-summarization\")\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"digit82/kobart-summarization\").to(DEVICE)\n",
    "# model.eval()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3. ìš”ì•½ í•¨ìˆ˜ ìˆ˜ì • (í•µì‹¬)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def summarize_batch(texts: list[str]) -> list[str]:\n",
    "    enc = tok(\n",
    "        texts,\n",
    "        max_length=MAX_INPUT_LEN,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # KoBARTì—ëŠ” token_type_ids ë¶ˆí•„ìš” â€“ ì œê±°\n",
    "    if \"token_type_ids\" in enc:\n",
    "        enc.pop(\"token_type_ids\")\n",
    "\n",
    "    enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **enc,\n",
    "            max_length=MAX_SUMMARY_LEN,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    return tok.batch_decode(out, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# # 3. ë°ì´í„° ë¡œë“œ\n",
    "# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# df = pd.read_csv(IN_CSV, encoding=\"utf-8\")\n",
    "# print(f\"ğŸ“‚ Loaded {len(df):,} rows\")\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4. KoBART ìš”ì•½ (ë°°ì¹˜ ì²˜ë¦¬)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "summaries: list[str] = []\n",
    "total_batches = math.ceil(len(df) / BATCH_SIZE)\n",
    "\n",
    "for i in range(total_batches):\n",
    "    batch_texts = df[\"text\"].iloc[i * BATCH_SIZE:(i + 1) * BATCH_SIZE].tolist()\n",
    "    summaries.extend(summarize_batch(batch_texts))\n",
    "\n",
    "    if (i + 1) % 10 == 0 or i == total_batches - 1:\n",
    "        processed = min((i + 1) * BATCH_SIZE, len(df))\n",
    "        print(f\"ğŸ“ Summarized {processed:,} / {len(df):,}\")\n",
    "\n",
    "df.insert(2, \"summary\", summaries)   # summary ì»¬ëŸ¼ ì‚½ì…\n",
    "df = df.drop(columns=\"text\")         # âœ”ï¸ text ì»¬ëŸ¼ ì œê±°\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5. ì €ì¥\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "out_path = Path(OUT_CSV)\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"ğŸ‰ Saved â†’ {out_path}  ({len(df):,} rows, columns: {list(df.columns)})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
