{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f19d29e0",
   "metadata": {},
   "source": [
    "| 단계                                            | 목적                 | 이유                                 |\n",
    "| --------------------------------------------- | ------------------ | ---------------------------------- |\n",
    "| 1️⃣ **라이트 정제**<br>(Unicode NFKC, 공백·URL 제거 등) | 요약 입력을 깨끗하게        | KoBART가 HTML 잔여·줄바꿈·URL에 민감 → 품질 ↓ |\n",
    "| 2️⃣ **1차 중복 제거**<br>(`link`·`title+datetime`) | 불필요한 인퍼런스 절감       | 요약·감성 분석 시간은 텍스트 건수에 비례            |\n",
    "| 3️⃣ **KoBART 요약**                             | `summary` 컬럼 생성    | 이후 단계에서 짧은 텍스트 활용 → RAM·CPU 부담↓    |\n",
    "| 4️⃣ **감성 분석**<br>(KB-ALBERT·FinBERT 등)        | `neg/neu/pos` 스코어  | 짧은 summary를 쓰면 분석 속도 2-3배 개선       |\n",
    "| 5️⃣ **최종 CSV 저장**                             | `raw_text`까지 보존 권장 | 추후 디버깅·재학습 대비                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea8ad19",
   "metadata": {},
   "source": [
    "위 절차 중 현재 파일에서는 1,2,3번 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae2a729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Found 4 CSV file(s)\n",
      "✅ After cleaning: (728, 4)\n",
      "🎉 Saved → /Users/yujimin/KB AI CHALLENGE/project/news_clean_20250805.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Naver Finance 뉴스 → 통합·정제·중복 제거 → news_clean_YYYYMMDD.csv\n",
    "\"\"\"\n",
    "import glob, re, unicodedata, pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 1. 파일 로드\n",
    "# ───────────────────────────────\n",
    "def load_csvs(pattern=\"/Users/yujimin/KB AI CHALLENGE/project/results/*.csv\") -> pd.DataFrame:\n",
    "    files = glob.glob(pattern)\n",
    "    print(f\"📂 Found {len(files)} CSV file(s)\")\n",
    "    return pd.concat((pd.read_csv(f, encoding=\"utf-8\") for f in files), ignore_index=True)\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 2. 텍스트 정규화\n",
    "# ───────────────────────────────\n",
    "URL_RE = re.compile(r\"https?://\\S+\")\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFKC\", str(text))\n",
    "    text = URL_RE.sub(\"\", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 3. 전처리 & 중복 제거\n",
    "# ───────────────────────────────\n",
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.dropna(subset=[\"title\", \"content\"]).drop_duplicates(subset=[\"link\"])\n",
    "\n",
    "    df[\"title\"]   = df[\"title\"].apply(normalize)\n",
    "    df[\"content\"] = df[\"content\"].apply(normalize)\n",
    "    df[\"text\"]    = df[\"title\"] + \" [SEP] \" + df[\"content\"]\n",
    "\n",
    "    # (선택) 동일 제목·날짜 중복 제거 – 본문 길이 긴 기사 우선\n",
    "    df[\"len\"] = df[\"content\"].str.len()\n",
    "    df = (\n",
    "        df.sort_values(\"len\", ascending=False)\n",
    "          .drop_duplicates(subset=[\"title\", \"datetime\"], keep=\"first\")\n",
    "          .drop(columns=\"len\")\n",
    "    )\n",
    "    return df[[\"stock_name\", \"datetime\", \"text\", \"link\"]]\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 4. 저장 파일명 결정 로직\n",
    "# ───────────────────────────────\n",
    "def decide_filename(df: pd.DataFrame, out_dir=\"/Users/yujimin/KB AI CHALLENGE/project\") -> str:\n",
    "    # datetime 컬럼에서 ‘YYYY.MM.DD’ 부분만 추출\n",
    "    dates = df[\"datetime\"].astype(str).str.extract(r\"(\\d{4}\\.\\d{2}\\.\\d{2})\")[0]\n",
    "    # 가장 이른 날짜(earliest) 선택 — 필요 시 latest/most common 으로 교체\n",
    "    target = pd.to_datetime(dates, format=\"%Y.%m.%d\").min().strftime(\"%Y%m%d\")\n",
    "    return f\"{out_dir}/news_clean_{target}.csv\"\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 5. 메인\n",
    "# ───────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    raw_df   = load_csvs()\n",
    "    clean_df = preprocess(raw_df)\n",
    "    print(f\"✅ After cleaning: {clean_df.shape}\")\n",
    "\n",
    "    out_csv = decide_filename(clean_df)\n",
    "    clean_df.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "    print(f\"🎉 Saved → {out_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eb5e01",
   "metadata": {},
   "source": [
    "### 뉴스 텍스트 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc7b4071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄  Loading KoBART summarizer …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=30000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 코랩\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "news_clean_20250805.csv → KoBART 요약 → text 컬럼 제거 → news_summary_20250805.csv\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import math, sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 1. 경로·하이퍼파라미터\n",
    "# ───────────────────────────────\n",
    "# IN_CSV  = \"/content/drive/MyDrive/Invemotion/news_clean_20250805.csv\"\n",
    "# OUT_CSV = \"/content/drive/MyDrive/Invemotion/news_summary_20250805.csv\"\n",
    "IN_CSV  = \"/Users/yujimin/KB AI CHALLENGE/project/news_clean_20250805.csv\"\n",
    "OUT_CSV = \"/Users/yujimin/KB AI CHALLENGE/project/news_summary_20250805.csv\"\n",
    "\n",
    "\n",
    "BATCH_SIZE      = 8          # GPU VRAM 4 GB 기준\n",
    "MAX_INPUT_LEN   = 512\n",
    "MAX_SUMMARY_LEN = 128\n",
    "DEVICE          = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 2. 모델 로드\n",
    "# ───────────────────────────────\n",
    "print(\"🔄  Loading KoBART summarizer …\")\n",
    "tok   = AutoTokenizer.from_pretrained(\"digit82/kobart-summarization\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"digit82/kobart-summarization\").to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4783e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49313186813186816\n"
     ]
    }
   ],
   "source": [
    "# df = pd.read_csv(IN_CSV, encoding=\"utf-8\")\n",
    "# # print(f\"📂 Loaded {len(df):,} rows\")\n",
    "\n",
    "# # 요약 돌리기 전에 한 번만 실행\n",
    "# enc = tok(df[\"text\"].tolist(), truncation=False, padding=False)\n",
    "# df[\"tok_len\"] = [len(x) for x in enc[\"input_ids\"]]\n",
    "# df[\"truncated\"] = df[\"tok_len\"] > MAX_INPUT_LEN\n",
    "# print(df[\"truncated\"].mean())  # 512토큰 초과 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4142e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "CHUNK_TOKENS   = 400   # 각 조각 입력 길이 (512보다 여유있게)\n",
    "OVERLAP_TOKENS = 80    # 조각 간 겹침(문맥 보존)\n",
    "\n",
    "def chunk_by_tokens(text: str, chunk_tokens=CHUNK_TOKENS, overlap=OVERLAP_TOKENS) -> list[str]:\n",
    "    ids = tok.encode(text, add_special_tokens=True)\n",
    "    chunks = []\n",
    "    step = max(1, chunk_tokens - overlap)\n",
    "    for s in range(0, len(ids), step):\n",
    "        piece = ids[s:s+chunk_tokens]\n",
    "        if not piece:\n",
    "            break\n",
    "        chunks.append(tok.decode(piece, skip_special_tokens=True))\n",
    "        if s + chunk_tokens >= len(ids):\n",
    "            break\n",
    "    return chunks\n",
    "\n",
    "def summarize_long(text: str) -> str:\n",
    "    parts = chunk_by_tokens(text)\n",
    "    # 1차: 조각별 요약 (배치 처리)\n",
    "    part_sums = []\n",
    "    for i in range(0, len(parts), BATCH_SIZE):\n",
    "        part_sums += summarize_batch(parts[i:i+BATCH_SIZE])\n",
    "    # 2차: 요약들의 요약(메타 요약)\n",
    "    final = summarize_batch([\" \".join(part_sums)])[0]\n",
    "    return final\n",
    "\n",
    "# 1) 길이 측정\n",
    "enc = tok(df[\"text\"].tolist(), truncation=False, padding=False)\n",
    "df[\"tok_len\"]    = [len(x) for x in enc[\"input_ids\"]]\n",
    "df[\"truncated\"]  = df[\"tok_len\"] > MAX_INPUT_LEN\n",
    "\n",
    "# 2) 요약 실행: 짧은 문서(≤512)는 기존 배치 요약, 긴 문서(>512)는 슬라이딩 윈도우\n",
    "summaries = [\"\"] * len(df)\n",
    "\n",
    "# 2-a) 짧은 문서 일괄 처리\n",
    "short_idx = df.index[~df[\"truncated\"]].tolist()\n",
    "for i in tqdm(range(0, len(short_idx), BATCH_SIZE), desc=\"Short docs\"):\n",
    "    batch_ids = short_idx[i:i+BATCH_SIZE]\n",
    "    texts = df.loc[batch_ids, \"text\"].tolist()\n",
    "    outs = summarize_batch(texts)\n",
    "    for j, idx in enumerate(batch_ids):\n",
    "        summaries[idx] = outs[j]\n",
    "\n",
    "# 2-b) 긴 문서 개별 처리(조각 요약 → 메타 요약)\n",
    "long_idx = df.index[df[\"truncated\"]].tolist()\n",
    "for idx in tqdm(long_idx, desc=\"Long docs\"):\n",
    "    summaries[idx] = summarize_long(df.at[idx, \"text\"])\n",
    "\n",
    "# 3) 컬럼 반영 및 정리\n",
    "df.insert(2, \"summary\", summaries)\n",
    "df = df.drop(columns=[\"text\", \"tok_len\", \"truncated\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43a1b50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄  Loading KoBART summarizer …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loaded 728 rows\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['token_type_ids'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(total_batches):\n\u001b[1;32m     71\u001b[0m     batch_texts \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[i \u001b[38;5;241m*\u001b[39m BATCH_SIZE:(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m BATCH_SIZE]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m---> 72\u001b[0m     summaries\u001b[38;5;241m.\u001b[39mextend(\u001b[43msummarize_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_texts\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i \u001b[38;5;241m==\u001b[39m total_batches \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     75\u001b[0m         processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m((i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m BATCH_SIZE, \u001b[38;5;28mlen\u001b[39m(df))\n",
      "Cell \u001b[0;32mIn[3], line 48\u001b[0m, in \u001b[0;36msummarize_batch\u001b[0;34m(texts)\u001b[0m\n\u001b[1;32m     39\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tok(\n\u001b[1;32m     40\u001b[0m     texts,\n\u001b[1;32m     41\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mMAX_INPUT_LEN,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     45\u001b[0m )\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 48\u001b[0m     outs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_SUMMARY_LEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tok\u001b[38;5;241m.\u001b[39mbatch_decode(outs, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/transformers/generation/utils.py:2384\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2379\u001b[0m assistant_tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# only used for assisted generation\u001b[39;00m\n\u001b[1;32m   2381\u001b[0m generation_config, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_generation_config(\n\u001b[1;32m   2382\u001b[0m     generation_config, use_model_defaults, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   2383\u001b[0m )\n\u001b[0;32m-> 2384\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2385\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_assistant(assistant_model, tokenizer, assistant_tokenizer)\n\u001b[1;32m   2387\u001b[0m \u001b[38;5;66;03m# 2. Set generation parameters if not already defined\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/transformers/generation/utils.py:1607\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m   1604\u001b[0m         unused_model_args\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[1;32m   1606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[0;32m-> 1607\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1608\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (note: typos in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1609\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m generate arguments will also show up in this list)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1610\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['token_type_ids'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "# #!/usr/bin/env python\n",
    "# # -*- coding: utf-8 -*-\n",
    "# \"\"\"\n",
    "# news_clean_20250805.csv → KoBART 요약 → text 컬럼 제거 → news_summary_20250805.csv\n",
    "# \"\"\"\n",
    "\n",
    "# from __future__ import annotations\n",
    "# import math, sys\n",
    "# from pathlib import Path\n",
    "\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "# # ───────────────────────────────\n",
    "# # 1. 경로·하이퍼파라미터\n",
    "# # ───────────────────────────────\n",
    "# IN_CSV  = \"/content/drive/MyDrive/Invemotion/news_clean_20250805.csv\"\n",
    "# OUT_CSV = \"/content/drive/MyDrive/Invemotion/news_summary_20250805.csv\"\n",
    "\n",
    "# BATCH_SIZE      = 8          # GPU VRAM 4 GB 기준\n",
    "# MAX_INPUT_LEN   = 512\n",
    "# MAX_SUMMARY_LEN = 128\n",
    "# DEVICE          = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# # ───────────────────────────────\n",
    "# # 2. 모델 로드\n",
    "# # ───────────────────────────────\n",
    "# print(\"🔄  Loading KoBART summarizer …\")\n",
    "# tok   = AutoTokenizer.from_pretrained(\"digit82/kobart-summarization\")\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"digit82/kobart-summarization\").to(DEVICE)\n",
    "# model.eval()\n",
    "\n",
    "# ───────────────────────────\n",
    "# 3. 요약 함수 수정 (핵심)\n",
    "# ───────────────────────────\n",
    "def summarize_batch(texts: list[str]) -> list[str]:\n",
    "    enc = tok(\n",
    "        texts,\n",
    "        max_length=MAX_INPUT_LEN,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # KoBART에는 token_type_ids 불필요 – 제거\n",
    "    if \"token_type_ids\" in enc:\n",
    "        enc.pop(\"token_type_ids\")\n",
    "\n",
    "    enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **enc,\n",
    "            max_length=MAX_SUMMARY_LEN,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    return tok.batch_decode(out, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# # ───────────────────────────────\n",
    "# # 3. 데이터 로드\n",
    "# # ───────────────────────────────\n",
    "# df = pd.read_csv(IN_CSV, encoding=\"utf-8\")\n",
    "# print(f\"📂 Loaded {len(df):,} rows\")\n",
    "\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 4. KoBART 요약 (배치 처리)\n",
    "# ───────────────────────────────\n",
    "summaries: list[str] = []\n",
    "total_batches = math.ceil(len(df) / BATCH_SIZE)\n",
    "\n",
    "for i in range(total_batches):\n",
    "    batch_texts = df[\"text\"].iloc[i * BATCH_SIZE:(i + 1) * BATCH_SIZE].tolist()\n",
    "    summaries.extend(summarize_batch(batch_texts))\n",
    "\n",
    "    if (i + 1) % 10 == 0 or i == total_batches - 1:\n",
    "        processed = min((i + 1) * BATCH_SIZE, len(df))\n",
    "        print(f\"📝 Summarized {processed:,} / {len(df):,}\")\n",
    "\n",
    "df.insert(2, \"summary\", summaries)   # summary 컬럼 삽입\n",
    "df = df.drop(columns=\"text\")         # ✔️ text 컬럼 제거\n",
    "\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 5. 저장\n",
    "# ───────────────────────────────\n",
    "out_path = Path(OUT_CSV)\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"🎉 Saved → {out_path}  ({len(df):,} rows, columns: {list(df.columns)})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
