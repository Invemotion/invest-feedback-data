# run_local_export_jsonl.py
# -*- coding: utf-8 -*-
import json
from pathlib import Path
from crawler_core import crawl_many

STOCKS = [
    {"name": "ì‚¼ì„±ì „ì", "code": "005930"},
    {"name": "NAVER",   "code": "035420"},
    {"name": "ì¹´ì¹´ì˜¤",   "code": "035720"},
    {"name": "í˜„ëŒ€ì°¨",   "code": "005380"},
]

TARGET_DATE = "2025.08.05"  # YYYY.MM.DD
OUT_ROOT   = Path("./news_raw")  # ë£¨íŠ¸ ë””ë ‰í„°ë¦¬

def safe_filename(name: str) -> str:
    """íŒŒì¼/í´ë”ëª… ì•ˆì „í™”: ê¸ˆì§€ë¬¸ì ì œê±°, ê³µë°± ì••ì¶•"""
    import re
    s = str(name).strip()
    s = re.sub(r"[\\/:*?\"<>|]+", "_", s)  # ê¸ˆì§€ë¬¸ì â†’ _
    s = re.sub(r"\s+", "_", s)             # ë‹¤ì¤‘ ê³µë°± â†’ _
    return s

def date_token(date_str: str) -> str:
    """'2025.08.05' â†’ '20250805'"""
    return date_str.replace(".", "")

def main():
    dt_tok = date_token(TARGET_DATE)
    day_dir = OUT_ROOT / dt_tok
    day_dir.mkdir(parents=True, exist_ok=True)

    for stock in STOCKS:
        stock_name = stock["name"]
        print(f"\nğŸ“Œ [{stock_name}] {TARGET_DATE} ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘...")
        
        fname = f"{safe_filename(stock_name)}.jsonl"
        out_path = day_dir / fname

        count = 0
        with open(out_path, "w", encoding="utf-8") as f:
            for r in crawl_many([stock], TARGET_DATE, max_pages=200):
                f.write(json.dumps(r, ensure_ascii=False) + "\n")
                count += 1
                # 10ê±´ ë‹¨ìœ„ë¡œ ì§„í–‰ ìƒí™© í‘œì‹œ
                if count % 10 == 0:
                    print(f"  â”œâ”€ ì§„í–‰: {count}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
        
        print(f"âœ… [{stock_name}] ì™„ë£Œ: {count}ê±´ ì €ì¥ â†’ {out_path}")

if __name__ == "__main__":
    main()
